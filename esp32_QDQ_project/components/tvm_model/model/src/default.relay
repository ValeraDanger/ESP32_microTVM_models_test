def @main(%input_1: Tensor[(1, 32, 32, 3), float32] /* ty=Tensor[(1, 32, 32, 3), float32] span=Transpose_1.input_1:0:0 */) -> Tensor[(1, 10), float32] {
  %0 = transpose(%input_1, axes=[0, 3, 1, 2]) /* ty=Tensor[(1, 3, 32, 32), float32] span=Transpose_1:0:0 */;
  %1 = qnn.quantize(%0, 2.00787f /* ty=float32 span=QuantizeLinear_1.model/activation/Relu;model/batch_normalization/FusedBatchNormV3;model/conv2d/BiasAdd/ReadVariableOp/resource;model/conv2d/BiasAdd;model/conv2d_2/Conv2D;model/conv2d/Conv2D1__31:0_scale:0:0 */, 0 /* ty=int32 span=QuantizeLinear_1:0:0 */, out_dtype="int8", axis=1) /* ty=Tensor[(1, 3, 32, 32), int8] span=QuantizeLinear_1:0:0 */;
  %2 = qnn.dequantize(%1, 2.00787f /* ty=float32 span=QuantizeLinear_1.model/activation/Relu;model/batch_normalization/FusedBatchNormV3;model/conv2d/BiasAdd/ReadVariableOp/resource;model/conv2d/BiasAdd;model/conv2d_2/Conv2D;model/conv2d/Conv2D1__31:0_scale:0:0 */, 0 /* ty=int32 span=DequantizeLinear_1:0:0 */, out_dtype="float32", axis=1) /* ty=Tensor[(1, 3, 32, 32), float32] span=DequantizeLinear_1:0:0 */;
  %3 = qnn.dequantize(meta[relay.Constant][0] /* ty=Tensor[(16, 3, 3, 3), int8] span=DequantizeLinear_3.const_fold_opt__108_quantized:0:0 */, 0.000166913f /* ty=float32 span=DequantizeLinear_3.const_fold_opt__108_scale:0:0 */, 0 /* ty=int32 span=DequantizeLinear_3:0:0 */, out_dtype="float32", axis=1) /* ty=Tensor[(16, 3, 3, 3), float32] span=DequantizeLinear_3:0:0 */;
  %4 = nn.conv2d(%2, %3, padding=[1, 1, 1, 1], channels=16, kernel_size=[3, 3]) /* ty=Tensor[(1, 16, 32, 32), float32] span=Conv_1:0:0 */;
  %5 = qnn.dequantize(meta[relay.Constant][1] /* ty=Tensor[(16), int32] span=DequantizeLinear_34.model/activation/Relu;model/batch_normalization/FusedBatchNormV3;model/conv2d/BiasAdd/ReadVariableOp/resource;model/conv2d/BiasAdd;model/conv2d_2/Conv2D;model/conv2d/Conv2D_quantized:0:0 */, 0.00033514f /* ty=float32 span=DequantizeLinear_34.model/activation/Relu;model/batch_normalization/FusedBatchNormV3;model/conv2d/BiasAdd/ReadVariableOp/resource;model/conv2d/BiasAdd;model/conv2d_2/Conv2D;model/conv2d/Conv2D_quantized_scale:0:0 */, 0 /* ty=int32 span=DequantizeLinear_34:0:0 */, out_dtype="float32", axis=0) /* ty=Tensor[(16), float32] span=DequantizeLinear_34:0:0 */;
  %6 = nn.bias_add(%4, %5) /* ty=Tensor[(1, 16, 32, 32), float32] span=Conv_1:0:0 */;
  %7 = qnn.quantize(%6, 0.0790973f /* ty=float32 span=QuantizeLinear_2.model/activation/Relu;model/batch_normalization/FusedBatchNormV3;model/conv2d/BiasAdd/ReadVariableOp/resource;model/conv2d/BiasAdd;model/conv2d_2/Conv2D;model/conv2d/Conv2D1_scale:0:0 */, 0 /* ty=int32 span=QuantizeLinear_2:0:0 */, out_dtype="int8", axis=1) /* ty=Tensor[(1, 16, 32, 32), int8] span=QuantizeLinear_2:0:0 */;
  %8 = nn.relu(%7) /* ty=Tensor[(1, 16, 32, 32), int8] span=Relu_1:0:0 */;
  %9 = qnn.dequantize(%8, 0.0790973f /* ty=float32 span=DequantizeLinear_4.Relu__5:0_scale:0:0 */, 0 /* ty=int32 span=DequantizeLinear_4:0:0 */, out_dtype="float32", axis=1) /* ty=Tensor[(1, 16, 32, 32), float32] span=DequantizeLinear_4:0:0 */;
  %10 = qnn.dequantize(meta[relay.Constant][2] /* ty=Tensor[(16, 16, 3, 3), int8] span=DequantizeLinear_6.const_fold_opt__119_quantized:0:0 */, 0.00775049f /* ty=float32 span=DequantizeLinear_6.const_fold_opt__119_scale:0:0 */, 0 /* ty=int32 span=DequantizeLinear_6:0:0 */, out_dtype="float32", axis=1) /* ty=Tensor[(16, 16, 3, 3), float32] span=DequantizeLinear_6:0:0 */;
  %11 = nn.conv2d(%9, %10, padding=[1, 1, 1, 1], channels=16, kernel_size=[3, 3]) /* ty=Tensor[(1, 16, 32, 32), float32] span=Conv_2:0:0 */;
  %12 = qnn.dequantize(meta[relay.Constant][3] /* ty=Tensor[(16), int32] span=DequantizeLinear_35.model/activation_1/Relu;model/batch_normalization_1/FusedBatchNormV3;model/conv2d_1/BiasAdd/ReadVariableOp/resource;model/conv2d_1/BiasAdd;model/conv2d_2/Conv2D;model/conv2d_1/Conv2D_quantized:0:0 */, 0.000613043f /* ty=float32 span=DequantizeLinear_35.model/activation_1/Relu;model/batch_normalization_1/FusedBatchNormV3;model/conv2d_1/BiasAdd/ReadVariableOp/resource;model/conv2d_1/BiasAdd;model/conv2d_2/Conv2D;model/conv2d_1/Conv2D_quantized_scale:0:0 */, 0 /* ty=int32 span=DequantizeLinear_35:0:0 */, out_dtype="float32", axis=0) /* ty=Tensor[(16), float32] span=DequantizeLinear_35:0:0 */;
  %13 = nn.bias_add(%11, %12) /* ty=Tensor[(1, 16, 32, 32), float32] span=Conv_2:0:0 */;
  %14 = qnn.quantize(%13, 0.20158f /* ty=float32 span=QuantizeLinear_4.model/activation_1/Relu;model/batch_normalization_1/FusedBatchNormV3;model/conv2d_1/BiasAdd/ReadVariableOp/resource;model/conv2d_1/BiasAdd;model/conv2d_2/Conv2D;model/conv2d_1/Conv2D1_scale:0:0 */, 0 /* ty=int32 span=QuantizeLinear_4:0:0 */, out_dtype="int8", axis=1) /* ty=Tensor[(1, 16, 32, 32), int8] span=QuantizeLinear_4:0:0 */;
  %15 = qnn.dequantize(%14, 0.20158f /* ty=float32 span=QuantizeLinear_4.model/activation_1/Relu;model/batch_normalization_1/FusedBatchNormV3;model/conv2d_1/BiasAdd/ReadVariableOp/resource;model/conv2d_1/BiasAdd;model/conv2d_2/Conv2D;model/conv2d_1/Conv2D1_scale:0:0 */, 0 /* ty=int32 span=DequantizeLinear_5:0:0 */, out_dtype="float32", axis=1) /* ty=Tensor[(1, 16, 32, 32), float32] span=DequantizeLinear_5:0:0 */;
  %16 = nn.relu(%15) /* ty=Tensor[(1, 16, 32, 32), float32] span=Relu_2:0:0 */;
  %17 = qnn.quantize(%16, 0.153187f /* ty=float32 span=QuantizeLinear_5.Relu__8:0_scale:0:0 */, 0 /* ty=int32 span=QuantizeLinear_5:0:0 */, out_dtype="int8", axis=1) /* ty=Tensor[(1, 16, 32, 32), int8] span=QuantizeLinear_5:0:0 */;
  %18 = qnn.dequantize(%17, 0.153187f /* ty=float32 span=QuantizeLinear_5.Relu__8:0_scale:0:0 */, 0 /* ty=int32 span=DequantizeLinear_7:0:0 */, out_dtype="float32", axis=1) /* ty=Tensor[(1, 16, 32, 32), float32] span=DequantizeLinear_7:0:0 */;
  %19 = qnn.dequantize(meta[relay.Constant][4] /* ty=Tensor[(16, 16, 3, 3), int8] span=DequantizeLinear_9.const_fold_opt__125_quantized:0:0 */, 0.00407408f /* ty=float32 span=DequantizeLinear_9.const_fold_opt__125_scale:0:0 */, 0 /* ty=int32 span=DequantizeLinear_9:0:0 */, out_dtype="float32", axis=1) /* ty=Tensor[(16, 16, 3, 3), float32] span=DequantizeLinear_9:0:0 */;
  %20 = nn.conv2d(%18, %19, padding=[1, 1, 1, 1], channels=16, kernel_size=[3, 3]) /* ty=Tensor[(1, 16, 32, 32), float32] span=Conv_3:0:0 */;
  %21 = qnn.dequantize(meta[relay.Constant][5] /* ty=Tensor[(16), int32] span=DequantizeLinear_36.model/batch_normalization_2/FusedBatchNormV3;model/conv2d_2/BiasAdd/ReadVariableOp/resource;model/conv2d_2/BiasAdd_quantized:0:0 */, 0.000624096f /* ty=float32 span=DequantizeLinear_36.model/batch_normalization_2/FusedBatchNormV3;model/conv2d_2/BiasAdd/ReadVariableOp/resource;model/conv2d_2/BiasAdd_quantized_scale:0:0 */, 0 /* ty=int32 span=DequantizeLinear_36:0:0 */, out_dtype="float32", axis=0) /* ty=Tensor[(16), float32] span=DequantizeLinear_36:0:0 */;
  %22 = nn.bias_add(%20, %21) /* ty=Tensor[(1, 16, 32, 32), float32] span=Conv_3:0:0 */;
  %23 = qnn.quantize(%22, 0.107967f /* ty=float32 span=QuantizeLinear_6.model/batch_normalization_2/FusedBatchNormV3;model/conv2d_2/BiasAdd/ReadVariableOp/resource;model/conv2d_2/BiasAdd;model/conv2d_2/Conv2D_scale:0:0 */, 0 /* ty=int32 span=QuantizeLinear_6:0:0 */, out_dtype="int8", axis=1) /* ty=Tensor[(1, 16, 32, 32), int8] span=QuantizeLinear_6:0:0 */;
  %24 = qnn.dequantize(%23, 0.107967f /* ty=float32 span=QuantizeLinear_6.model/batch_normalization_2/FusedBatchNormV3;model/conv2d_2/BiasAdd/ReadVariableOp/resource;model/conv2d_2/BiasAdd;model/conv2d_2/Conv2D_scale:0:0 */, 0 /* ty=int32 span=DequantizeLinear_8:0:0 */, out_dtype="float32", axis=1) /* ty=Tensor[(1, 16, 32, 32), float32] span=DequantizeLinear_8:0:0 */;
  %25 = add(%9, %24) /* ty=Tensor[(1, 16, 32, 32), float32] span=Add_1:0:0 */;
  %26 = qnn.quantize(%25, 0.107967f /* ty=float32 span=QuantizeLinear_7.model/activation_2/Relu;model/add/add_scale:0:0 */, 0 /* ty=int32 span=QuantizeLinear_7:0:0 */, out_dtype="int8", axis=1) /* ty=Tensor[(1, 16, 32, 32), int8] span=QuantizeLinear_7:0:0 */;
  %27 = qnn.dequantize(%26, 0.107967f /* ty=float32 span=QuantizeLinear_7.model/activation_2/Relu;model/add/add_scale:0:0 */, 0 /* ty=int32 span=DequantizeLinear_10:0:0 */, out_dtype="float32", axis=1) /* ty=Tensor[(1, 16, 32, 32), float32] span=DequantizeLinear_10:0:0 */;
  %28 = nn.relu(%27) /* ty=Tensor[(1, 16, 32, 32), float32] span=Relu_3:0:0 */;
  %29 = qnn.quantize(%28, 0.102292f /* ty=float32 span=QuantizeLinear_8.Relu__12:0_scale:0:0 */, 0 /* ty=int32 span=QuantizeLinear_8:0:0 */, out_dtype="int8", axis=1) /* ty=Tensor[(1, 16, 32, 32), int8] span=QuantizeLinear_8:0:0 */;
  %30 = qnn.dequantize(%29, 0.102292f /* ty=float32 span=QuantizeLinear_8.Relu__12:0_scale:0:0 */, 0 /* ty=int32 span=DequantizeLinear_11:0:0 */, out_dtype="float32", axis=1) /* ty=Tensor[(1, 16, 32, 32), float32] span=DequantizeLinear_11:0:0 */;
  %31 = qnn.dequantize(meta[relay.Constant][6] /* ty=Tensor[(32, 16, 1, 1), int8] span=DequantizeLinear_18.const_fold_opt__112_quantized:0:0 */, 0.00468374f /* ty=float32 span=DequantizeLinear_18.const_fold_opt__112_scale:0:0 */, 0 /* ty=int32 span=DequantizeLinear_18:0:0 */, out_dtype="float32", axis=1) /* ty=Tensor[(32, 16, 1, 1), float32] span=DequantizeLinear_18:0:0 */;
  %32 = nn.conv2d(%30, %31, strides=[2, 2], padding=[0, 0, 0, 0], channels=32, kernel_size=[1, 1]) /* ty=Tensor[(1, 32, 16, 16), float32] span=Conv_6:0:0 */;
  %33 = qnn.dequantize(meta[relay.Constant][7] /* ty=Tensor[(32), int32] span=DequantizeLinear_39.model/conv2d_5/BiasAdd;model/conv2d_5/Conv2D;model/conv2d_5/BiasAdd/ReadVariableOp/resource_quantized:0:0 */, 0.000479111f /* ty=float32 span=DequantizeLinear_39.model/conv2d_5/BiasAdd;model/conv2d_5/Conv2D;model/conv2d_5/BiasAdd/ReadVariableOp/resource_quantized_scale:0:0 */, 0 /* ty=int32 span=DequantizeLinear_39:0:0 */, out_dtype="float32", axis=0) /* ty=Tensor[(32), float32] span=DequantizeLinear_39:0:0 */;
  %34 = nn.bias_add(%32, %33) /* ty=Tensor[(1, 32, 16, 16), float32] span=Conv_6:0:0 */;
  %35 = qnn.quantize(%34, 0.0508577f /* ty=float32 span=QuantizeLinear_12.model/conv2d_5/BiasAdd;model/conv2d_5/Conv2D;model/conv2d_5/BiasAdd/ReadVariableOp/resource1_scale:0:0 */, 0 /* ty=int32 span=QuantizeLinear_12:0:0 */, out_dtype="int8", axis=1) /* ty=Tensor[(1, 32, 16, 16), int8] span=QuantizeLinear_12:0:0 */;
  %36 = qnn.dequantize(meta[relay.Constant][8] /* ty=Tensor[(32, 16, 3, 3), int8] span=DequantizeLinear_13.const_fold_opt__115_quantized:0:0 */, 0.00341045f /* ty=float32 span=DequantizeLinear_13.const_fold_opt__115_scale:0:0 */, 0 /* ty=int32 span=DequantizeLinear_13:0:0 */, out_dtype="float32", axis=1) /* ty=Tensor[(32, 16, 3, 3), float32] span=DequantizeLinear_13:0:0 */;
  %37 = nn.conv2d(%30, %36, strides=[2, 2], padding=[0, 0, 1, 1], channels=32, kernel_size=[3, 3]) /* ty=Tensor[(1, 32, 16, 16), float32] span=Conv_4:0:0 */;
  %38 = qnn.dequantize(meta[relay.Constant][9] /* ty=Tensor[(32), int32] span=DequantizeLinear_37.model/activation_3/Relu;model/batch_normalization_3/FusedBatchNormV3;model/conv2d_3/BiasAdd/ReadVariableOp/resource;model/conv2d_3/BiasAdd;model/conv2d_5/Conv2D;model/conv2d_3/Conv2D_quantized:0:0 */, 0.000348864f /* ty=float32 span=DequantizeLinear_37.model/activation_3/Relu;model/batch_normalization_3/FusedBatchNormV3;model/conv2d_3/BiasAdd/ReadVariableOp/resource;model/conv2d_3/BiasAdd;model/conv2d_5/Conv2D;model/conv2d_3/Conv2D_quantized_scale:0:0 */, 0 /* ty=int32 span=DequantizeLinear_37:0:0 */, out_dtype="float32", axis=0) /* ty=Tensor[(32), float32] span=DequantizeLinear_37:0:0 */;
  %39 = nn.bias_add(%37, %38) /* ty=Tensor[(1, 32, 16, 16), float32] span=Conv_4:0:0 */;
  %40 = qnn.quantize(%39, 0.0969278f /* ty=float32 span=QuantizeLinear_9.model/activation_3/Relu;model/batch_normalization_3/FusedBatchNormV3;model/conv2d_3/BiasAdd/ReadVariableOp/resource;model/conv2d_3/BiasAdd;model/conv2d_5/Conv2D;model/conv2d_3/Conv2D1_scale:0:0 */, 0 /* ty=int32 span=QuantizeLinear_9:0:0 */, out_dtype="int8", axis=1) /* ty=Tensor[(1, 32, 16, 16), int8] span=QuantizeLinear_9:0:0 */;
  %41 = qnn.dequantize(%40, 0.0969278f /* ty=float32 span=QuantizeLinear_9.model/activation_3/Relu;model/batch_normalization_3/FusedBatchNormV3;model/conv2d_3/BiasAdd/ReadVariableOp/resource;model/conv2d_3/BiasAdd;model/conv2d_5/Conv2D;model/conv2d_3/Conv2D1_scale:0:0 */, 0 /* ty=int32 span=DequantizeLinear_12:0:0 */, out_dtype="float32", axis=1) /* ty=Tensor[(1, 32, 16, 16), float32] span=DequantizeLinear_12:0:0 */;
  %42 = nn.relu(%41) /* ty=Tensor[(1, 32, 16, 16), float32] span=Relu_4:0:0 */;
  %43 = qnn.quantize(%42, 0.0917053f /* ty=float32 span=QuantizeLinear_10.Relu__14:0_scale:0:0 */, 0 /* ty=int32 span=QuantizeLinear_10:0:0 */, out_dtype="int8", axis=1) /* ty=Tensor[(1, 32, 16, 16), int8] span=QuantizeLinear_10:0:0 */;
  %44 = qnn.dequantize(%43, 0.0917053f /* ty=float32 span=QuantizeLinear_10.Relu__14:0_scale:0:0 */, 0 /* ty=int32 span=DequantizeLinear_14:0:0 */, out_dtype="float32", axis=1) /* ty=Tensor[(1, 32, 16, 16), float32] span=DequantizeLinear_14:0:0 */;
  %45 = qnn.dequantize(meta[relay.Constant][10] /* ty=Tensor[(32, 32, 3, 3), int8] span=DequantizeLinear_16.const_fold_opt__121_quantized:0:0 */, 0.00603094f /* ty=float32 span=DequantizeLinear_16.const_fold_opt__121_scale:0:0 */, 0 /* ty=int32 span=DequantizeLinear_16:0:0 */, out_dtype="float32", axis=1) /* ty=Tensor[(32, 32, 3, 3), float32] span=DequantizeLinear_16:0:0 */;
  %46 = nn.conv2d(%44, %45, padding=[1, 1, 1, 1], channels=32, kernel_size=[3, 3]) /* ty=Tensor[(1, 32, 16, 16), float32] span=Conv_5:0:0 */;
  %47 = qnn.dequantize(meta[relay.Constant][11] /* ty=Tensor[(32), int32] span=DequantizeLinear_38.model/batch_normalization_4/FusedBatchNormV3;model/conv2d_4/BiasAdd/ReadVariableOp/resource;model/conv2d_4/BiasAdd_quantized:0:0 */, 0.000553069f /* ty=float32 span=DequantizeLinear_38.model/batch_normalization_4/FusedBatchNormV3;model/conv2d_4/BiasAdd/ReadVariableOp/resource;model/conv2d_4/BiasAdd_quantized_scale:0:0 */, 0 /* ty=int32 span=DequantizeLinear_38:0:0 */, out_dtype="float32", axis=0) /* ty=Tensor[(32), float32] span=DequantizeLinear_38:0:0 */;
  %48 = nn.bias_add(%46, %47) /* ty=Tensor[(1, 32, 16, 16), float32] span=Conv_5:0:0 */;
  %49 = qnn.quantize(%48, 0.117619f /* ty=float32 span=QuantizeLinear_11.model/batch_normalization_4/FusedBatchNormV3;model/conv2d_4/BiasAdd/ReadVariableOp/resource;model/conv2d_4/BiasAdd;model/conv2d_5/Conv2D;model/conv2d_4/Conv2D_scale:0:0 */, 0 /* ty=int32 span=QuantizeLinear_11:0:0 */, out_dtype="int8", axis=1) /* ty=Tensor[(1, 32, 16, 16), int8] span=QuantizeLinear_11:0:0 */;
  %50 = qnn.dequantize(%35, 0.0508577f /* ty=float32 span=QuantizeLinear_12.model/conv2d_5/BiasAdd;model/conv2d_5/Conv2D;model/conv2d_5/BiasAdd/ReadVariableOp/resource1_scale:0:0 */, 0 /* ty=int32 span=DequantizeLinear_17:0:0 */, out_dtype="float32", axis=1) /* ty=Tensor[(1, 32, 16, 16), float32] span=DequantizeLinear_17:0:0 */;
  %51 = qnn.dequantize(%49, 0.117619f /* ty=float32 span=QuantizeLinear_11.model/batch_normalization_4/FusedBatchNormV3;model/conv2d_4/BiasAdd/ReadVariableOp/resource;model/conv2d_4/BiasAdd;model/conv2d_5/Conv2D;model/conv2d_4/Conv2D_scale:0:0 */, 0 /* ty=int32 span=DequantizeLinear_15:0:0 */, out_dtype="float32", axis=1) /* ty=Tensor[(1, 32, 16, 16), float32] span=DequantizeLinear_15:0:0 */;
  %52 = add(%50, %51) /* ty=Tensor[(1, 32, 16, 16), float32] span=Add_2:0:0 */;
  %53 = qnn.quantize(%52, 0.137376f /* ty=float32 span=QuantizeLinear_13.model/activation_4/Relu;model/add_1/add_scale:0:0 */, 0 /* ty=int32 span=QuantizeLinear_13:0:0 */, out_dtype="int8", axis=1) /* ty=Tensor[(1, 32, 16, 16), int8] span=QuantizeLinear_13:0:0 */;
  %54 = qnn.dequantize(%53, 0.137376f /* ty=float32 span=QuantizeLinear_13.model/activation_4/Relu;model/add_1/add_scale:0:0 */, 0 /* ty=int32 span=DequantizeLinear_19:0:0 */, out_dtype="float32", axis=1) /* ty=Tensor[(1, 32, 16, 16), float32] span=DequantizeLinear_19:0:0 */;
  %55 = nn.relu(%54) /* ty=Tensor[(1, 32, 16, 16), float32] span=Relu_5:0:0 */;
  %56 = qnn.quantize(%55, 0.106892f /* ty=float32 span=QuantizeLinear_14.Relu__19:0_scale:0:0 */, 0 /* ty=int32 span=QuantizeLinear_14:0:0 */, out_dtype="int8", axis=1) /* ty=Tensor[(1, 32, 16, 16), int8] span=QuantizeLinear_14:0:0 */;
  %57 = qnn.dequantize(%56, 0.106892f /* ty=float32 span=QuantizeLinear_14.Relu__19:0_scale:0:0 */, 0 /* ty=int32 span=DequantizeLinear_20:0:0 */, out_dtype="float32", axis=1) /* ty=Tensor[(1, 32, 16, 16), float32] span=DequantizeLinear_20:0:0 */;
  %58 = qnn.dequantize(meta[relay.Constant][12] /* ty=Tensor[(64, 32, 1, 1), int8] span=DequantizeLinear_27.const_fold_opt__117_quantized:0:0 */, 0.00545248f /* ty=float32 span=DequantizeLinear_27.const_fold_opt__117_scale:0:0 */, 0 /* ty=int32 span=DequantizeLinear_27:0:0 */, out_dtype="float32", axis=1) /* ty=Tensor[(64, 32, 1, 1), float32] span=DequantizeLinear_27:0:0 */;
  %59 = nn.conv2d(%57, %58, strides=[2, 2], padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(1, 64, 8, 8), float32] span=Conv_9:0:0 */;
  %60 = qnn.dequantize(meta[relay.Constant][13] /* ty=Tensor[(64), int32] span=DequantizeLinear_42.model/conv2d_8/BiasAdd;model/conv2d_8/Conv2D;model/conv2d_8/BiasAdd/ReadVariableOp/resource_quantized:0:0 */, 0.000582824f /* ty=float32 span=DequantizeLinear_42.model/conv2d_8/BiasAdd;model/conv2d_8/Conv2D;model/conv2d_8/BiasAdd/ReadVariableOp/resource_quantized_scale:0:0 */, 0 /* ty=int32 span=DequantizeLinear_42:0:0 */, out_dtype="float32", axis=0) /* ty=Tensor[(64), float32] span=DequantizeLinear_42:0:0 */;
  %61 = nn.bias_add(%59, %60) /* ty=Tensor[(1, 64, 8, 8), float32] span=Conv_9:0:0 */;
  %62 = qnn.quantize(%61, 0.109617f /* ty=float32 span=QuantizeLinear_18.model/conv2d_8/BiasAdd;model/conv2d_8/Conv2D;model/conv2d_8/BiasAdd/ReadVariableOp/resource1_scale:0:0 */, 0 /* ty=int32 span=QuantizeLinear_18:0:0 */, out_dtype="int8", axis=1) /* ty=Tensor[(1, 64, 8, 8), int8] span=QuantizeLinear_18:0:0 */;
  %63 = qnn.dequantize(meta[relay.Constant][14] /* ty=Tensor[(64, 32, 3, 3), int8] span=DequantizeLinear_22.const_fold_opt__120_quantized:0:0 */, 0.00147699f /* ty=float32 span=DequantizeLinear_22.const_fold_opt__120_scale:0:0 */, 0 /* ty=int32 span=DequantizeLinear_22:0:0 */, out_dtype="float32", axis=1) /* ty=Tensor[(64, 32, 3, 3), float32] span=DequantizeLinear_22:0:0 */;
  %64 = nn.conv2d(%57, %63, strides=[2, 2], padding=[0, 0, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 8, 8), float32] span=Conv_7:0:0 */;
  %65 = qnn.dequantize(meta[relay.Constant][15] /* ty=Tensor[(64), int32] span=DequantizeLinear_40.model/activation_5/Relu;model/batch_normalization_5/FusedBatchNormV3;model/conv2d_6/BiasAdd/ReadVariableOp/resource;model/conv2d_6/BiasAdd;model/conv2d_8/Conv2D;model/conv2d_6/Conv2D_quantized:0:0 */, 0.000157878f /* ty=float32 span=DequantizeLinear_40.model/activation_5/Relu;model/batch_normalization_5/FusedBatchNormV3;model/conv2d_6/BiasAdd/ReadVariableOp/resource;model/conv2d_6/BiasAdd;model/conv2d_8/Conv2D;model/conv2d_6/Conv2D_quantized_scale:0:0 */, 0 /* ty=int32 span=DequantizeLinear_40:0:0 */, out_dtype="float32", axis=0) /* ty=Tensor[(64), float32] span=DequantizeLinear_40:0:0 */;
  %66 = nn.bias_add(%64, %65) /* ty=Tensor[(1, 64, 8, 8), float32] span=Conv_7:0:0 */;
  %67 = qnn.quantize(%66, 0.0774434f /* ty=float32 span=QuantizeLinear_15.model/activation_5/Relu;model/batch_normalization_5/FusedBatchNormV3;model/conv2d_6/BiasAdd/ReadVariableOp/resource;model/conv2d_6/BiasAdd;model/conv2d_8/Conv2D;model/conv2d_6/Conv2D1_scale:0:0 */, 0 /* ty=int32 span=QuantizeLinear_15:0:0 */, out_dtype="int8", axis=1) /* ty=Tensor[(1, 64, 8, 8), int8] span=QuantizeLinear_15:0:0 */;
  %68 = qnn.dequantize(%67, 0.0774434f /* ty=float32 span=QuantizeLinear_15.model/activation_5/Relu;model/batch_normalization_5/FusedBatchNormV3;model/conv2d_6/BiasAdd/ReadVariableOp/resource;model/conv2d_6/BiasAdd;model/conv2d_8/Conv2D;model/conv2d_6/Conv2D1_scale:0:0 */, 0 /* ty=int32 span=DequantizeLinear_21:0:0 */, out_dtype="float32", axis=1) /* ty=Tensor[(1, 64, 8, 8), float32] span=DequantizeLinear_21:0:0 */;
  %69 = nn.relu(%68) /* ty=Tensor[(1, 64, 8, 8), float32] span=Relu_6:0:0 */;
  %70 = qnn.quantize(%69, 0.0571244f /* ty=float32 span=QuantizeLinear_16.Relu__21:0_scale:0:0 */, 0 /* ty=int32 span=QuantizeLinear_16:0:0 */, out_dtype="int8", axis=1) /* ty=Tensor[(1, 64, 8, 8), int8] span=QuantizeLinear_16:0:0 */;
  %71 = qnn.dequantize(%70, 0.0571244f /* ty=float32 span=QuantizeLinear_16.Relu__21:0_scale:0:0 */, 0 /* ty=int32 span=DequantizeLinear_23:0:0 */, out_dtype="float32", axis=1) /* ty=Tensor[(1, 64, 8, 8), float32] span=DequantizeLinear_23:0:0 */;
  %72 = qnn.dequantize(meta[relay.Constant][16] /* ty=Tensor[(64, 64, 3, 3), int8] span=DequantizeLinear_25.const_fold_opt__123_quantized:0:0 */, 0.0144741f /* ty=float32 span=DequantizeLinear_25.const_fold_opt__123_scale:0:0 */, 0 /* ty=int32 span=DequantizeLinear_25:0:0 */, out_dtype="float32", axis=1) /* ty=Tensor[(64, 64, 3, 3), float32] span=DequantizeLinear_25:0:0 */;
  %73 = nn.conv2d(%71, %72, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 8, 8), float32] span=Conv_8:0:0 */;
  %74 = qnn.dequantize(meta[relay.Constant][17] /* ty=Tensor[(64), int32] span=DequantizeLinear_41.model/batch_normalization_6/FusedBatchNormV3;model/conv2d_7/BiasAdd/ReadVariableOp/resource;model/conv2d_7/BiasAdd_quantized:0:0 */, 0.000826827f /* ty=float32 span=DequantizeLinear_41.model/batch_normalization_6/FusedBatchNormV3;model/conv2d_7/BiasAdd/ReadVariableOp/resource;model/conv2d_7/BiasAdd_quantized_scale:0:0 */, 0 /* ty=int32 span=DequantizeLinear_41:0:0 */, out_dtype="float32", axis=0) /* ty=Tensor[(64), float32] span=DequantizeLinear_41:0:0 */;
  %75 = nn.bias_add(%73, %74) /* ty=Tensor[(1, 64, 8, 8), float32] span=Conv_8:0:0 */;
  %76 = qnn.quantize(%75, 0.220277f /* ty=float32 span=QuantizeLinear_17.model/batch_normalization_6/FusedBatchNormV3;model/conv2d_7/BiasAdd/ReadVariableOp/resource;model/conv2d_7/BiasAdd;model/conv2d_8/Conv2D;model/conv2d_7/Conv2D_scale:0:0 */, 0 /* ty=int32 span=QuantizeLinear_17:0:0 */, out_dtype="int8", axis=1) /* ty=Tensor[(1, 64, 8, 8), int8] span=QuantizeLinear_17:0:0 */;
  %77 = qnn.dequantize(%62, 0.109617f /* ty=float32 span=QuantizeLinear_18.model/conv2d_8/BiasAdd;model/conv2d_8/Conv2D;model/conv2d_8/BiasAdd/ReadVariableOp/resource1_scale:0:0 */, 0 /* ty=int32 span=DequantizeLinear_26:0:0 */, out_dtype="float32", axis=1) /* ty=Tensor[(1, 64, 8, 8), float32] span=DequantizeLinear_26:0:0 */;
  %78 = qnn.dequantize(%76, 0.220277f /* ty=float32 span=QuantizeLinear_17.model/batch_normalization_6/FusedBatchNormV3;model/conv2d_7/BiasAdd/ReadVariableOp/resource;model/conv2d_7/BiasAdd;model/conv2d_8/Conv2D;model/conv2d_7/Conv2D_scale:0:0 */, 0 /* ty=int32 span=DequantizeLinear_24:0:0 */, out_dtype="float32", axis=1) /* ty=Tensor[(1, 64, 8, 8), float32] span=DequantizeLinear_24:0:0 */;
  %79 = add(%77, %78) /* ty=Tensor[(1, 64, 8, 8), float32] span=Add_3:0:0 */;
  %80 = qnn.quantize(%79, 0.258798f /* ty=float32 span=QuantizeLinear_19.model/activation_6/Relu;model/add_2/add_scale:0:0 */, 0 /* ty=int32 span=QuantizeLinear_19:0:0 */, out_dtype="int8", axis=1) /* ty=Tensor[(1, 64, 8, 8), int8] span=QuantizeLinear_19:0:0 */;
  %81 = qnn.dequantize(%80, 0.258798f /* ty=float32 span=QuantizeLinear_19.model/activation_6/Relu;model/add_2/add_scale:0:0 */, 0 /* ty=int32 span=DequantizeLinear_28:0:0 */, out_dtype="float32", axis=1) /* ty=Tensor[(1, 64, 8, 8), float32] span=DequantizeLinear_28:0:0 */;
  %82 = nn.relu(%81) /* ty=Tensor[(1, 64, 8, 8), float32] span=Relu_7:0:0 */;
  %83 = qnn.quantize(%82, 0.255139f /* ty=float32 span=QuantizeLinear_20.Relu__26:0_scale:0:0 */, 0 /* ty=int32 span=QuantizeLinear_20:0:0 */, out_dtype="int8", axis=1) /* ty=Tensor[(1, 64, 8, 8), int8] span=QuantizeLinear_20:0:0 */;
  %84 = qnn.dequantize(%83, 0.255139f /* ty=float32 span=QuantizeLinear_20.Relu__26:0_scale:0:0 */, 0 /* ty=int32 span=DequantizeLinear_29:0:0 */, out_dtype="float32", axis=1) /* ty=Tensor[(1, 64, 8, 8), float32] span=DequantizeLinear_29:0:0 */;
  %85 = nn.avg_pool2d(%84, pool_size=[8, 8], strides=[8, 8], padding=[0, 0, 0, 0]) /* ty=Tensor[(1, 64, 1, 1), float32] span=AveragePool_1:0:0 */;
  %86 = qnn.quantize(%85, 0.255139f /* ty=float32 span=QuantizeLinear_20.Relu__26:0_scale:0:0 */, 0 /* ty=int32 span=QuantizeLinear_22:0:0 */, out_dtype="int8", axis=1) /* ty=Tensor[(1, 64, 1, 1), int8] span=QuantizeLinear_22:0:0 */;
  %87 = qnn.dequantize(%86, 0.255139f /* ty=float32 span=QuantizeLinear_20.Relu__26:0_scale:0:0 */, 0 /* ty=int32 span=DequantizeLinear_32:0:0 */, out_dtype="float32", axis=1) /* ty=Tensor[(1, 64, 1, 1), float32] span=DequantizeLinear_32:0:0 */;
  %88 = reshape(%87, newshape=[-1, 64]) /* ty=Tensor[(1, 64), float32] span=Reshape_1:0:0 */;
  %89 = qnn.quantize(%88, 0.255139f /* ty=float32 span=QuantizeLinear_20.Relu__26:0_scale:0:0 */, 0 /* ty=int32 span=QuantizeLinear_23:0:0 */, out_dtype="int8", axis=1) /* ty=Tensor[(1, 64), int8] span=QuantizeLinear_23:0:0 */;
  %90 = qnn.dequantize(meta[relay.Constant][18] /* ty=Tensor[(64, 10), int8] span=DequantizeLinear_31.const_fold_opt__110_quantized:0:0 */, 0.0305544f /* ty=float32 span=DequantizeLinear_31.const_fold_opt__110_scale:0:0 */, 0 /* ty=int32 span=DequantizeLinear_31:0:0 */, out_dtype="float32", axis=1) /* ty=Tensor[(64, 10), float32] span=DequantizeLinear_31:0:0 */;
  %91 = qnn.dequantize(%89, 0.255139f /* ty=float32 span=QuantizeLinear_20.Relu__26:0_scale:0:0 */, 0 /* ty=int32 span=DequantizeLinear_33:0:0 */, out_dtype="float32", axis=1) /* ty=Tensor[(1, 64), float32] span=DequantizeLinear_33:0:0 */;
  %92 = transpose(%90, axes=[1, 0]) /* ty=Tensor[(10, 64), float32] span=Gemm_1:0:0 */;
  %93 = nn.dense(%91, %92, units=10) /* ty=Tensor[(1, 10), float32] span=Gemm_1:0:0 */;
  %94 = qnn.dequantize(meta[relay.Constant][19] /* ty=Tensor[(10), int32] span=DequantizeLinear_43.model/dense/BiasAdd/ReadVariableOp/resource_quantized:0:0 */, 0.00779561f /* ty=float32 span=DequantizeLinear_43.model/dense/BiasAdd/ReadVariableOp/resource_quantized_scale:0:0 */, 0 /* ty=int32 span=DequantizeLinear_43:0:0 */, out_dtype="float32", axis=0) /* ty=Tensor[(10), float32] span=DequantizeLinear_43:0:0 */;
  %95 = add(%93, %94) /* ty=Tensor[(1, 10), float32] span=Gemm_1:0:0 */;
  %96 = qnn.quantize(%95, 0.205823f /* ty=float32 span=QuantizeLinear_21.Add__29:0_scale:0:0 */, 0 /* ty=int32 span=QuantizeLinear_21:0:0 */, out_dtype="int8", axis=1) /* ty=Tensor[(1, 10), int8] span=QuantizeLinear_21:0:0 */;
  %97 = qnn.dequantize(%96, 0.205823f /* ty=float32 span=QuantizeLinear_21.Add__29:0_scale:0:0 */, 0 /* ty=int32 span=DequantizeLinear_30:0:0 */, out_dtype="float32", axis=1) /* ty=Tensor[(1, 10), float32] span=DequantizeLinear_30:0:0 */;
  nn.softmax(%97, axis=1) /* ty=Tensor[(1, 10), float32] span=Softmax_1:0:0 */
}

