def @main(%input: Tensor[(1, 3, 224, 224), float32] /* ty=Tensor[(1, 3, 224, 224), float32] span=input_QuantizeLinear.input:0:0 */) -> Tensor[(1, 1000), float32] {
  %0 = qnn.quantize(%input, 0.0186584f /* ty=float32 span=input_QuantizeLinear.input_scale:0:0 */, 114 /* ty=int32 span=input_QuantizeLinear:0:0 */, out_dtype="uint8", axis=0) /* ty=Tensor[(1, 3, 224, 224), uint8] span=input_QuantizeLinear:0:0 */;
  %1 = layout_transform(%0, src_layout="NCHW", dst_layout="NHWC") /* ty=Tensor[(1, 224, 224, 3), uint8] */;
  %2 = qnn.conv2d(%1, meta[relay.Constant][0] /* ty=Tensor[(3, 3, 3, 24), int8] */, 114 /* ty=int32 span=fused Conv_0_quant:0:0 */, meta[relay.Constant][1] /* ty=Tensor[(24), int32] span=fused Conv_0_quant:0:0 */, 0.0186584f /* ty=float32 span=fused Conv_0_quant:0:0 */, meta[relay.Constant][2] /* ty=Tensor[(24), float32] span=fused Conv_0_quant:0:0 */, strides=[2, 2], padding=[1, 1, 1, 1], channels=24, kernel_size=[3, 3], data_layout="NHWC", kernel_layout="HWIO", out_dtype="int32") /* ty=Tensor[(1, 112, 112, 24), int32] span=fused Conv_0_quant:0:0 */;
  %3 = add(%2, meta[relay.Constant][3] /* ty=Tensor[(1, 1, 1, 24), int32] */) /* ty=Tensor[(1, 112, 112, 24), int32] */;
  %4 = qnn.requantize(%3, meta[relay.Constant][4] /* ty=Tensor[(24), float32] span=fused Conv_0_quant:0:0 */, 0 /* ty=int32 span=fused Conv_0_quant:0:0 */, 0.0502939f /* ty=float32 span=fused Conv_0_quant:0:0 */, 0 /* ty=int32 span=fused Conv_0_quant:0:0 */, axis=3, rounding="UPWARD", compute_dtype="int64", out_dtype="uint8") /* ty=Tensor[(1, 112, 112, 24), uint8] span=fused Conv_0_quant:0:0 */;
  %5 = nn.max_pool2d(%4, pool_size=[3, 3], strides=[2, 2], padding=[1, 1, 1, 1], layout="NHWC") /* ty=Tensor[(1, 56, 56, 24), uint8] span=MaxPool_3_quant:0:0 */;
  %6 = qnn.conv2d(%5, meta[relay.Constant][5] /* ty=Tensor[(3, 3, 24, 1), int8] */, 0 /* ty=int32 span=Conv_4_quant:0:0 */, meta[relay.Constant][6] /* ty=Tensor[(24), int32] span=Conv_4_quant:0:0 */, 0.0502939f /* ty=float32 span=Conv_4_quant:0:0 */, meta[relay.Constant][7] /* ty=Tensor[(24), float32] span=Conv_4_quant:0:0 */, strides=[2, 2], padding=[1, 1, 1, 1], groups=24, channels=24, kernel_size=[3, 3], data_layout="NHWC", kernel_layout="HWOI", out_dtype="int32") /* ty=Tensor[(1, 28, 28, 24), int32] span=Conv_4_quant:0:0 */;
  %7 = add(%6, meta[relay.Constant][8] /* ty=Tensor[(1, 1, 1, 24), int32] */) /* ty=Tensor[(1, 28, 28, 24), int32] */;
  %8 = qnn.requantize(%7, meta[relay.Constant][9] /* ty=Tensor[(24), float32] span=Conv_4_quant:0:0 */, 0 /* ty=int32 span=Conv_4_quant:0:0 */, 0.0632944f /* ty=float32 span=Conv_4_quant:0:0 */, 123 /* ty=int32 span=Conv_4_quant:0:0 */, axis=3, rounding="UPWARD", compute_dtype="int64", out_dtype="uint8") /* ty=Tensor[(1, 28, 28, 24), uint8] span=Conv_4_quant:0:0 */;
  %9 = qnn.conv2d(%8, meta[relay.Constant][10] /* ty=Tensor[(1, 1, 24, 58), int8] */, 123 /* ty=int32 span=fused Conv_6_quant:0:0 */, meta[relay.Constant][11] /* ty=Tensor[(58), int32] span=fused Conv_6_quant:0:0 */, 0.0632944f /* ty=float32 span=fused Conv_6_quant:0:0 */, meta[relay.Constant][12] /* ty=Tensor[(58), float32] span=fused Conv_6_quant:0:0 */, padding=[0, 0, 0, 0], channels=58, kernel_size=[1, 1], data_layout="NHWC", kernel_layout="HWIO", out_dtype="int32") /* ty=Tensor[(1, 28, 28, 58), int32] span=fused Conv_6_quant:0:0 */;
  %10 = add(%9, meta[relay.Constant][13] /* ty=Tensor[(1, 1, 1, 58), int32] */) /* ty=Tensor[(1, 28, 28, 58), int32] */;
  %11 = qnn.requantize(%10, meta[relay.Constant][14] /* ty=Tensor[(58), float32] span=fused Conv_6_quant:0:0 */, 0 /* ty=int32 span=fused Conv_6_quant:0:0 */, 0.0322115f /* ty=float32 span=fused Conv_6_quant:0:0 */, 0 /* ty=int32 span=fused Conv_6_quant:0:0 */, axis=3, rounding="UPWARD", compute_dtype="int64", out_dtype="uint8") /* ty=Tensor[(1, 28, 28, 58), uint8] span=fused Conv_6_quant:0:0 */;
  %12 = qnn.conv2d(%5, meta[relay.Constant][15] /* ty=Tensor[(1, 1, 24, 58), int8] */, 0 /* ty=int32 span=fused Conv_9_quant:0:0 */, meta[relay.Constant][16] /* ty=Tensor[(58), int32] span=fused Conv_9_quant:0:0 */, 0.0502939f /* ty=float32 span=fused Conv_9_quant:0:0 */, meta[relay.Constant][17] /* ty=Tensor[(58), float32] span=fused Conv_9_quant:0:0 */, padding=[0, 0, 0, 0], channels=58, kernel_size=[1, 1], data_layout="NHWC", kernel_layout="HWIO", out_dtype="int32") /* ty=Tensor[(1, 56, 56, 58), int32] span=fused Conv_9_quant:0:0 */;
  %13 = add(%12, meta[relay.Constant][18] /* ty=Tensor[(1, 1, 1, 58), int32] */) /* ty=Tensor[(1, 56, 56, 58), int32] */;
  %14 = qnn.requantize(%13, meta[relay.Constant][19] /* ty=Tensor[(58), float32] span=fused Conv_9_quant:0:0 */, 0 /* ty=int32 span=fused Conv_9_quant:0:0 */, 0.0212741f /* ty=float32 span=fused Conv_9_quant:0:0 */, 0 /* ty=int32 span=fused Conv_9_quant:0:0 */, axis=3, rounding="UPWARD", compute_dtype="int64", out_dtype="uint8") /* ty=Tensor[(1, 56, 56, 58), uint8] span=fused Conv_9_quant:0:0 */;
  %15 = qnn.conv2d(%14, meta[relay.Constant][20] /* ty=Tensor[(3, 3, 58, 1), int8] */, 0 /* ty=int32 span=Conv_12_quant:0:0 */, meta[relay.Constant][21] /* ty=Tensor[(58), int32] span=Conv_12_quant:0:0 */, 0.0212741f /* ty=float32 span=Conv_12_quant:0:0 */, meta[relay.Constant][22] /* ty=Tensor[(58), float32] span=Conv_12_quant:0:0 */, strides=[2, 2], padding=[1, 1, 1, 1], groups=58, channels=58, kernel_size=[3, 3], data_layout="NHWC", kernel_layout="HWOI", out_dtype="int32") /* ty=Tensor[(1, 28, 28, 58), int32] span=Conv_12_quant:0:0 */;
  %16 = add(%15, meta[relay.Constant][23] /* ty=Tensor[(1, 1, 1, 58), int32] */) /* ty=Tensor[(1, 28, 28, 58), int32] */;
  %17 = qnn.requantize(%16, meta[relay.Constant][24] /* ty=Tensor[(58), float32] span=Conv_12_quant:0:0 */, 0 /* ty=int32 span=Conv_12_quant:0:0 */, 0.0632021f /* ty=float32 span=Conv_12_quant:0:0 */, 128 /* ty=int32 span=Conv_12_quant:0:0 */, axis=3, rounding="UPWARD", compute_dtype="int64", out_dtype="uint8") /* ty=Tensor[(1, 28, 28, 58), uint8] span=Conv_12_quant:0:0 */;
  %18 = qnn.conv2d(%17, meta[relay.Constant][25] /* ty=Tensor[(1, 1, 58, 58), int8] */, 128 /* ty=int32 span=fused Conv_14_quant:0:0 */, meta[relay.Constant][26] /* ty=Tensor[(58), int32] span=fused Conv_14_quant:0:0 */, 0.0632021f /* ty=float32 span=fused Conv_14_quant:0:0 */, meta[relay.Constant][27] /* ty=Tensor[(58), float32] span=fused Conv_14_quant:0:0 */, padding=[0, 0, 0, 0], channels=58, kernel_size=[1, 1], data_layout="NHWC", kernel_layout="HWIO", out_dtype="int32") /* ty=Tensor[(1, 28, 28, 58), int32] span=fused Conv_14_quant:0:0 */;
  %19 = add(%18, meta[relay.Constant][28] /* ty=Tensor[(1, 1, 1, 58), int32] */) /* ty=Tensor[(1, 28, 28, 58), int32] */;
  %20 = qnn.requantize(%19, meta[relay.Constant][29] /* ty=Tensor[(58), float32] span=fused Conv_14_quant:0:0 */, 0 /* ty=int32 span=fused Conv_14_quant:0:0 */, 0.0226402f /* ty=float32 span=fused Conv_14_quant:0:0 */, 0 /* ty=int32 span=fused Conv_14_quant:0:0 */, axis=3, rounding="UPWARD", compute_dtype="int64", out_dtype="uint8") /* ty=Tensor[(1, 28, 28, 58), uint8] span=fused Conv_14_quant:0:0 */;
  %21 = qnn.dequantize(%11, 0.0322115f /* ty=float32 span=fused Conv_6_quant.347_scale:0:0 */, 0 /* ty=int32 span=347_DequantizeLinear:0:0 */, out_dtype="float32", axis=0) /* ty=Tensor[(1, 28, 28, 58), float32] span=347_DequantizeLinear:0:0 */;
  %22 = qnn.dequantize(%20, 0.0226402f /* ty=float32 span=fused Conv_14_quant.355_scale:0:0 */, 0 /* ty=int32 span=355_DequantizeLinear:0:0 */, out_dtype="float32", axis=0) /* ty=Tensor[(1, 28, 28, 58), float32] span=355_DequantizeLinear:0:0 */;
  %23 = (%21, %22) /* ty=(Tensor[(1, 28, 28, 58), float32], Tensor[(1, 28, 28, 58), float32]) span=Concat_17:0:0 */;
  %24 = concatenate(%23, axis=3) /* ty=Tensor[(1, 28, 28, 116), float32] span=Concat_17:0:0 */;
  %25 = layout_transform(%24, src_layout="NHWC", dst_layout="NCHW") /* ty=Tensor[(1, 116, 28, 28), float32] */;
  %26 = reshape(%25, newshape=[1, 2, 58, 28, 28]) /* ty=Tensor[(1, 2, 58, 28, 28), float32] span=Reshape_19:0:0 */;
  %27 = transpose(%26, axes=[0, 2, 1, 3, 4]) /* ty=Tensor[(1, 58, 2, 28, 28), float32] span=Transpose_20:0:0 */;
  %28 = reshape(%27, newshape=[1, -1, 28, 28]) /* ty=Tensor[(1, 116, 28, 28), float32] span=Reshape_22:0:0 */;
  %29 = qnn.quantize(%28, 0.0322115f /* ty=float32 span=361_QuantizeLinear.361_scale:0:0 */, 0 /* ty=int32 span=361_QuantizeLinear:0:0 */, out_dtype="uint8", axis=0) /* ty=Tensor[(1, 116, 28, 28), uint8] span=361_QuantizeLinear:0:0 */;
  %30 = split(%29, indices_or_sections=[58], axis=1) /* ty=(Tensor[(1, 58, 28, 28), uint8], Tensor[(1, 58, 28, 28), uint8]) span=Split_23_quant:0:0 */;
  %31 = %30.0 /* ty=Tensor[(1, 58, 28, 28), uint8] */;
  %32 = %30.1 /* ty=Tensor[(1, 58, 28, 28), uint8] */;
  %33 = layout_transform(%32, src_layout="NCHW", dst_layout="NHWC") /* ty=Tensor[(1, 28, 28, 58), uint8] */;
  %34 = qnn.conv2d(%33, meta[relay.Constant][30] /* ty=Tensor[(1, 1, 58, 58), int8] */, 0 /* ty=int32 span=fused Conv_24_quant:0:0 */, meta[relay.Constant][31] /* ty=Tensor[(58), int32] span=fused Conv_24_quant:0:0 */, 0.0322115f /* ty=float32 span=fused Conv_24_quant:0:0 */, meta[relay.Constant][32] /* ty=Tensor[(58), float32] span=fused Conv_24_quant:0:0 */, padding=[0, 0, 0, 0], channels=58, kernel_size=[1, 1], data_layout="NHWC", kernel_layout="HWIO", out_dtype="int32") /* ty=Tensor[(1, 28, 28, 58), int32] span=fused Conv_24_quant:0:0 */;
  %35 = add(%34, meta[relay.Constant][33] /* ty=Tensor[(1, 1, 1, 58), int32] */) /* ty=Tensor[(1, 28, 28, 58), int32] */;
  %36 = qnn.requantize(%35, meta[relay.Constant][34] /* ty=Tensor[(58), float32] span=fused Conv_24_quant:0:0 */, 0 /* ty=int32 span=fused Conv_24_quant:0:0 */, 0.011695f /* ty=float32 span=fused Conv_24_quant:0:0 */, 0 /* ty=int32 span=fused Conv_24_quant:0:0 */, axis=3, rounding="UPWARD", compute_dtype="int64", out_dtype="uint8") /* ty=Tensor[(1, 28, 28, 58), uint8] span=fused Conv_24_quant:0:0 */;
  %37 = qnn.conv2d(%36, meta[relay.Constant][35] /* ty=Tensor[(3, 3, 58, 1), int8] */, 0 /* ty=int32 span=Conv_27_quant:0:0 */, meta[relay.Constant][36] /* ty=Tensor[(58), int32] span=Conv_27_quant:0:0 */, 0.011695f /* ty=float32 span=Conv_27_quant:0:0 */, meta[relay.Constant][37] /* ty=Tensor[(58), float32] span=Conv_27_quant:0:0 */, padding=[1, 1, 1, 1], groups=58, channels=58, kernel_size=[3, 3], data_layout="NHWC", kernel_layout="HWOI", out_dtype="int32") /* ty=Tensor[(1, 28, 28, 58), int32] span=Conv_27_quant:0:0 */;
  %38 = add(%37, meta[relay.Constant][38] /* ty=Tensor[(1, 1, 1, 58), int32] */) /* ty=Tensor[(1, 28, 28, 58), int32] */;
  %39 = qnn.requantize(%38, meta[relay.Constant][39] /* ty=Tensor[(58), float32] span=Conv_27_quant:0:0 */, 0 /* ty=int32 span=Conv_27_quant:0:0 */, 0.0329312f /* ty=float32 span=Conv_27_quant:0:0 */, 128 /* ty=int32 span=Conv_27_quant:0:0 */, axis=3, rounding="UPWARD", compute_dtype="int64", out_dtype="uint8") /* ty=Tensor[(1, 28, 28, 58), uint8] span=Conv_27_quant:0:0 */;
  %40 = qnn.conv2d(%39, meta[relay.Constant][40] /* ty=Tensor[(1, 1, 58, 58), int8] */, 128 /* ty=int32 span=fused Conv_29_quant:0:0 */, meta[relay.Constant][41] /* ty=Tensor[(58), int32] span=fused Conv_29_quant:0:0 */, 0.0329312f /* ty=float32 span=fused Conv_29_quant:0:0 */, meta[relay.Constant][42] /* ty=Tensor[(58), float32] span=fused Conv_29_quant:0:0 */, padding=[0, 0, 0, 0], channels=58, kernel_size=[1, 1], data_layout="NHWC", kernel_layout="HWIO", out_dtype="int32") /* ty=Tensor[(1, 28, 28, 58), int32] span=fused Conv_29_quant:0:0 */;
  %41 = add(%40, meta[relay.Constant][43] /* ty=Tensor[(1, 1, 1, 58), int32] */) /* ty=Tensor[(1, 28, 28, 58), int32] */;
  %42 = qnn.requantize(%41, meta[relay.Constant][44] /* ty=Tensor[(58), float32] span=fused Conv_29_quant:0:0 */, 0 /* ty=int32 span=fused Conv_29_quant:0:0 */, 0.0151881f /* ty=float32 span=fused Conv_29_quant:0:0 */, 0 /* ty=int32 span=fused Conv_29_quant:0:0 */, axis=3, rounding="UPWARD", compute_dtype="int64", out_dtype="uint8") /* ty=Tensor[(1, 28, 28, 58), uint8] span=fused Conv_29_quant:0:0 */;
  %43 = qnn.dequantize(%42, 0.0151881f /* ty=float32 span=fused Conv_29_quant.371_scale:0:0 */, 0 /* ty=int32 span=371_DequantizeLinear:0:0 */, out_dtype="float32", axis=0) /* ty=Tensor[(1, 28, 28, 58), float32] span=371_DequantizeLinear:0:0 */;
  %44 = qnn.dequantize(%31, 0.0322115f /* ty=float32 span=361_QuantizeLinear.361_scale:0:0 */, 0 /* ty=int32 span=362_DequantizeLinear:0:0 */, out_dtype="float32", axis=0) /* ty=Tensor[(1, 58, 28, 28), float32] span=362_DequantizeLinear:0:0 */;
  %45 = layout_transform(%43, src_layout="NHWC", dst_layout="NCHW") /* ty=Tensor[(1, 58, 28, 28), float32] */;
  %46 = (%44, %45) /* ty=(Tensor[(1, 58, 28, 28), float32], Tensor[(1, 58, 28, 28), float32]) span=Concat_32:0:0 */;
  %47 = concatenate(%46, axis=1) /* ty=Tensor[(1, 116, 28, 28), float32] span=Concat_32:0:0 */;
  %48 = reshape(%47, newshape=[1, 2, 58, 28, 28]) /* ty=Tensor[(1, 2, 58, 28, 28), float32] span=Reshape_34:0:0 */;
  %49 = transpose(%48, axes=[0, 2, 1, 3, 4]) /* ty=Tensor[(1, 58, 2, 28, 28), float32] span=Transpose_35:0:0 */;
  %50 = reshape(%49, newshape=[1, -1, 28, 28]) /* ty=Tensor[(1, 116, 28, 28), float32] span=Reshape_37:0:0 */;
  %51 = qnn.quantize(%50, 0.0322115f /* ty=float32 span=377_QuantizeLinear.377_scale:0:0 */, 0 /* ty=int32 span=377_QuantizeLinear:0:0 */, out_dtype="uint8", axis=0) /* ty=Tensor[(1, 116, 28, 28), uint8] span=377_QuantizeLinear:0:0 */;
  %52 = split(%51, indices_or_sections=[58], axis=1) /* ty=(Tensor[(1, 58, 28, 28), uint8], Tensor[(1, 58, 28, 28), uint8]) span=Split_38_quant:0:0 */;
  %53 = %52.0 /* ty=Tensor[(1, 58, 28, 28), uint8] */;
  %54 = %52.1 /* ty=Tensor[(1, 58, 28, 28), uint8] */;
  %55 = layout_transform(%54, src_layout="NCHW", dst_layout="NHWC") /* ty=Tensor[(1, 28, 28, 58), uint8] */;
  %56 = qnn.conv2d(%55, meta[relay.Constant][45] /* ty=Tensor[(1, 1, 58, 58), int8] */, 0 /* ty=int32 span=fused Conv_39_quant:0:0 */, meta[relay.Constant][46] /* ty=Tensor[(58), int32] span=fused Conv_39_quant:0:0 */, 0.0322115f /* ty=float32 span=fused Conv_39_quant:0:0 */, meta[relay.Constant][47] /* ty=Tensor[(58), float32] span=fused Conv_39_quant:0:0 */, padding=[0, 0, 0, 0], channels=58, kernel_size=[1, 1], data_layout="NHWC", kernel_layout="HWIO", out_dtype="int32") /* ty=Tensor[(1, 28, 28, 58), int32] span=fused Conv_39_quant:0:0 */;
  %57 = add(%56, meta[relay.Constant][48] /* ty=Tensor[(1, 1, 1, 58), int32] */) /* ty=Tensor[(1, 28, 28, 58), int32] */;
  %58 = qnn.requantize(%57, meta[relay.Constant][49] /* ty=Tensor[(58), float32] span=fused Conv_39_quant:0:0 */, 0 /* ty=int32 span=fused Conv_39_quant:0:0 */, 0.0113703f /* ty=float32 span=fused Conv_39_quant:0:0 */, 0 /* ty=int32 span=fused Conv_39_quant:0:0 */, axis=3, rounding="UPWARD", compute_dtype="int64", out_dtype="uint8") /* ty=Tensor[(1, 28, 28, 58), uint8] span=fused Conv_39_quant:0:0 */;
  %59 = qnn.conv2d(%58, meta[relay.Constant][50] /* ty=Tensor[(3, 3, 58, 1), int8] */, 0 /* ty=int32 span=Conv_42_quant:0:0 */, meta[relay.Constant][51] /* ty=Tensor[(58), int32] span=Conv_42_quant:0:0 */, 0.0113703f /* ty=float32 span=Conv_42_quant:0:0 */, meta[relay.Constant][52] /* ty=Tensor[(58), float32] span=Conv_42_quant:0:0 */, padding=[1, 1, 1, 1], groups=58, channels=58, kernel_size=[3, 3], data_layout="NHWC", kernel_layout="HWOI", out_dtype="int32") /* ty=Tensor[(1, 28, 28, 58), int32] span=Conv_42_quant:0:0 */;
  %60 = add(%59, meta[relay.Constant][53] /* ty=Tensor[(1, 1, 1, 58), int32] */) /* ty=Tensor[(1, 28, 28, 58), int32] */;
  %61 = qnn.requantize(%60, meta[relay.Constant][54] /* ty=Tensor[(58), float32] span=Conv_42_quant:0:0 */, 0 /* ty=int32 span=Conv_42_quant:0:0 */, 0.0318996f /* ty=float32 span=Conv_42_quant:0:0 */, 129 /* ty=int32 span=Conv_42_quant:0:0 */, axis=3, rounding="UPWARD", compute_dtype="int64", out_dtype="uint8") /* ty=Tensor[(1, 28, 28, 58), uint8] span=Conv_42_quant:0:0 */;
  %62 = qnn.conv2d(%61, meta[relay.Constant][55] /* ty=Tensor[(1, 1, 58, 58), int8] */, 129 /* ty=int32 span=fused Conv_44_quant:0:0 */, meta[relay.Constant][56] /* ty=Tensor[(58), int32] span=fused Conv_44_quant:0:0 */, 0.0318996f /* ty=float32 span=fused Conv_44_quant:0:0 */, meta[relay.Constant][57] /* ty=Tensor[(58), float32] span=fused Conv_44_quant:0:0 */, padding=[0, 0, 0, 0], channels=58, kernel_size=[1, 1], data_layout="NHWC", kernel_layout="HWIO", out_dtype="int32") /* ty=Tensor[(1, 28, 28, 58), int32] span=fused Conv_44_quant:0:0 */;
  %63 = add(%62, meta[relay.Constant][58] /* ty=Tensor[(1, 1, 1, 58), int32] */) /* ty=Tensor[(1, 28, 28, 58), int32] */;
  %64 = qnn.requantize(%63, meta[relay.Constant][59] /* ty=Tensor[(58), float32] span=fused Conv_44_quant:0:0 */, 0 /* ty=int32 span=fused Conv_44_quant:0:0 */, 0.0191096f /* ty=float32 span=fused Conv_44_quant:0:0 */, 0 /* ty=int32 span=fused Conv_44_quant:0:0 */, axis=3, rounding="UPWARD", compute_dtype="int64", out_dtype="uint8") /* ty=Tensor[(1, 28, 28, 58), uint8] span=fused Conv_44_quant:0:0 */;
  %65 = qnn.dequantize(%64, 0.0191096f /* ty=float32 span=fused Conv_44_quant.387_scale:0:0 */, 0 /* ty=int32 span=387_DequantizeLinear:0:0 */, out_dtype="float32", axis=0) /* ty=Tensor[(1, 28, 28, 58), float32] span=387_DequantizeLinear:0:0 */;
  %66 = qnn.dequantize(%53, 0.0322115f /* ty=float32 span=377_QuantizeLinear.377_scale:0:0 */, 0 /* ty=int32 span=378_DequantizeLinear:0:0 */, out_dtype="float32", axis=0) /* ty=Tensor[(1, 58, 28, 28), float32] span=378_DequantizeLinear:0:0 */;
  %67 = layout_transform(%65, src_layout="NHWC", dst_layout="NCHW") /* ty=Tensor[(1, 58, 28, 28), float32] */;
  %68 = (%66, %67) /* ty=(Tensor[(1, 58, 28, 28), float32], Tensor[(1, 58, 28, 28), float32]) span=Concat_47:0:0 */;
  %69 = concatenate(%68, axis=1) /* ty=Tensor[(1, 116, 28, 28), float32] span=Concat_47:0:0 */;
  %70 = reshape(%69, newshape=[1, 2, 58, 28, 28]) /* ty=Tensor[(1, 2, 58, 28, 28), float32] span=Reshape_49:0:0 */;
  %71 = transpose(%70, axes=[0, 2, 1, 3, 4]) /* ty=Tensor[(1, 58, 2, 28, 28), float32] span=Transpose_50:0:0 */;
  %72 = reshape(%71, newshape=[1, -1, 28, 28]) /* ty=Tensor[(1, 116, 28, 28), float32] span=Reshape_52:0:0 */;
  %73 = qnn.quantize(%72, 0.0322115f /* ty=float32 span=393_QuantizeLinear.393_scale:0:0 */, 0 /* ty=int32 span=393_QuantizeLinear:0:0 */, out_dtype="uint8", axis=0) /* ty=Tensor[(1, 116, 28, 28), uint8] span=393_QuantizeLinear:0:0 */;
  %74 = split(%73, indices_or_sections=[58], axis=1) /* ty=(Tensor[(1, 58, 28, 28), uint8], Tensor[(1, 58, 28, 28), uint8]) span=Split_53_quant:0:0 */;
  %75 = %74.0 /* ty=Tensor[(1, 58, 28, 28), uint8] */;
  %76 = %74.1 /* ty=Tensor[(1, 58, 28, 28), uint8] */;
  %77 = layout_transform(%76, src_layout="NCHW", dst_layout="NHWC") /* ty=Tensor[(1, 28, 28, 58), uint8] */;
  %78 = qnn.conv2d(%77, meta[relay.Constant][60] /* ty=Tensor[(1, 1, 58, 58), int8] */, 0 /* ty=int32 span=fused Conv_54_quant:0:0 */, meta[relay.Constant][61] /* ty=Tensor[(58), int32] span=fused Conv_54_quant:0:0 */, 0.0322115f /* ty=float32 span=fused Conv_54_quant:0:0 */, meta[relay.Constant][62] /* ty=Tensor[(58), float32] span=fused Conv_54_quant:0:0 */, padding=[0, 0, 0, 0], channels=58, kernel_size=[1, 1], data_layout="NHWC", kernel_layout="HWIO", out_dtype="int32") /* ty=Tensor[(1, 28, 28, 58), int32] span=fused Conv_54_quant:0:0 */;
  %79 = add(%78, meta[relay.Constant][63] /* ty=Tensor[(1, 1, 1, 58), int32] */) /* ty=Tensor[(1, 28, 28, 58), int32] */;
  %80 = qnn.requantize(%79, meta[relay.Constant][64] /* ty=Tensor[(58), float32] span=fused Conv_54_quant:0:0 */, 0 /* ty=int32 span=fused Conv_54_quant:0:0 */, 0.0149116f /* ty=float32 span=fused Conv_54_quant:0:0 */, 0 /* ty=int32 span=fused Conv_54_quant:0:0 */, axis=3, rounding="UPWARD", compute_dtype="int64", out_dtype="uint8") /* ty=Tensor[(1, 28, 28, 58), uint8] span=fused Conv_54_quant:0:0 */;
  %81 = qnn.conv2d(%80, meta[relay.Constant][65] /* ty=Tensor[(3, 3, 58, 1), int8] */, 0 /* ty=int32 span=Conv_57_quant:0:0 */, meta[relay.Constant][66] /* ty=Tensor[(58), int32] span=Conv_57_quant:0:0 */, 0.0149116f /* ty=float32 span=Conv_57_quant:0:0 */, meta[relay.Constant][67] /* ty=Tensor[(58), float32] span=Conv_57_quant:0:0 */, padding=[1, 1, 1, 1], groups=58, channels=58, kernel_size=[3, 3], data_layout="NHWC", kernel_layout="HWOI", out_dtype="int32") /* ty=Tensor[(1, 28, 28, 58), int32] span=Conv_57_quant:0:0 */;
  %82 = add(%81, meta[relay.Constant][68] /* ty=Tensor[(1, 1, 1, 58), int32] */) /* ty=Tensor[(1, 28, 28, 58), int32] */;
  %83 = qnn.requantize(%82, meta[relay.Constant][69] /* ty=Tensor[(58), float32] span=Conv_57_quant:0:0 */, 0 /* ty=int32 span=Conv_57_quant:0:0 */, 0.0439247f /* ty=float32 span=Conv_57_quant:0:0 */, 129 /* ty=int32 span=Conv_57_quant:0:0 */, axis=3, rounding="UPWARD", compute_dtype="int64", out_dtype="uint8") /* ty=Tensor[(1, 28, 28, 58), uint8] span=Conv_57_quant:0:0 */;
  %84 = qnn.conv2d(%83, meta[relay.Constant][70] /* ty=Tensor[(1, 1, 58, 58), int8] */, 129 /* ty=int32 span=fused Conv_59_quant:0:0 */, meta[relay.Constant][71] /* ty=Tensor[(58), int32] span=fused Conv_59_quant:0:0 */, 0.0439247f /* ty=float32 span=fused Conv_59_quant:0:0 */, meta[relay.Constant][72] /* ty=Tensor[(58), float32] span=fused Conv_59_quant:0:0 */, padding=[0, 0, 0, 0], channels=58, kernel_size=[1, 1], data_layout="NHWC", kernel_layout="HWIO", out_dtype="int32") /* ty=Tensor[(1, 28, 28, 58), int32] span=fused Conv_59_quant:0:0 */;
  %85 = add(%84, meta[relay.Constant][73] /* ty=Tensor[(1, 1, 1, 58), int32] */) /* ty=Tensor[(1, 28, 28, 58), int32] */;
  %86 = qnn.requantize(%85, meta[relay.Constant][74] /* ty=Tensor[(58), float32] span=fused Conv_59_quant:0:0 */, 0 /* ty=int32 span=fused Conv_59_quant:0:0 */, 0.0212046f /* ty=float32 span=fused Conv_59_quant:0:0 */, 0 /* ty=int32 span=fused Conv_59_quant:0:0 */, axis=3, rounding="UPWARD", compute_dtype="int64", out_dtype="uint8") /* ty=Tensor[(1, 28, 28, 58), uint8] span=fused Conv_59_quant:0:0 */;
  %87 = qnn.dequantize(%86, 0.0212046f /* ty=float32 span=fused Conv_59_quant.403_scale:0:0 */, 0 /* ty=int32 span=403_DequantizeLinear:0:0 */, out_dtype="float32", axis=0) /* ty=Tensor[(1, 28, 28, 58), float32] span=403_DequantizeLinear:0:0 */;
  %88 = qnn.dequantize(%75, 0.0322115f /* ty=float32 span=393_QuantizeLinear.393_scale:0:0 */, 0 /* ty=int32 span=394_DequantizeLinear:0:0 */, out_dtype="float32", axis=0) /* ty=Tensor[(1, 58, 28, 28), float32] span=394_DequantizeLinear:0:0 */;
  %89 = layout_transform(%87, src_layout="NHWC", dst_layout="NCHW") /* ty=Tensor[(1, 58, 28, 28), float32] */;
  %90 = (%88, %89) /* ty=(Tensor[(1, 58, 28, 28), float32], Tensor[(1, 58, 28, 28), float32]) span=Concat_62:0:0 */;
  %91 = concatenate(%90, axis=1) /* ty=Tensor[(1, 116, 28, 28), float32] span=Concat_62:0:0 */;
  %92 = reshape(%91, newshape=[1, 2, 58, 28, 28]) /* ty=Tensor[(1, 2, 58, 28, 28), float32] span=Reshape_64:0:0 */;
  %93 = transpose(%92, axes=[0, 2, 1, 3, 4]) /* ty=Tensor[(1, 58, 2, 28, 28), float32] span=Transpose_65:0:0 */;
  %94 = reshape(%93, newshape=[1, -1, 28, 28]) /* ty=Tensor[(1, 116, 28, 28), float32] span=Reshape_67:0:0 */;
  %95 = qnn.quantize(%94, 0.0238836f /* ty=float32 span=409_QuantizeLinear.409_scale:0:0 */, 0 /* ty=int32 span=409_QuantizeLinear:0:0 */, out_dtype="uint8", axis=0) /* ty=Tensor[(1, 116, 28, 28), uint8] span=409_QuantizeLinear:0:0 */;
  %96 = layout_transform(%95, src_layout="NCHW", dst_layout="NHWC") /* ty=Tensor[(1, 28, 28, 116), uint8] */;
  %97 = qnn.conv2d(%96, meta[relay.Constant][75] /* ty=Tensor[(3, 3, 116, 1), int8] */, 0 /* ty=int32 span=Conv_68_quant:0:0 */, meta[relay.Constant][76] /* ty=Tensor[(116), int32] span=Conv_68_quant:0:0 */, 0.0238836f /* ty=float32 span=Conv_68_quant:0:0 */, meta[relay.Constant][77] /* ty=Tensor[(116), float32] span=Conv_68_quant:0:0 */, strides=[2, 2], padding=[1, 1, 1, 1], groups=116, channels=116, kernel_size=[3, 3], data_layout="NHWC", kernel_layout="HWOI", out_dtype="int32") /* ty=Tensor[(1, 14, 14, 116), int32] span=Conv_68_quant:0:0 */;
  %98 = add(%97, meta[relay.Constant][78] /* ty=Tensor[(1, 1, 1, 116), int32] */) /* ty=Tensor[(1, 14, 14, 116), int32] */;
  %99 = qnn.requantize(%98, meta[relay.Constant][79] /* ty=Tensor[(116), float32] span=Conv_68_quant:0:0 */, 0 /* ty=int32 span=Conv_68_quant:0:0 */, 0.0366107f /* ty=float32 span=Conv_68_quant:0:0 */, 129 /* ty=int32 span=Conv_68_quant:0:0 */, axis=3, rounding="UPWARD", compute_dtype="int64", out_dtype="uint8") /* ty=Tensor[(1, 14, 14, 116), uint8] span=Conv_68_quant:0:0 */;
  %100 = qnn.conv2d(%99, meta[relay.Constant][80] /* ty=Tensor[(1, 1, 116, 116), int8] */, 129 /* ty=int32 span=fused Conv_70_quant:0:0 */, meta[relay.Constant][81] /* ty=Tensor[(116), int32] span=fused Conv_70_quant:0:0 */, 0.0366107f /* ty=float32 span=fused Conv_70_quant:0:0 */, meta[relay.Constant][82] /* ty=Tensor[(116), float32] span=fused Conv_70_quant:0:0 */, padding=[0, 0, 0, 0], channels=116, kernel_size=[1, 1], data_layout="NHWC", kernel_layout="HWIO", out_dtype="int32") /* ty=Tensor[(1, 14, 14, 116), int32] span=fused Conv_70_quant:0:0 */;
  %101 = add(%100, meta[relay.Constant][83] /* ty=Tensor[(1, 1, 1, 116), int32] */) /* ty=Tensor[(1, 14, 14, 116), int32] */;
  %102 = qnn.requantize(%101, meta[relay.Constant][84] /* ty=Tensor[(116), float32] span=fused Conv_70_quant:0:0 */, 0 /* ty=int32 span=fused Conv_70_quant:0:0 */, 0.0100475f /* ty=float32 span=fused Conv_70_quant:0:0 */, 0 /* ty=int32 span=fused Conv_70_quant:0:0 */, axis=3, rounding="UPWARD", compute_dtype="int64", out_dtype="uint8") /* ty=Tensor[(1, 14, 14, 116), uint8] span=fused Conv_70_quant:0:0 */;
  %103 = qnn.conv2d(%96, meta[relay.Constant][85] /* ty=Tensor[(1, 1, 116, 116), int8] */, 0 /* ty=int32 span=fused Conv_73_quant:0:0 */, meta[relay.Constant][86] /* ty=Tensor[(116), int32] span=fused Conv_73_quant:0:0 */, 0.0238836f /* ty=float32 span=fused Conv_73_quant:0:0 */, meta[relay.Constant][87] /* ty=Tensor[(116), float32] span=fused Conv_73_quant:0:0 */, padding=[0, 0, 0, 0], channels=116, kernel_size=[1, 1], data_layout="NHWC", kernel_layout="HWIO", out_dtype="int32") /* ty=Tensor[(1, 28, 28, 116), int32] span=fused Conv_73_quant:0:0 */;
  %104 = add(%103, meta[relay.Constant][88] /* ty=Tensor[(1, 1, 1, 116), int32] */) /* ty=Tensor[(1, 28, 28, 116), int32] */;
  %105 = qnn.requantize(%104, meta[relay.Constant][89] /* ty=Tensor[(116), float32] span=fused Conv_73_quant:0:0 */, 0 /* ty=int32 span=fused Conv_73_quant:0:0 */, 0.0132246f /* ty=float32 span=fused Conv_73_quant:0:0 */, 0 /* ty=int32 span=fused Conv_73_quant:0:0 */, axis=3, rounding="UPWARD", compute_dtype="int64", out_dtype="uint8") /* ty=Tensor[(1, 28, 28, 116), uint8] span=fused Conv_73_quant:0:0 */;
  %106 = qnn.conv2d(%105, meta[relay.Constant][90] /* ty=Tensor[(3, 3, 116, 1), int8] */, 0 /* ty=int32 span=Conv_76_quant:0:0 */, meta[relay.Constant][91] /* ty=Tensor[(116), int32] span=Conv_76_quant:0:0 */, 0.0132246f /* ty=float32 span=Conv_76_quant:0:0 */, meta[relay.Constant][92] /* ty=Tensor[(116), float32] span=Conv_76_quant:0:0 */, strides=[2, 2], padding=[1, 1, 1, 1], groups=116, channels=116, kernel_size=[3, 3], data_layout="NHWC", kernel_layout="HWOI", out_dtype="int32") /* ty=Tensor[(1, 14, 14, 116), int32] span=Conv_76_quant:0:0 */;
  %107 = add(%106, meta[relay.Constant][93] /* ty=Tensor[(1, 1, 1, 116), int32] */) /* ty=Tensor[(1, 14, 14, 116), int32] */;
  %108 = qnn.requantize(%107, meta[relay.Constant][94] /* ty=Tensor[(116), float32] span=Conv_76_quant:0:0 */, 0 /* ty=int32 span=Conv_76_quant:0:0 */, 0.0423683f /* ty=float32 span=Conv_76_quant:0:0 */, 111 /* ty=int32 span=Conv_76_quant:0:0 */, axis=3, rounding="UPWARD", compute_dtype="int64", out_dtype="uint8") /* ty=Tensor[(1, 14, 14, 116), uint8] span=Conv_76_quant:0:0 */;
  %109 = qnn.conv2d(%108, meta[relay.Constant][95] /* ty=Tensor[(1, 1, 116, 116), int8] */, 111 /* ty=int32 span=fused Conv_78_quant:0:0 */, meta[relay.Constant][96] /* ty=Tensor[(116), int32] span=fused Conv_78_quant:0:0 */, 0.0423683f /* ty=float32 span=fused Conv_78_quant:0:0 */, meta[relay.Constant][97] /* ty=Tensor[(116), float32] span=fused Conv_78_quant:0:0 */, padding=[0, 0, 0, 0], channels=116, kernel_size=[1, 1], data_layout="NHWC", kernel_layout="HWIO", out_dtype="int32") /* ty=Tensor[(1, 14, 14, 116), int32] span=fused Conv_78_quant:0:0 */;
  %110 = add(%109, meta[relay.Constant][98] /* ty=Tensor[(1, 1, 1, 116), int32] */) /* ty=Tensor[(1, 14, 14, 116), int32] */;
  %111 = qnn.requantize(%110, meta[relay.Constant][99] /* ty=Tensor[(116), float32] span=fused Conv_78_quant:0:0 */, 0 /* ty=int32 span=fused Conv_78_quant:0:0 */, 0.0149856f /* ty=float32 span=fused Conv_78_quant:0:0 */, 0 /* ty=int32 span=fused Conv_78_quant:0:0 */, axis=3, rounding="UPWARD", compute_dtype="int64", out_dtype="uint8") /* ty=Tensor[(1, 14, 14, 116), uint8] span=fused Conv_78_quant:0:0 */;
  %112 = qnn.dequantize(%102, 0.0100475f /* ty=float32 span=fused Conv_70_quant.414_scale:0:0 */, 0 /* ty=int32 span=414_DequantizeLinear:0:0 */, out_dtype="float32", axis=0) /* ty=Tensor[(1, 14, 14, 116), float32] span=414_DequantizeLinear:0:0 */;
  %113 = qnn.dequantize(%111, 0.0149856f /* ty=float32 span=fused Conv_78_quant.422_scale:0:0 */, 0 /* ty=int32 span=422_DequantizeLinear:0:0 */, out_dtype="float32", axis=0) /* ty=Tensor[(1, 14, 14, 116), float32] span=422_DequantizeLinear:0:0 */;
  %114 = (%112, %113) /* ty=(Tensor[(1, 14, 14, 116), float32], Tensor[(1, 14, 14, 116), float32]) span=Concat_81:0:0 */;
  %115 = concatenate(%114, axis=3) /* ty=Tensor[(1, 14, 14, 232), float32] span=Concat_81:0:0 */;
  %116 = layout_transform(%115, src_layout="NHWC", dst_layout="NCHW") /* ty=Tensor[(1, 232, 14, 14), float32] */;
  %117 = reshape(%116, newshape=[1, 2, 116, 14, 14]) /* ty=Tensor[(1, 2, 116, 14, 14), float32] span=Reshape_83:0:0 */;
  %118 = transpose(%117, axes=[0, 2, 1, 3, 4]) /* ty=Tensor[(1, 116, 2, 14, 14), float32] span=Transpose_84:0:0 */;
  %119 = reshape(%118, newshape=[1, -1, 14, 14]) /* ty=Tensor[(1, 232, 14, 14), float32] span=Reshape_86:0:0 */;
  %120 = qnn.quantize(%119, 0.0149856f /* ty=float32 span=428_QuantizeLinear.428_scale:0:0 */, 0 /* ty=int32 span=428_QuantizeLinear:0:0 */, out_dtype="uint8", axis=0) /* ty=Tensor[(1, 232, 14, 14), uint8] span=428_QuantizeLinear:0:0 */;
  %121 = split(%120, indices_or_sections=[116], axis=1) /* ty=(Tensor[(1, 116, 14, 14), uint8], Tensor[(1, 116, 14, 14), uint8]) span=Split_87_quant:0:0 */;
  %122 = %121.0 /* ty=Tensor[(1, 116, 14, 14), uint8] */;
  %123 = %121.1 /* ty=Tensor[(1, 116, 14, 14), uint8] */;
  %124 = layout_transform(%123, src_layout="NCHW", dst_layout="NHWC") /* ty=Tensor[(1, 14, 14, 116), uint8] */;
  %125 = qnn.conv2d(%124, meta[relay.Constant][100] /* ty=Tensor[(1, 1, 116, 116), int8] */, 0 /* ty=int32 span=fused Conv_88_quant:0:0 */, meta[relay.Constant][101] /* ty=Tensor[(116), int32] span=fused Conv_88_quant:0:0 */, 0.0149856f /* ty=float32 span=fused Conv_88_quant:0:0 */, meta[relay.Constant][102] /* ty=Tensor[(116), float32] span=fused Conv_88_quant:0:0 */, padding=[0, 0, 0, 0], channels=116, kernel_size=[1, 1], data_layout="NHWC", kernel_layout="HWIO", out_dtype="int32") /* ty=Tensor[(1, 14, 14, 116), int32] span=fused Conv_88_quant:0:0 */;
  %126 = add(%125, meta[relay.Constant][103] /* ty=Tensor[(1, 1, 1, 116), int32] */) /* ty=Tensor[(1, 14, 14, 116), int32] */;
  %127 = qnn.requantize(%126, meta[relay.Constant][104] /* ty=Tensor[(116), float32] span=fused Conv_88_quant:0:0 */, 0 /* ty=int32 span=fused Conv_88_quant:0:0 */, 0.00679558f /* ty=float32 span=fused Conv_88_quant:0:0 */, 0 /* ty=int32 span=fused Conv_88_quant:0:0 */, axis=3, rounding="UPWARD", compute_dtype="int64", out_dtype="uint8") /* ty=Tensor[(1, 14, 14, 116), uint8] span=fused Conv_88_quant:0:0 */;
  %128 = qnn.conv2d(%127, meta[relay.Constant][105] /* ty=Tensor[(3, 3, 116, 1), int8] */, 0 /* ty=int32 span=Conv_91_quant:0:0 */, meta[relay.Constant][106] /* ty=Tensor[(116), int32] span=Conv_91_quant:0:0 */, 0.00679558f /* ty=float32 span=Conv_91_quant:0:0 */, meta[relay.Constant][107] /* ty=Tensor[(116), float32] span=Conv_91_quant:0:0 */, padding=[1, 1, 1, 1], groups=116, channels=116, kernel_size=[3, 3], data_layout="NHWC", kernel_layout="HWOI", out_dtype="int32") /* ty=Tensor[(1, 14, 14, 116), int32] span=Conv_91_quant:0:0 */;
  %129 = add(%128, meta[relay.Constant][108] /* ty=Tensor[(1, 1, 1, 116), int32] */) /* ty=Tensor[(1, 14, 14, 116), int32] */;
  %130 = qnn.requantize(%129, meta[relay.Constant][109] /* ty=Tensor[(116), float32] span=Conv_91_quant:0:0 */, 0 /* ty=int32 span=Conv_91_quant:0:0 */, 0.0249611f /* ty=float32 span=Conv_91_quant:0:0 */, 130 /* ty=int32 span=Conv_91_quant:0:0 */, axis=3, rounding="UPWARD", compute_dtype="int64", out_dtype="uint8") /* ty=Tensor[(1, 14, 14, 116), uint8] span=Conv_91_quant:0:0 */;
  %131 = qnn.conv2d(%130, meta[relay.Constant][110] /* ty=Tensor[(1, 1, 116, 116), int8] */, 130 /* ty=int32 span=fused Conv_93_quant:0:0 */, meta[relay.Constant][111] /* ty=Tensor[(116), int32] span=fused Conv_93_quant:0:0 */, 0.0249611f /* ty=float32 span=fused Conv_93_quant:0:0 */, meta[relay.Constant][112] /* ty=Tensor[(116), float32] span=fused Conv_93_quant:0:0 */, padding=[0, 0, 0, 0], channels=116, kernel_size=[1, 1], data_layout="NHWC", kernel_layout="HWIO", out_dtype="int32") /* ty=Tensor[(1, 14, 14, 116), int32] span=fused Conv_93_quant:0:0 */;
  %132 = add(%131, meta[relay.Constant][113] /* ty=Tensor[(1, 1, 1, 116), int32] */) /* ty=Tensor[(1, 14, 14, 116), int32] */;
  %133 = qnn.requantize(%132, meta[relay.Constant][114] /* ty=Tensor[(116), float32] span=fused Conv_93_quant:0:0 */, 0 /* ty=int32 span=fused Conv_93_quant:0:0 */, 0.0127221f /* ty=float32 span=fused Conv_93_quant:0:0 */, 0 /* ty=int32 span=fused Conv_93_quant:0:0 */, axis=3, rounding="UPWARD", compute_dtype="int64", out_dtype="uint8") /* ty=Tensor[(1, 14, 14, 116), uint8] span=fused Conv_93_quant:0:0 */;
  %134 = qnn.dequantize(%133, 0.0127221f /* ty=float32 span=fused Conv_93_quant.438_scale:0:0 */, 0 /* ty=int32 span=438_DequantizeLinear:0:0 */, out_dtype="float32", axis=0) /* ty=Tensor[(1, 14, 14, 116), float32] span=438_DequantizeLinear:0:0 */;
  %135 = qnn.dequantize(%122, 0.0149856f /* ty=float32 span=428_QuantizeLinear.428_scale:0:0 */, 0 /* ty=int32 span=429_DequantizeLinear:0:0 */, out_dtype="float32", axis=0) /* ty=Tensor[(1, 116, 14, 14), float32] span=429_DequantizeLinear:0:0 */;
  %136 = layout_transform(%134, src_layout="NHWC", dst_layout="NCHW") /* ty=Tensor[(1, 116, 14, 14), float32] */;
  %137 = (%135, %136) /* ty=(Tensor[(1, 116, 14, 14), float32], Tensor[(1, 116, 14, 14), float32]) span=Concat_96:0:0 */;
  %138 = concatenate(%137, axis=1) /* ty=Tensor[(1, 232, 14, 14), float32] span=Concat_96:0:0 */;
  %139 = reshape(%138, newshape=[1, 2, 116, 14, 14]) /* ty=Tensor[(1, 2, 116, 14, 14), float32] span=Reshape_98:0:0 */;
  %140 = transpose(%139, axes=[0, 2, 1, 3, 4]) /* ty=Tensor[(1, 116, 2, 14, 14), float32] span=Transpose_99:0:0 */;
  %141 = reshape(%140, newshape=[1, -1, 14, 14]) /* ty=Tensor[(1, 232, 14, 14), float32] span=Reshape_101:0:0 */;
  %142 = qnn.quantize(%141, 0.0149856f /* ty=float32 span=444_QuantizeLinear.444_scale:0:0 */, 0 /* ty=int32 span=444_QuantizeLinear:0:0 */, out_dtype="uint8", axis=0) /* ty=Tensor[(1, 232, 14, 14), uint8] span=444_QuantizeLinear:0:0 */;
  %143 = split(%142, indices_or_sections=[116], axis=1) /* ty=(Tensor[(1, 116, 14, 14), uint8], Tensor[(1, 116, 14, 14), uint8]) span=Split_102_quant:0:0 */;
  %144 = %143.0 /* ty=Tensor[(1, 116, 14, 14), uint8] */;
  %145 = %143.1 /* ty=Tensor[(1, 116, 14, 14), uint8] */;
  %146 = layout_transform(%145, src_layout="NCHW", dst_layout="NHWC") /* ty=Tensor[(1, 14, 14, 116), uint8] */;
  %147 = qnn.conv2d(%146, meta[relay.Constant][115] /* ty=Tensor[(1, 1, 116, 116), int8] */, 0 /* ty=int32 span=fused Conv_103_quant:0:0 */, meta[relay.Constant][116] /* ty=Tensor[(116), int32] span=fused Conv_103_quant:0:0 */, 0.0149856f /* ty=float32 span=fused Conv_103_quant:0:0 */, meta[relay.Constant][117] /* ty=Tensor[(116), float32] span=fused Conv_103_quant:0:0 */, padding=[0, 0, 0, 0], channels=116, kernel_size=[1, 1], data_layout="NHWC", kernel_layout="HWIO", out_dtype="int32") /* ty=Tensor[(1, 14, 14, 116), int32] span=fused Conv_103_quant:0:0 */;
  %148 = add(%147, meta[relay.Constant][118] /* ty=Tensor[(1, 1, 1, 116), int32] */) /* ty=Tensor[(1, 14, 14, 116), int32] */;
  %149 = qnn.requantize(%148, meta[relay.Constant][119] /* ty=Tensor[(116), float32] span=fused Conv_103_quant:0:0 */, 0 /* ty=int32 span=fused Conv_103_quant:0:0 */, 0.00760231f /* ty=float32 span=fused Conv_103_quant:0:0 */, 0 /* ty=int32 span=fused Conv_103_quant:0:0 */, axis=3, rounding="UPWARD", compute_dtype="int64", out_dtype="uint8") /* ty=Tensor[(1, 14, 14, 116), uint8] span=fused Conv_103_quant:0:0 */;
  %150 = qnn.conv2d(%149, meta[relay.Constant][120] /* ty=Tensor[(3, 3, 116, 1), int8] */, 0 /* ty=int32 span=Conv_106_quant:0:0 */, meta[relay.Constant][121] /* ty=Tensor[(116), int32] span=Conv_106_quant:0:0 */, 0.00760231f /* ty=float32 span=Conv_106_quant:0:0 */, meta[relay.Constant][122] /* ty=Tensor[(116), float32] span=Conv_106_quant:0:0 */, padding=[1, 1, 1, 1], groups=116, channels=116, kernel_size=[3, 3], data_layout="NHWC", kernel_layout="HWOI", out_dtype="int32") /* ty=Tensor[(1, 14, 14, 116), int32] span=Conv_106_quant:0:0 */;
  %151 = add(%150, meta[relay.Constant][123] /* ty=Tensor[(1, 1, 1, 116), int32] */) /* ty=Tensor[(1, 14, 14, 116), int32] */;
  %152 = qnn.requantize(%151, meta[relay.Constant][124] /* ty=Tensor[(116), float32] span=Conv_106_quant:0:0 */, 0 /* ty=int32 span=Conv_106_quant:0:0 */, 0.0265386f /* ty=float32 span=Conv_106_quant:0:0 */, 138 /* ty=int32 span=Conv_106_quant:0:0 */, axis=3, rounding="UPWARD", compute_dtype="int64", out_dtype="uint8") /* ty=Tensor[(1, 14, 14, 116), uint8] span=Conv_106_quant:0:0 */;
  %153 = qnn.conv2d(%152, meta[relay.Constant][125] /* ty=Tensor[(1, 1, 116, 116), int8] */, 138 /* ty=int32 span=fused Conv_108_quant:0:0 */, meta[relay.Constant][126] /* ty=Tensor[(116), int32] span=fused Conv_108_quant:0:0 */, 0.0265386f /* ty=float32 span=fused Conv_108_quant:0:0 */, meta[relay.Constant][127] /* ty=Tensor[(116), float32] span=fused Conv_108_quant:0:0 */, padding=[0, 0, 0, 0], channels=116, kernel_size=[1, 1], data_layout="NHWC", kernel_layout="HWIO", out_dtype="int32") /* ty=Tensor[(1, 14, 14, 116), int32] span=fused Conv_108_quant:0:0 */;
  %154 = add(%153, meta[relay.Constant][128] /* ty=Tensor[(1, 1, 1, 116), int32] */) /* ty=Tensor[(1, 14, 14, 116), int32] */;
  %155 = qnn.requantize(%154, meta[relay.Constant][129] /* ty=Tensor[(116), float32] span=fused Conv_108_quant:0:0 */, 0 /* ty=int32 span=fused Conv_108_quant:0:0 */, 0.00980095f /* ty=float32 span=fused Conv_108_quant:0:0 */, 0 /* ty=int32 span=fused Conv_108_quant:0:0 */, axis=3, rounding="UPWARD", compute_dtype="int64", out_dtype="uint8") /* ty=Tensor[(1, 14, 14, 116), uint8] span=fused Conv_108_quant:0:0 */;
  %156 = qnn.dequantize(%155, 0.00980095f /* ty=float32 span=fused Conv_108_quant.454_scale:0:0 */, 0 /* ty=int32 span=454_DequantizeLinear:0:0 */, out_dtype="float32", axis=0) /* ty=Tensor[(1, 14, 14, 116), float32] span=454_DequantizeLinear:0:0 */;
  %157 = qnn.dequantize(%144, 0.0149856f /* ty=float32 span=444_QuantizeLinear.444_scale:0:0 */, 0 /* ty=int32 span=445_DequantizeLinear:0:0 */, out_dtype="float32", axis=0) /* ty=Tensor[(1, 116, 14, 14), float32] span=445_DequantizeLinear:0:0 */;
  %158 = layout_transform(%156, src_layout="NHWC", dst_layout="NCHW") /* ty=Tensor[(1, 116, 14, 14), float32] */;
  %159 = (%157, %158) /* ty=(Tensor[(1, 116, 14, 14), float32], Tensor[(1, 116, 14, 14), float32]) span=Concat_111:0:0 */;
  %160 = concatenate(%159, axis=1) /* ty=Tensor[(1, 232, 14, 14), float32] span=Concat_111:0:0 */;
  %161 = reshape(%160, newshape=[1, 2, 116, 14, 14]) /* ty=Tensor[(1, 2, 116, 14, 14), float32] span=Reshape_113:0:0 */;
  %162 = transpose(%161, axes=[0, 2, 1, 3, 4]) /* ty=Tensor[(1, 116, 2, 14, 14), float32] span=Transpose_114:0:0 */;
  %163 = reshape(%162, newshape=[1, -1, 14, 14]) /* ty=Tensor[(1, 232, 14, 14), float32] span=Reshape_116:0:0 */;
  %164 = qnn.quantize(%163, 0.0109102f /* ty=float32 span=460_QuantizeLinear.460_scale:0:0 */, 0 /* ty=int32 span=460_QuantizeLinear:0:0 */, out_dtype="uint8", axis=0) /* ty=Tensor[(1, 232, 14, 14), uint8] span=460_QuantizeLinear:0:0 */;
  %165 = split(%164, indices_or_sections=[116], axis=1) /* ty=(Tensor[(1, 116, 14, 14), uint8], Tensor[(1, 116, 14, 14), uint8]) span=Split_117_quant:0:0 */;
  %166 = %165.0 /* ty=Tensor[(1, 116, 14, 14), uint8] */;
  %167 = %165.1 /* ty=Tensor[(1, 116, 14, 14), uint8] */;
  %168 = layout_transform(%167, src_layout="NCHW", dst_layout="NHWC") /* ty=Tensor[(1, 14, 14, 116), uint8] */;
  %169 = qnn.conv2d(%168, meta[relay.Constant][130] /* ty=Tensor[(1, 1, 116, 116), int8] */, 0 /* ty=int32 span=fused Conv_118_quant:0:0 */, meta[relay.Constant][131] /* ty=Tensor[(116), int32] span=fused Conv_118_quant:0:0 */, 0.0109102f /* ty=float32 span=fused Conv_118_quant:0:0 */, meta[relay.Constant][132] /* ty=Tensor[(116), float32] span=fused Conv_118_quant:0:0 */, padding=[0, 0, 0, 0], channels=116, kernel_size=[1, 1], data_layout="NHWC", kernel_layout="HWIO", out_dtype="int32") /* ty=Tensor[(1, 14, 14, 116), int32] span=fused Conv_118_quant:0:0 */;
  %170 = add(%169, meta[relay.Constant][133] /* ty=Tensor[(1, 1, 1, 116), int32] */) /* ty=Tensor[(1, 14, 14, 116), int32] */;
  %171 = qnn.requantize(%170, meta[relay.Constant][134] /* ty=Tensor[(116), float32] span=fused Conv_118_quant:0:0 */, 0 /* ty=int32 span=fused Conv_118_quant:0:0 */, 0.0109673f /* ty=float32 span=fused Conv_118_quant:0:0 */, 0 /* ty=int32 span=fused Conv_118_quant:0:0 */, axis=3, rounding="UPWARD", compute_dtype="int64", out_dtype="uint8") /* ty=Tensor[(1, 14, 14, 116), uint8] span=fused Conv_118_quant:0:0 */;
  %172 = qnn.conv2d(%171, meta[relay.Constant][135] /* ty=Tensor[(3, 3, 116, 1), int8] */, 0 /* ty=int32 span=Conv_121_quant:0:0 */, meta[relay.Constant][136] /* ty=Tensor[(116), int32] span=Conv_121_quant:0:0 */, 0.0109673f /* ty=float32 span=Conv_121_quant:0:0 */, meta[relay.Constant][137] /* ty=Tensor[(116), float32] span=Conv_121_quant:0:0 */, padding=[1, 1, 1, 1], groups=116, channels=116, kernel_size=[3, 3], data_layout="NHWC", kernel_layout="HWOI", out_dtype="int32") /* ty=Tensor[(1, 14, 14, 116), int32] span=Conv_121_quant:0:0 */;
  %173 = add(%172, meta[relay.Constant][138] /* ty=Tensor[(1, 1, 1, 116), int32] */) /* ty=Tensor[(1, 14, 14, 116), int32] */;
  %174 = qnn.requantize(%173, meta[relay.Constant][139] /* ty=Tensor[(116), float32] span=Conv_121_quant:0:0 */, 0 /* ty=int32 span=Conv_121_quant:0:0 */, 0.0368718f /* ty=float32 span=Conv_121_quant:0:0 */, 145 /* ty=int32 span=Conv_121_quant:0:0 */, axis=3, rounding="UPWARD", compute_dtype="int64", out_dtype="uint8") /* ty=Tensor[(1, 14, 14, 116), uint8] span=Conv_121_quant:0:0 */;
  %175 = qnn.conv2d(%174, meta[relay.Constant][140] /* ty=Tensor[(1, 1, 116, 116), int8] */, 145 /* ty=int32 span=fused Conv_123_quant:0:0 */, meta[relay.Constant][141] /* ty=Tensor[(116), int32] span=fused Conv_123_quant:0:0 */, 0.0368718f /* ty=float32 span=fused Conv_123_quant:0:0 */, meta[relay.Constant][142] /* ty=Tensor[(116), float32] span=fused Conv_123_quant:0:0 */, padding=[0, 0, 0, 0], channels=116, kernel_size=[1, 1], data_layout="NHWC", kernel_layout="HWIO", out_dtype="int32") /* ty=Tensor[(1, 14, 14, 116), int32] span=fused Conv_123_quant:0:0 */;
  %176 = add(%175, meta[relay.Constant][143] /* ty=Tensor[(1, 1, 1, 116), int32] */) /* ty=Tensor[(1, 14, 14, 116), int32] */;
  %177 = qnn.requantize(%176, meta[relay.Constant][144] /* ty=Tensor[(116), float32] span=fused Conv_123_quant:0:0 */, 0 /* ty=int32 span=fused Conv_123_quant:0:0 */, 0.0127529f /* ty=float32 span=fused Conv_123_quant:0:0 */, 0 /* ty=int32 span=fused Conv_123_quant:0:0 */, axis=3, rounding="UPWARD", compute_dtype="int64", out_dtype="uint8") /* ty=Tensor[(1, 14, 14, 116), uint8] span=fused Conv_123_quant:0:0 */;
  %178 = qnn.dequantize(%177, 0.0127529f /* ty=float32 span=fused Conv_123_quant.470_scale:0:0 */, 0 /* ty=int32 span=470_DequantizeLinear:0:0 */, out_dtype="float32", axis=0) /* ty=Tensor[(1, 14, 14, 116), float32] span=470_DequantizeLinear:0:0 */;
  %179 = qnn.dequantize(%166, 0.0109102f /* ty=float32 span=460_QuantizeLinear.460_scale:0:0 */, 0 /* ty=int32 span=461_DequantizeLinear:0:0 */, out_dtype="float32", axis=0) /* ty=Tensor[(1, 116, 14, 14), float32] span=461_DequantizeLinear:0:0 */;
  %180 = layout_transform(%178, src_layout="NHWC", dst_layout="NCHW") /* ty=Tensor[(1, 116, 14, 14), float32] */;
  %181 = (%179, %180) /* ty=(Tensor[(1, 116, 14, 14), float32], Tensor[(1, 116, 14, 14), float32]) span=Concat_126:0:0 */;
  %182 = concatenate(%181, axis=1) /* ty=Tensor[(1, 232, 14, 14), float32] span=Concat_126:0:0 */;
  %183 = reshape(%182, newshape=[1, 2, 116, 14, 14]) /* ty=Tensor[(1, 2, 116, 14, 14), float32] span=Reshape_128:0:0 */;
  %184 = transpose(%183, axes=[0, 2, 1, 3, 4]) /* ty=Tensor[(1, 116, 2, 14, 14), float32] span=Transpose_129:0:0 */;
  %185 = reshape(%184, newshape=[1, -1, 14, 14]) /* ty=Tensor[(1, 232, 14, 14), float32] span=Reshape_131:0:0 */;
  %186 = qnn.quantize(%185, 0.0127529f /* ty=float32 span=476_QuantizeLinear.476_scale:0:0 */, 0 /* ty=int32 span=476_QuantizeLinear:0:0 */, out_dtype="uint8", axis=0) /* ty=Tensor[(1, 232, 14, 14), uint8] span=476_QuantizeLinear:0:0 */;
  %187 = split(%186, indices_or_sections=[116], axis=1) /* ty=(Tensor[(1, 116, 14, 14), uint8], Tensor[(1, 116, 14, 14), uint8]) span=Split_132_quant:0:0 */;
  %188 = %187.0 /* ty=Tensor[(1, 116, 14, 14), uint8] */;
  %189 = %187.1 /* ty=Tensor[(1, 116, 14, 14), uint8] */;
  %190 = layout_transform(%189, src_layout="NCHW", dst_layout="NHWC") /* ty=Tensor[(1, 14, 14, 116), uint8] */;
  %191 = qnn.conv2d(%190, meta[relay.Constant][145] /* ty=Tensor[(1, 1, 116, 116), int8] */, 0 /* ty=int32 span=fused Conv_133_quant:0:0 */, meta[relay.Constant][146] /* ty=Tensor[(116), int32] span=fused Conv_133_quant:0:0 */, 0.0127529f /* ty=float32 span=fused Conv_133_quant:0:0 */, meta[relay.Constant][147] /* ty=Tensor[(116), float32] span=fused Conv_133_quant:0:0 */, padding=[0, 0, 0, 0], channels=116, kernel_size=[1, 1], data_layout="NHWC", kernel_layout="HWIO", out_dtype="int32") /* ty=Tensor[(1, 14, 14, 116), int32] span=fused Conv_133_quant:0:0 */;
  %192 = add(%191, meta[relay.Constant][148] /* ty=Tensor[(1, 1, 1, 116), int32] */) /* ty=Tensor[(1, 14, 14, 116), int32] */;
  %193 = qnn.requantize(%192, meta[relay.Constant][149] /* ty=Tensor[(116), float32] span=fused Conv_133_quant:0:0 */, 0 /* ty=int32 span=fused Conv_133_quant:0:0 */, 0.00810356f /* ty=float32 span=fused Conv_133_quant:0:0 */, 0 /* ty=int32 span=fused Conv_133_quant:0:0 */, axis=3, rounding="UPWARD", compute_dtype="int64", out_dtype="uint8") /* ty=Tensor[(1, 14, 14, 116), uint8] span=fused Conv_133_quant:0:0 */;
  %194 = qnn.conv2d(%193, meta[relay.Constant][150] /* ty=Tensor[(3, 3, 116, 1), int8] */, 0 /* ty=int32 span=Conv_136_quant:0:0 */, meta[relay.Constant][151] /* ty=Tensor[(116), int32] span=Conv_136_quant:0:0 */, 0.00810356f /* ty=float32 span=Conv_136_quant:0:0 */, meta[relay.Constant][152] /* ty=Tensor[(116), float32] span=Conv_136_quant:0:0 */, padding=[1, 1, 1, 1], groups=116, channels=116, kernel_size=[3, 3], data_layout="NHWC", kernel_layout="HWOI", out_dtype="int32") /* ty=Tensor[(1, 14, 14, 116), int32] span=Conv_136_quant:0:0 */;
  %195 = add(%194, meta[relay.Constant][153] /* ty=Tensor[(1, 1, 1, 116), int32] */) /* ty=Tensor[(1, 14, 14, 116), int32] */;
  %196 = qnn.requantize(%195, meta[relay.Constant][154] /* ty=Tensor[(116), float32] span=Conv_136_quant:0:0 */, 0 /* ty=int32 span=Conv_136_quant:0:0 */, 0.0308984f /* ty=float32 span=Conv_136_quant:0:0 */, 132 /* ty=int32 span=Conv_136_quant:0:0 */, axis=3, rounding="UPWARD", compute_dtype="int64", out_dtype="uint8") /* ty=Tensor[(1, 14, 14, 116), uint8] span=Conv_136_quant:0:0 */;
  %197 = qnn.conv2d(%196, meta[relay.Constant][155] /* ty=Tensor[(1, 1, 116, 116), int8] */, 132 /* ty=int32 span=fused Conv_138_quant:0:0 */, meta[relay.Constant][156] /* ty=Tensor[(116), int32] span=fused Conv_138_quant:0:0 */, 0.0308984f /* ty=float32 span=fused Conv_138_quant:0:0 */, meta[relay.Constant][157] /* ty=Tensor[(116), float32] span=fused Conv_138_quant:0:0 */, padding=[0, 0, 0, 0], channels=116, kernel_size=[1, 1], data_layout="NHWC", kernel_layout="HWIO", out_dtype="int32") /* ty=Tensor[(1, 14, 14, 116), int32] span=fused Conv_138_quant:0:0 */;
  %198 = add(%197, meta[relay.Constant][158] /* ty=Tensor[(1, 1, 1, 116), int32] */) /* ty=Tensor[(1, 14, 14, 116), int32] */;
  %199 = qnn.requantize(%198, meta[relay.Constant][159] /* ty=Tensor[(116), float32] span=fused Conv_138_quant:0:0 */, 0 /* ty=int32 span=fused Conv_138_quant:0:0 */, 0.0148973f /* ty=float32 span=fused Conv_138_quant:0:0 */, 0 /* ty=int32 span=fused Conv_138_quant:0:0 */, axis=3, rounding="UPWARD", compute_dtype="int64", out_dtype="uint8") /* ty=Tensor[(1, 14, 14, 116), uint8] span=fused Conv_138_quant:0:0 */;
  %200 = qnn.dequantize(%199, 0.0148973f /* ty=float32 span=fused Conv_138_quant.486_scale:0:0 */, 0 /* ty=int32 span=486_DequantizeLinear:0:0 */, out_dtype="float32", axis=0) /* ty=Tensor[(1, 14, 14, 116), float32] span=486_DequantizeLinear:0:0 */;
  %201 = qnn.dequantize(%188, 0.0127529f /* ty=float32 span=476_QuantizeLinear.476_scale:0:0 */, 0 /* ty=int32 span=477_DequantizeLinear:0:0 */, out_dtype="float32", axis=0) /* ty=Tensor[(1, 116, 14, 14), float32] span=477_DequantizeLinear:0:0 */;
  %202 = layout_transform(%200, src_layout="NHWC", dst_layout="NCHW") /* ty=Tensor[(1, 116, 14, 14), float32] */;
  %203 = (%201, %202) /* ty=(Tensor[(1, 116, 14, 14), float32], Tensor[(1, 116, 14, 14), float32]) span=Concat_141:0:0 */;
  %204 = concatenate(%203, axis=1) /* ty=Tensor[(1, 232, 14, 14), float32] span=Concat_141:0:0 */;
  %205 = reshape(%204, newshape=[1, 2, 116, 14, 14]) /* ty=Tensor[(1, 2, 116, 14, 14), float32] span=Reshape_143:0:0 */;
  %206 = transpose(%205, axes=[0, 2, 1, 3, 4]) /* ty=Tensor[(1, 116, 2, 14, 14), float32] span=Transpose_144:0:0 */;
  %207 = reshape(%206, newshape=[1, -1, 14, 14]) /* ty=Tensor[(1, 232, 14, 14), float32] span=Reshape_146:0:0 */;
  %208 = qnn.quantize(%207, 0.0148973f /* ty=float32 span=492_QuantizeLinear.492_scale:0:0 */, 0 /* ty=int32 span=492_QuantizeLinear:0:0 */, out_dtype="uint8", axis=0) /* ty=Tensor[(1, 232, 14, 14), uint8] span=492_QuantizeLinear:0:0 */;
  %209 = split(%208, indices_or_sections=[116], axis=1) /* ty=(Tensor[(1, 116, 14, 14), uint8], Tensor[(1, 116, 14, 14), uint8]) span=Split_147_quant:0:0 */;
  %210 = %209.0 /* ty=Tensor[(1, 116, 14, 14), uint8] */;
  %211 = %209.1 /* ty=Tensor[(1, 116, 14, 14), uint8] */;
  %212 = layout_transform(%211, src_layout="NCHW", dst_layout="NHWC") /* ty=Tensor[(1, 14, 14, 116), uint8] */;
  %213 = qnn.conv2d(%212, meta[relay.Constant][160] /* ty=Tensor[(1, 1, 116, 116), int8] */, 0 /* ty=int32 span=fused Conv_148_quant:0:0 */, meta[relay.Constant][161] /* ty=Tensor[(116), int32] span=fused Conv_148_quant:0:0 */, 0.0148973f /* ty=float32 span=fused Conv_148_quant:0:0 */, meta[relay.Constant][162] /* ty=Tensor[(116), float32] span=fused Conv_148_quant:0:0 */, padding=[0, 0, 0, 0], channels=116, kernel_size=[1, 1], data_layout="NHWC", kernel_layout="HWIO", out_dtype="int32") /* ty=Tensor[(1, 14, 14, 116), int32] span=fused Conv_148_quant:0:0 */;
  %214 = add(%213, meta[relay.Constant][163] /* ty=Tensor[(1, 1, 1, 116), int32] */) /* ty=Tensor[(1, 14, 14, 116), int32] */;
  %215 = qnn.requantize(%214, meta[relay.Constant][164] /* ty=Tensor[(116), float32] span=fused Conv_148_quant:0:0 */, 0 /* ty=int32 span=fused Conv_148_quant:0:0 */, 0.0136885f /* ty=float32 span=fused Conv_148_quant:0:0 */, 0 /* ty=int32 span=fused Conv_148_quant:0:0 */, axis=3, rounding="UPWARD", compute_dtype="int64", out_dtype="uint8") /* ty=Tensor[(1, 14, 14, 116), uint8] span=fused Conv_148_quant:0:0 */;
  %216 = qnn.conv2d(%215, meta[relay.Constant][165] /* ty=Tensor[(3, 3, 116, 1), int8] */, 0 /* ty=int32 span=Conv_151_quant:0:0 */, meta[relay.Constant][166] /* ty=Tensor[(116), int32] span=Conv_151_quant:0:0 */, 0.0136885f /* ty=float32 span=Conv_151_quant:0:0 */, meta[relay.Constant][167] /* ty=Tensor[(116), float32] span=Conv_151_quant:0:0 */, padding=[1, 1, 1, 1], groups=116, channels=116, kernel_size=[3, 3], data_layout="NHWC", kernel_layout="HWOI", out_dtype="int32") /* ty=Tensor[(1, 14, 14, 116), int32] span=Conv_151_quant:0:0 */;
  %217 = add(%216, meta[relay.Constant][168] /* ty=Tensor[(1, 1, 1, 116), int32] */) /* ty=Tensor[(1, 14, 14, 116), int32] */;
  %218 = qnn.requantize(%217, meta[relay.Constant][169] /* ty=Tensor[(116), float32] span=Conv_151_quant:0:0 */, 0 /* ty=int32 span=Conv_151_quant:0:0 */, 0.0381442f /* ty=float32 span=Conv_151_quant:0:0 */, 113 /* ty=int32 span=Conv_151_quant:0:0 */, axis=3, rounding="UPWARD", compute_dtype="int64", out_dtype="uint8") /* ty=Tensor[(1, 14, 14, 116), uint8] span=Conv_151_quant:0:0 */;
  %219 = qnn.conv2d(%218, meta[relay.Constant][170] /* ty=Tensor[(1, 1, 116, 116), int8] */, 113 /* ty=int32 span=fused Conv_153_quant:0:0 */, meta[relay.Constant][171] /* ty=Tensor[(116), int32] span=fused Conv_153_quant:0:0 */, 0.0381442f /* ty=float32 span=fused Conv_153_quant:0:0 */, meta[relay.Constant][172] /* ty=Tensor[(116), float32] span=fused Conv_153_quant:0:0 */, padding=[0, 0, 0, 0], channels=116, kernel_size=[1, 1], data_layout="NHWC", kernel_layout="HWIO", out_dtype="int32") /* ty=Tensor[(1, 14, 14, 116), int32] span=fused Conv_153_quant:0:0 */;
  %220 = add(%219, meta[relay.Constant][173] /* ty=Tensor[(1, 1, 1, 116), int32] */) /* ty=Tensor[(1, 14, 14, 116), int32] */;
  %221 = qnn.requantize(%220, meta[relay.Constant][174] /* ty=Tensor[(116), float32] span=fused Conv_153_quant:0:0 */, 0 /* ty=int32 span=fused Conv_153_quant:0:0 */, 0.0116791f /* ty=float32 span=fused Conv_153_quant:0:0 */, 0 /* ty=int32 span=fused Conv_153_quant:0:0 */, axis=3, rounding="UPWARD", compute_dtype="int64", out_dtype="uint8") /* ty=Tensor[(1, 14, 14, 116), uint8] span=fused Conv_153_quant:0:0 */;
  %222 = qnn.dequantize(%221, 0.0116791f /* ty=float32 span=fused Conv_153_quant.502_scale:0:0 */, 0 /* ty=int32 span=502_DequantizeLinear:0:0 */, out_dtype="float32", axis=0) /* ty=Tensor[(1, 14, 14, 116), float32] span=502_DequantizeLinear:0:0 */;
  %223 = qnn.dequantize(%210, 0.0148973f /* ty=float32 span=492_QuantizeLinear.492_scale:0:0 */, 0 /* ty=int32 span=493_DequantizeLinear:0:0 */, out_dtype="float32", axis=0) /* ty=Tensor[(1, 116, 14, 14), float32] span=493_DequantizeLinear:0:0 */;
  %224 = layout_transform(%222, src_layout="NHWC", dst_layout="NCHW") /* ty=Tensor[(1, 116, 14, 14), float32] */;
  %225 = (%223, %224) /* ty=(Tensor[(1, 116, 14, 14), float32], Tensor[(1, 116, 14, 14), float32]) span=Concat_156:0:0 */;
  %226 = concatenate(%225, axis=1) /* ty=Tensor[(1, 232, 14, 14), float32] span=Concat_156:0:0 */;
  %227 = reshape(%226, newshape=[1, 2, 116, 14, 14]) /* ty=Tensor[(1, 2, 116, 14, 14), float32] span=Reshape_158:0:0 */;
  %228 = transpose(%227, axes=[0, 2, 1, 3, 4]) /* ty=Tensor[(1, 116, 2, 14, 14), float32] span=Transpose_159:0:0 */;
  %229 = reshape(%228, newshape=[1, -1, 14, 14]) /* ty=Tensor[(1, 232, 14, 14), float32] span=Reshape_161:0:0 */;
  %230 = qnn.quantize(%229, 0.0116791f /* ty=float32 span=508_QuantizeLinear.508_scale:0:0 */, 0 /* ty=int32 span=508_QuantizeLinear:0:0 */, out_dtype="uint8", axis=0) /* ty=Tensor[(1, 232, 14, 14), uint8] span=508_QuantizeLinear:0:0 */;
  %231 = split(%230, indices_or_sections=[116], axis=1) /* ty=(Tensor[(1, 116, 14, 14), uint8], Tensor[(1, 116, 14, 14), uint8]) span=Split_162_quant:0:0 */;
  %232 = %231.0 /* ty=Tensor[(1, 116, 14, 14), uint8] */;
  %233 = %231.1 /* ty=Tensor[(1, 116, 14, 14), uint8] */;
  %234 = layout_transform(%233, src_layout="NCHW", dst_layout="NHWC") /* ty=Tensor[(1, 14, 14, 116), uint8] */;
  %235 = qnn.conv2d(%234, meta[relay.Constant][175] /* ty=Tensor[(1, 1, 116, 116), int8] */, 0 /* ty=int32 span=fused Conv_163_quant:0:0 */, meta[relay.Constant][176] /* ty=Tensor[(116), int32] span=fused Conv_163_quant:0:0 */, 0.0116791f /* ty=float32 span=fused Conv_163_quant:0:0 */, meta[relay.Constant][177] /* ty=Tensor[(116), float32] span=fused Conv_163_quant:0:0 */, padding=[0, 0, 0, 0], channels=116, kernel_size=[1, 1], data_layout="NHWC", kernel_layout="HWIO", out_dtype="int32") /* ty=Tensor[(1, 14, 14, 116), int32] span=fused Conv_163_quant:0:0 */;
  %236 = add(%235, meta[relay.Constant][178] /* ty=Tensor[(1, 1, 1, 116), int32] */) /* ty=Tensor[(1, 14, 14, 116), int32] */;
  %237 = qnn.requantize(%236, meta[relay.Constant][179] /* ty=Tensor[(116), float32] span=fused Conv_163_quant:0:0 */, 0 /* ty=int32 span=fused Conv_163_quant:0:0 */, 0.0084726f /* ty=float32 span=fused Conv_163_quant:0:0 */, 0 /* ty=int32 span=fused Conv_163_quant:0:0 */, axis=3, rounding="UPWARD", compute_dtype="int64", out_dtype="uint8") /* ty=Tensor[(1, 14, 14, 116), uint8] span=fused Conv_163_quant:0:0 */;
  %238 = qnn.conv2d(%237, meta[relay.Constant][180] /* ty=Tensor[(3, 3, 116, 1), int8] */, 0 /* ty=int32 span=Conv_166_quant:0:0 */, meta[relay.Constant][181] /* ty=Tensor[(116), int32] span=Conv_166_quant:0:0 */, 0.0084726f /* ty=float32 span=Conv_166_quant:0:0 */, meta[relay.Constant][182] /* ty=Tensor[(116), float32] span=Conv_166_quant:0:0 */, padding=[1, 1, 1, 1], groups=116, channels=116, kernel_size=[3, 3], data_layout="NHWC", kernel_layout="HWOI", out_dtype="int32") /* ty=Tensor[(1, 14, 14, 116), int32] span=Conv_166_quant:0:0 */;
  %239 = add(%238, meta[relay.Constant][183] /* ty=Tensor[(1, 1, 1, 116), int32] */) /* ty=Tensor[(1, 14, 14, 116), int32] */;
  %240 = qnn.requantize(%239, meta[relay.Constant][184] /* ty=Tensor[(116), float32] span=Conv_166_quant:0:0 */, 0 /* ty=int32 span=Conv_166_quant:0:0 */, 0.0382486f /* ty=float32 span=Conv_166_quant:0:0 */, 152 /* ty=int32 span=Conv_166_quant:0:0 */, axis=3, rounding="UPWARD", compute_dtype="int64", out_dtype="uint8") /* ty=Tensor[(1, 14, 14, 116), uint8] span=Conv_166_quant:0:0 */;
  %241 = qnn.conv2d(%240, meta[relay.Constant][185] /* ty=Tensor[(1, 1, 116, 116), int8] */, 152 /* ty=int32 span=fused Conv_168_quant:0:0 */, meta[relay.Constant][186] /* ty=Tensor[(116), int32] span=fused Conv_168_quant:0:0 */, 0.0382486f /* ty=float32 span=fused Conv_168_quant:0:0 */, meta[relay.Constant][187] /* ty=Tensor[(116), float32] span=fused Conv_168_quant:0:0 */, padding=[0, 0, 0, 0], channels=116, kernel_size=[1, 1], data_layout="NHWC", kernel_layout="HWIO", out_dtype="int32") /* ty=Tensor[(1, 14, 14, 116), int32] span=fused Conv_168_quant:0:0 */;
  %242 = add(%241, meta[relay.Constant][188] /* ty=Tensor[(1, 1, 1, 116), int32] */) /* ty=Tensor[(1, 14, 14, 116), int32] */;
  %243 = qnn.requantize(%242, meta[relay.Constant][189] /* ty=Tensor[(116), float32] span=fused Conv_168_quant:0:0 */, 0 /* ty=int32 span=fused Conv_168_quant:0:0 */, 0.0139776f /* ty=float32 span=fused Conv_168_quant:0:0 */, 0 /* ty=int32 span=fused Conv_168_quant:0:0 */, axis=3, rounding="UPWARD", compute_dtype="int64", out_dtype="uint8") /* ty=Tensor[(1, 14, 14, 116), uint8] span=fused Conv_168_quant:0:0 */;
  %244 = qnn.dequantize(%243, 0.0139776f /* ty=float32 span=fused Conv_168_quant.518_scale:0:0 */, 0 /* ty=int32 span=518_DequantizeLinear:0:0 */, out_dtype="float32", axis=0) /* ty=Tensor[(1, 14, 14, 116), float32] span=518_DequantizeLinear:0:0 */;
  %245 = qnn.dequantize(%232, 0.0116791f /* ty=float32 span=508_QuantizeLinear.508_scale:0:0 */, 0 /* ty=int32 span=509_DequantizeLinear:0:0 */, out_dtype="float32", axis=0) /* ty=Tensor[(1, 116, 14, 14), float32] span=509_DequantizeLinear:0:0 */;
  %246 = layout_transform(%244, src_layout="NHWC", dst_layout="NCHW") /* ty=Tensor[(1, 116, 14, 14), float32] */;
  %247 = (%245, %246) /* ty=(Tensor[(1, 116, 14, 14), float32], Tensor[(1, 116, 14, 14), float32]) span=Concat_171:0:0 */;
  %248 = concatenate(%247, axis=1) /* ty=Tensor[(1, 232, 14, 14), float32] span=Concat_171:0:0 */;
  %249 = reshape(%248, newshape=[1, 2, 116, 14, 14]) /* ty=Tensor[(1, 2, 116, 14, 14), float32] span=Reshape_173:0:0 */;
  %250 = transpose(%249, axes=[0, 2, 1, 3, 4]) /* ty=Tensor[(1, 116, 2, 14, 14), float32] span=Transpose_174:0:0 */;
  %251 = reshape(%250, newshape=[1, -1, 14, 14]) /* ty=Tensor[(1, 232, 14, 14), float32] span=Reshape_176:0:0 */;
  %252 = qnn.quantize(%251, 0.0139776f /* ty=float32 span=524_QuantizeLinear.524_scale:0:0 */, 0 /* ty=int32 span=524_QuantizeLinear:0:0 */, out_dtype="uint8", axis=0) /* ty=Tensor[(1, 232, 14, 14), uint8] span=524_QuantizeLinear:0:0 */;
  %253 = split(%252, indices_or_sections=[116], axis=1) /* ty=(Tensor[(1, 116, 14, 14), uint8], Tensor[(1, 116, 14, 14), uint8]) span=Split_177_quant:0:0 */;
  %254 = %253.0 /* ty=Tensor[(1, 116, 14, 14), uint8] */;
  %255 = %253.1 /* ty=Tensor[(1, 116, 14, 14), uint8] */;
  %256 = layout_transform(%255, src_layout="NCHW", dst_layout="NHWC") /* ty=Tensor[(1, 14, 14, 116), uint8] */;
  %257 = qnn.conv2d(%256, meta[relay.Constant][190] /* ty=Tensor[(1, 1, 116, 116), int8] */, 0 /* ty=int32 span=fused Conv_178_quant:0:0 */, meta[relay.Constant][191] /* ty=Tensor[(116), int32] span=fused Conv_178_quant:0:0 */, 0.0139776f /* ty=float32 span=fused Conv_178_quant:0:0 */, meta[relay.Constant][192] /* ty=Tensor[(116), float32] span=fused Conv_178_quant:0:0 */, padding=[0, 0, 0, 0], channels=116, kernel_size=[1, 1], data_layout="NHWC", kernel_layout="HWIO", out_dtype="int32") /* ty=Tensor[(1, 14, 14, 116), int32] span=fused Conv_178_quant:0:0 */;
  %258 = add(%257, meta[relay.Constant][193] /* ty=Tensor[(1, 1, 1, 116), int32] */) /* ty=Tensor[(1, 14, 14, 116), int32] */;
  %259 = qnn.requantize(%258, meta[relay.Constant][194] /* ty=Tensor[(116), float32] span=fused Conv_178_quant:0:0 */, 0 /* ty=int32 span=fused Conv_178_quant:0:0 */, 0.00988168f /* ty=float32 span=fused Conv_178_quant:0:0 */, 0 /* ty=int32 span=fused Conv_178_quant:0:0 */, axis=3, rounding="UPWARD", compute_dtype="int64", out_dtype="uint8") /* ty=Tensor[(1, 14, 14, 116), uint8] span=fused Conv_178_quant:0:0 */;
  %260 = qnn.conv2d(%259, meta[relay.Constant][195] /* ty=Tensor[(3, 3, 116, 1), int8] */, 0 /* ty=int32 span=Conv_181_quant:0:0 */, meta[relay.Constant][196] /* ty=Tensor[(116), int32] span=Conv_181_quant:0:0 */, 0.00988168f /* ty=float32 span=Conv_181_quant:0:0 */, meta[relay.Constant][197] /* ty=Tensor[(116), float32] span=Conv_181_quant:0:0 */, padding=[1, 1, 1, 1], groups=116, channels=116, kernel_size=[3, 3], data_layout="NHWC", kernel_layout="HWOI", out_dtype="int32") /* ty=Tensor[(1, 14, 14, 116), int32] span=Conv_181_quant:0:0 */;
  %261 = add(%260, meta[relay.Constant][198] /* ty=Tensor[(1, 1, 1, 116), int32] */) /* ty=Tensor[(1, 14, 14, 116), int32] */;
  %262 = qnn.requantize(%261, meta[relay.Constant][199] /* ty=Tensor[(116), float32] span=Conv_181_quant:0:0 */, 0 /* ty=int32 span=Conv_181_quant:0:0 */, 0.0421829f /* ty=float32 span=Conv_181_quant:0:0 */, 144 /* ty=int32 span=Conv_181_quant:0:0 */, axis=3, rounding="UPWARD", compute_dtype="int64", out_dtype="uint8") /* ty=Tensor[(1, 14, 14, 116), uint8] span=Conv_181_quant:0:0 */;
  %263 = qnn.conv2d(%262, meta[relay.Constant][200] /* ty=Tensor[(1, 1, 116, 116), int8] */, 144 /* ty=int32 span=fused Conv_183_quant:0:0 */, meta[relay.Constant][201] /* ty=Tensor[(116), int32] span=fused Conv_183_quant:0:0 */, 0.0421829f /* ty=float32 span=fused Conv_183_quant:0:0 */, meta[relay.Constant][202] /* ty=Tensor[(116), float32] span=fused Conv_183_quant:0:0 */, padding=[0, 0, 0, 0], channels=116, kernel_size=[1, 1], data_layout="NHWC", kernel_layout="HWIO", out_dtype="int32") /* ty=Tensor[(1, 14, 14, 116), int32] span=fused Conv_183_quant:0:0 */;
  %264 = add(%263, meta[relay.Constant][203] /* ty=Tensor[(1, 1, 1, 116), int32] */) /* ty=Tensor[(1, 14, 14, 116), int32] */;
  %265 = qnn.requantize(%264, meta[relay.Constant][204] /* ty=Tensor[(116), float32] span=fused Conv_183_quant:0:0 */, 0 /* ty=int32 span=fused Conv_183_quant:0:0 */, 0.016239f /* ty=float32 span=fused Conv_183_quant:0:0 */, 0 /* ty=int32 span=fused Conv_183_quant:0:0 */, axis=3, rounding="UPWARD", compute_dtype="int64", out_dtype="uint8") /* ty=Tensor[(1, 14, 14, 116), uint8] span=fused Conv_183_quant:0:0 */;
  %266 = qnn.dequantize(%265, 0.016239f /* ty=float32 span=fused Conv_183_quant.534_scale:0:0 */, 0 /* ty=int32 span=534_DequantizeLinear:0:0 */, out_dtype="float32", axis=0) /* ty=Tensor[(1, 14, 14, 116), float32] span=534_DequantizeLinear:0:0 */;
  %267 = qnn.dequantize(%254, 0.0139776f /* ty=float32 span=524_QuantizeLinear.524_scale:0:0 */, 0 /* ty=int32 span=525_DequantizeLinear:0:0 */, out_dtype="float32", axis=0) /* ty=Tensor[(1, 116, 14, 14), float32] span=525_DequantizeLinear:0:0 */;
  %268 = layout_transform(%266, src_layout="NHWC", dst_layout="NCHW") /* ty=Tensor[(1, 116, 14, 14), float32] */;
  %269 = (%267, %268) /* ty=(Tensor[(1, 116, 14, 14), float32], Tensor[(1, 116, 14, 14), float32]) span=Concat_186:0:0 */;
  %270 = concatenate(%269, axis=1) /* ty=Tensor[(1, 232, 14, 14), float32] span=Concat_186:0:0 */;
  %271 = reshape(%270, newshape=[1, 2, 116, 14, 14]) /* ty=Tensor[(1, 2, 116, 14, 14), float32] span=Reshape_188:0:0 */;
  %272 = transpose(%271, axes=[0, 2, 1, 3, 4]) /* ty=Tensor[(1, 116, 2, 14, 14), float32] span=Transpose_189:0:0 */;
  %273 = reshape(%272, newshape=[1, -1, 14, 14]) /* ty=Tensor[(1, 232, 14, 14), float32] span=Reshape_191:0:0 */;
  %274 = qnn.quantize(%273, 0.016239f /* ty=float32 span=540_QuantizeLinear.540_scale:0:0 */, 0 /* ty=int32 span=540_QuantizeLinear:0:0 */, out_dtype="uint8", axis=0) /* ty=Tensor[(1, 232, 14, 14), uint8] span=540_QuantizeLinear:0:0 */;
  %275 = layout_transform(%274, src_layout="NCHW", dst_layout="NHWC") /* ty=Tensor[(1, 14, 14, 232), uint8] */;
  %276 = qnn.conv2d(%275, meta[relay.Constant][205] /* ty=Tensor[(3, 3, 232, 1), int8] */, 0 /* ty=int32 span=Conv_192_quant:0:0 */, meta[relay.Constant][206] /* ty=Tensor[(232), int32] span=Conv_192_quant:0:0 */, 0.016239f /* ty=float32 span=Conv_192_quant:0:0 */, meta[relay.Constant][207] /* ty=Tensor[(232), float32] span=Conv_192_quant:0:0 */, strides=[2, 2], padding=[1, 1, 1, 1], groups=232, channels=232, kernel_size=[3, 3], data_layout="NHWC", kernel_layout="HWOI", out_dtype="int32") /* ty=Tensor[(1, 7, 7, 232), int32] span=Conv_192_quant:0:0 */;
  %277 = add(%276, meta[relay.Constant][208] /* ty=Tensor[(1, 1, 1, 232), int32] */) /* ty=Tensor[(1, 7, 7, 232), int32] */;
  %278 = qnn.requantize(%277, meta[relay.Constant][209] /* ty=Tensor[(232), float32] span=Conv_192_quant:0:0 */, 0 /* ty=int32 span=Conv_192_quant:0:0 */, 0.0334291f /* ty=float32 span=Conv_192_quant:0:0 */, 117 /* ty=int32 span=Conv_192_quant:0:0 */, axis=3, rounding="UPWARD", compute_dtype="int64", out_dtype="uint8") /* ty=Tensor[(1, 7, 7, 232), uint8] span=Conv_192_quant:0:0 */;
  %279 = qnn.conv2d(%278, meta[relay.Constant][210] /* ty=Tensor[(1, 1, 232, 232), int8] */, 117 /* ty=int32 span=fused Conv_194_quant:0:0 */, meta[relay.Constant][211] /* ty=Tensor[(232), int32] span=fused Conv_194_quant:0:0 */, 0.0334291f /* ty=float32 span=fused Conv_194_quant:0:0 */, meta[relay.Constant][212] /* ty=Tensor[(232), float32] span=fused Conv_194_quant:0:0 */, padding=[0, 0, 0, 0], channels=232, kernel_size=[1, 1], data_layout="NHWC", kernel_layout="HWIO", out_dtype="int32") /* ty=Tensor[(1, 7, 7, 232), int32] span=fused Conv_194_quant:0:0 */;
  %280 = add(%279, meta[relay.Constant][213] /* ty=Tensor[(1, 1, 1, 232), int32] */) /* ty=Tensor[(1, 7, 7, 232), int32] */;
  %281 = qnn.requantize(%280, meta[relay.Constant][214] /* ty=Tensor[(232), float32] span=fused Conv_194_quant:0:0 */, 0 /* ty=int32 span=fused Conv_194_quant:0:0 */, 0.0108523f /* ty=float32 span=fused Conv_194_quant:0:0 */, 0 /* ty=int32 span=fused Conv_194_quant:0:0 */, axis=3, rounding="UPWARD", compute_dtype="int64", out_dtype="uint8") /* ty=Tensor[(1, 7, 7, 232), uint8] span=fused Conv_194_quant:0:0 */;
  %282 = qnn.conv2d(%275, meta[relay.Constant][215] /* ty=Tensor[(1, 1, 232, 232), int8] */, 0 /* ty=int32 span=fused Conv_197_quant:0:0 */, meta[relay.Constant][216] /* ty=Tensor[(232), int32] span=fused Conv_197_quant:0:0 */, 0.016239f /* ty=float32 span=fused Conv_197_quant:0:0 */, meta[relay.Constant][217] /* ty=Tensor[(232), float32] span=fused Conv_197_quant:0:0 */, padding=[0, 0, 0, 0], channels=232, kernel_size=[1, 1], data_layout="NHWC", kernel_layout="HWIO", out_dtype="int32") /* ty=Tensor[(1, 14, 14, 232), int32] span=fused Conv_197_quant:0:0 */;
  %283 = add(%282, meta[relay.Constant][218] /* ty=Tensor[(1, 1, 1, 232), int32] */) /* ty=Tensor[(1, 14, 14, 232), int32] */;
  %284 = qnn.requantize(%283, meta[relay.Constant][219] /* ty=Tensor[(232), float32] span=fused Conv_197_quant:0:0 */, 0 /* ty=int32 span=fused Conv_197_quant:0:0 */, 0.0111939f /* ty=float32 span=fused Conv_197_quant:0:0 */, 0 /* ty=int32 span=fused Conv_197_quant:0:0 */, axis=3, rounding="UPWARD", compute_dtype="int64", out_dtype="uint8") /* ty=Tensor[(1, 14, 14, 232), uint8] span=fused Conv_197_quant:0:0 */;
  %285 = qnn.conv2d(%284, meta[relay.Constant][220] /* ty=Tensor[(3, 3, 232, 1), int8] */, 0 /* ty=int32 span=Conv_200_quant:0:0 */, meta[relay.Constant][221] /* ty=Tensor[(232), int32] span=Conv_200_quant:0:0 */, 0.0111939f /* ty=float32 span=Conv_200_quant:0:0 */, meta[relay.Constant][222] /* ty=Tensor[(232), float32] span=Conv_200_quant:0:0 */, strides=[2, 2], padding=[1, 1, 1, 1], groups=232, channels=232, kernel_size=[3, 3], data_layout="NHWC", kernel_layout="HWOI", out_dtype="int32") /* ty=Tensor[(1, 7, 7, 232), int32] span=Conv_200_quant:0:0 */;
  %286 = add(%285, meta[relay.Constant][223] /* ty=Tensor[(1, 1, 1, 232), int32] */) /* ty=Tensor[(1, 7, 7, 232), int32] */;
  %287 = qnn.requantize(%286, meta[relay.Constant][224] /* ty=Tensor[(232), float32] span=Conv_200_quant:0:0 */, 0 /* ty=int32 span=Conv_200_quant:0:0 */, 0.048227f /* ty=float32 span=Conv_200_quant:0:0 */, 128 /* ty=int32 span=Conv_200_quant:0:0 */, axis=3, rounding="UPWARD", compute_dtype="int64", out_dtype="uint8") /* ty=Tensor[(1, 7, 7, 232), uint8] span=Conv_200_quant:0:0 */;
  %288 = qnn.conv2d(%287, meta[relay.Constant][225] /* ty=Tensor[(1, 1, 232, 232), int8] */, 128 /* ty=int32 span=fused Conv_202_quant:0:0 */, meta[relay.Constant][226] /* ty=Tensor[(232), int32] span=fused Conv_202_quant:0:0 */, 0.048227f /* ty=float32 span=fused Conv_202_quant:0:0 */, meta[relay.Constant][227] /* ty=Tensor[(232), float32] span=fused Conv_202_quant:0:0 */, padding=[0, 0, 0, 0], channels=232, kernel_size=[1, 1], data_layout="NHWC", kernel_layout="HWIO", out_dtype="int32") /* ty=Tensor[(1, 7, 7, 232), int32] span=fused Conv_202_quant:0:0 */;
  %289 = add(%288, meta[relay.Constant][228] /* ty=Tensor[(1, 1, 1, 232), int32] */) /* ty=Tensor[(1, 7, 7, 232), int32] */;
  %290 = qnn.requantize(%289, meta[relay.Constant][229] /* ty=Tensor[(232), float32] span=fused Conv_202_quant:0:0 */, 0 /* ty=int32 span=fused Conv_202_quant:0:0 */, 0.0197203f /* ty=float32 span=fused Conv_202_quant:0:0 */, 0 /* ty=int32 span=fused Conv_202_quant:0:0 */, axis=3, rounding="UPWARD", compute_dtype="int64", out_dtype="uint8") /* ty=Tensor[(1, 7, 7, 232), uint8] span=fused Conv_202_quant:0:0 */;
  %291 = qnn.dequantize(%281, 0.0108523f /* ty=float32 span=fused Conv_194_quant.545_scale:0:0 */, 0 /* ty=int32 span=545_DequantizeLinear:0:0 */, out_dtype="float32", axis=0) /* ty=Tensor[(1, 7, 7, 232), float32] span=545_DequantizeLinear:0:0 */;
  %292 = qnn.dequantize(%290, 0.0197203f /* ty=float32 span=fused Conv_202_quant.553_scale:0:0 */, 0 /* ty=int32 span=553_DequantizeLinear:0:0 */, out_dtype="float32", axis=0) /* ty=Tensor[(1, 7, 7, 232), float32] span=553_DequantizeLinear:0:0 */;
  %293 = (%291, %292) /* ty=(Tensor[(1, 7, 7, 232), float32], Tensor[(1, 7, 7, 232), float32]) span=Concat_205:0:0 */;
  %294 = concatenate(%293, axis=3) /* ty=Tensor[(1, 7, 7, 464), float32] span=Concat_205:0:0 */;
  %295 = layout_transform(%294, src_layout="NHWC", dst_layout="NCHW") /* ty=Tensor[(1, 464, 7, 7), float32] */;
  %296 = reshape(%295, newshape=[1, 2, 232, 7, 7]) /* ty=Tensor[(1, 2, 232, 7, 7), float32] span=Reshape_207:0:0 */;
  %297 = transpose(%296, axes=[0, 2, 1, 3, 4]) /* ty=Tensor[(1, 232, 2, 7, 7), float32] span=Transpose_208:0:0 */;
  %298 = reshape(%297, newshape=[1, -1, 7, 7]) /* ty=Tensor[(1, 464, 7, 7), float32] span=Reshape_210:0:0 */;
  %299 = qnn.quantize(%298, 0.0197203f /* ty=float32 span=559_QuantizeLinear.559_scale:0:0 */, 0 /* ty=int32 span=559_QuantizeLinear:0:0 */, out_dtype="uint8", axis=0) /* ty=Tensor[(1, 464, 7, 7), uint8] span=559_QuantizeLinear:0:0 */;
  %300 = split(%299, indices_or_sections=[232], axis=1) /* ty=(Tensor[(1, 232, 7, 7), uint8], Tensor[(1, 232, 7, 7), uint8]) span=Split_211_quant:0:0 */;
  %301 = %300.0 /* ty=Tensor[(1, 232, 7, 7), uint8] */;
  %302 = %300.1 /* ty=Tensor[(1, 232, 7, 7), uint8] */;
  %303 = layout_transform(%302, src_layout="NCHW", dst_layout="NHWC") /* ty=Tensor[(1, 7, 7, 232), uint8] */;
  %304 = qnn.conv2d(%303, meta[relay.Constant][230] /* ty=Tensor[(1, 1, 232, 232), int8] */, 0 /* ty=int32 span=fused Conv_212_quant:0:0 */, meta[relay.Constant][231] /* ty=Tensor[(232), int32] span=fused Conv_212_quant:0:0 */, 0.0197203f /* ty=float32 span=fused Conv_212_quant:0:0 */, meta[relay.Constant][232] /* ty=Tensor[(232), float32] span=fused Conv_212_quant:0:0 */, padding=[0, 0, 0, 0], channels=232, kernel_size=[1, 1], data_layout="NHWC", kernel_layout="HWIO", out_dtype="int32") /* ty=Tensor[(1, 7, 7, 232), int32] span=fused Conv_212_quant:0:0 */;
  %305 = add(%304, meta[relay.Constant][233] /* ty=Tensor[(1, 1, 1, 232), int32] */) /* ty=Tensor[(1, 7, 7, 232), int32] */;
  %306 = qnn.requantize(%305, meta[relay.Constant][234] /* ty=Tensor[(232), float32] span=fused Conv_212_quant:0:0 */, 0 /* ty=int32 span=fused Conv_212_quant:0:0 */, 0.0112321f /* ty=float32 span=fused Conv_212_quant:0:0 */, 0 /* ty=int32 span=fused Conv_212_quant:0:0 */, axis=3, rounding="UPWARD", compute_dtype="int64", out_dtype="uint8") /* ty=Tensor[(1, 7, 7, 232), uint8] span=fused Conv_212_quant:0:0 */;
  %307 = qnn.conv2d(%306, meta[relay.Constant][235] /* ty=Tensor[(3, 3, 232, 1), int8] */, 0 /* ty=int32 span=Conv_215_quant:0:0 */, meta[relay.Constant][236] /* ty=Tensor[(232), int32] span=Conv_215_quant:0:0 */, 0.0112321f /* ty=float32 span=Conv_215_quant:0:0 */, meta[relay.Constant][237] /* ty=Tensor[(232), float32] span=Conv_215_quant:0:0 */, padding=[1, 1, 1, 1], groups=232, channels=232, kernel_size=[3, 3], data_layout="NHWC", kernel_layout="HWOI", out_dtype="int32") /* ty=Tensor[(1, 7, 7, 232), int32] span=Conv_215_quant:0:0 */;
  %308 = add(%307, meta[relay.Constant][238] /* ty=Tensor[(1, 1, 1, 232), int32] */) /* ty=Tensor[(1, 7, 7, 232), int32] */;
  %309 = qnn.requantize(%308, meta[relay.Constant][239] /* ty=Tensor[(232), float32] span=Conv_215_quant:0:0 */, 0 /* ty=int32 span=Conv_215_quant:0:0 */, 0.0430797f /* ty=float32 span=Conv_215_quant:0:0 */, 124 /* ty=int32 span=Conv_215_quant:0:0 */, axis=3, rounding="UPWARD", compute_dtype="int64", out_dtype="uint8") /* ty=Tensor[(1, 7, 7, 232), uint8] span=Conv_215_quant:0:0 */;
  %310 = qnn.conv2d(%309, meta[relay.Constant][240] /* ty=Tensor[(1, 1, 232, 232), int8] */, 124 /* ty=int32 span=fused Conv_217_quant:0:0 */, meta[relay.Constant][241] /* ty=Tensor[(232), int32] span=fused Conv_217_quant:0:0 */, 0.0430797f /* ty=float32 span=fused Conv_217_quant:0:0 */, meta[relay.Constant][242] /* ty=Tensor[(232), float32] span=fused Conv_217_quant:0:0 */, padding=[0, 0, 0, 0], channels=232, kernel_size=[1, 1], data_layout="NHWC", kernel_layout="HWIO", out_dtype="int32") /* ty=Tensor[(1, 7, 7, 232), int32] span=fused Conv_217_quant:0:0 */;
  %311 = add(%310, meta[relay.Constant][243] /* ty=Tensor[(1, 1, 1, 232), int32] */) /* ty=Tensor[(1, 7, 7, 232), int32] */;
  %312 = qnn.requantize(%311, meta[relay.Constant][244] /* ty=Tensor[(232), float32] span=fused Conv_217_quant:0:0 */, 0 /* ty=int32 span=fused Conv_217_quant:0:0 */, 0.0173247f /* ty=float32 span=fused Conv_217_quant:0:0 */, 0 /* ty=int32 span=fused Conv_217_quant:0:0 */, axis=3, rounding="UPWARD", compute_dtype="int64", out_dtype="uint8") /* ty=Tensor[(1, 7, 7, 232), uint8] span=fused Conv_217_quant:0:0 */;
  %313 = qnn.dequantize(%312, 0.0173247f /* ty=float32 span=fused Conv_217_quant.569_scale:0:0 */, 0 /* ty=int32 span=569_DequantizeLinear:0:0 */, out_dtype="float32", axis=0) /* ty=Tensor[(1, 7, 7, 232), float32] span=569_DequantizeLinear:0:0 */;
  %314 = qnn.dequantize(%301, 0.0197203f /* ty=float32 span=559_QuantizeLinear.559_scale:0:0 */, 0 /* ty=int32 span=560_DequantizeLinear:0:0 */, out_dtype="float32", axis=0) /* ty=Tensor[(1, 232, 7, 7), float32] span=560_DequantizeLinear:0:0 */;
  %315 = layout_transform(%313, src_layout="NHWC", dst_layout="NCHW") /* ty=Tensor[(1, 232, 7, 7), float32] */;
  %316 = (%314, %315) /* ty=(Tensor[(1, 232, 7, 7), float32], Tensor[(1, 232, 7, 7), float32]) span=Concat_220:0:0 */;
  %317 = concatenate(%316, axis=1) /* ty=Tensor[(1, 464, 7, 7), float32] span=Concat_220:0:0 */;
  %318 = reshape(%317, newshape=[1, 2, 232, 7, 7]) /* ty=Tensor[(1, 2, 232, 7, 7), float32] span=Reshape_222:0:0 */;
  %319 = transpose(%318, axes=[0, 2, 1, 3, 4]) /* ty=Tensor[(1, 232, 2, 7, 7), float32] span=Transpose_223:0:0 */;
  %320 = reshape(%319, newshape=[1, -1, 7, 7]) /* ty=Tensor[(1, 464, 7, 7), float32] span=Reshape_225:0:0 */;
  %321 = qnn.quantize(%320, 0.0173247f /* ty=float32 span=575_QuantizeLinear.575_scale:0:0 */, 0 /* ty=int32 span=575_QuantizeLinear:0:0 */, out_dtype="uint8", axis=0) /* ty=Tensor[(1, 464, 7, 7), uint8] span=575_QuantizeLinear:0:0 */;
  %322 = split(%321, indices_or_sections=[232], axis=1) /* ty=(Tensor[(1, 232, 7, 7), uint8], Tensor[(1, 232, 7, 7), uint8]) span=Split_226_quant:0:0 */;
  %323 = %322.0 /* ty=Tensor[(1, 232, 7, 7), uint8] */;
  %324 = %322.1 /* ty=Tensor[(1, 232, 7, 7), uint8] */;
  %325 = layout_transform(%324, src_layout="NCHW", dst_layout="NHWC") /* ty=Tensor[(1, 7, 7, 232), uint8] */;
  %326 = qnn.conv2d(%325, meta[relay.Constant][245] /* ty=Tensor[(1, 1, 232, 232), int8] */, 0 /* ty=int32 span=fused Conv_227_quant:0:0 */, meta[relay.Constant][246] /* ty=Tensor[(232), int32] span=fused Conv_227_quant:0:0 */, 0.0173247f /* ty=float32 span=fused Conv_227_quant:0:0 */, meta[relay.Constant][247] /* ty=Tensor[(232), float32] span=fused Conv_227_quant:0:0 */, padding=[0, 0, 0, 0], channels=232, kernel_size=[1, 1], data_layout="NHWC", kernel_layout="HWIO", out_dtype="int32") /* ty=Tensor[(1, 7, 7, 232), int32] span=fused Conv_227_quant:0:0 */;
  %327 = add(%326, meta[relay.Constant][248] /* ty=Tensor[(1, 1, 1, 232), int32] */) /* ty=Tensor[(1, 7, 7, 232), int32] */;
  %328 = qnn.requantize(%327, meta[relay.Constant][249] /* ty=Tensor[(232), float32] span=fused Conv_227_quant:0:0 */, 0 /* ty=int32 span=fused Conv_227_quant:0:0 */, 0.0113337f /* ty=float32 span=fused Conv_227_quant:0:0 */, 0 /* ty=int32 span=fused Conv_227_quant:0:0 */, axis=3, rounding="UPWARD", compute_dtype="int64", out_dtype="uint8") /* ty=Tensor[(1, 7, 7, 232), uint8] span=fused Conv_227_quant:0:0 */;
  %329 = qnn.conv2d(%328, meta[relay.Constant][250] /* ty=Tensor[(3, 3, 232, 1), int8] */, 0 /* ty=int32 span=Conv_230_quant:0:0 */, meta[relay.Constant][251] /* ty=Tensor[(232), int32] span=Conv_230_quant:0:0 */, 0.0113337f /* ty=float32 span=Conv_230_quant:0:0 */, meta[relay.Constant][252] /* ty=Tensor[(232), float32] span=Conv_230_quant:0:0 */, padding=[1, 1, 1, 1], groups=232, channels=232, kernel_size=[3, 3], data_layout="NHWC", kernel_layout="HWOI", out_dtype="int32") /* ty=Tensor[(1, 7, 7, 232), int32] span=Conv_230_quant:0:0 */;
  %330 = add(%329, meta[relay.Constant][253] /* ty=Tensor[(1, 1, 1, 232), int32] */) /* ty=Tensor[(1, 7, 7, 232), int32] */;
  %331 = qnn.requantize(%330, meta[relay.Constant][254] /* ty=Tensor[(232), float32] span=Conv_230_quant:0:0 */, 0 /* ty=int32 span=Conv_230_quant:0:0 */, 0.0407685f /* ty=float32 span=Conv_230_quant:0:0 */, 141 /* ty=int32 span=Conv_230_quant:0:0 */, axis=3, rounding="UPWARD", compute_dtype="int64", out_dtype="uint8") /* ty=Tensor[(1, 7, 7, 232), uint8] span=Conv_230_quant:0:0 */;
  %332 = qnn.conv2d(%331, meta[relay.Constant][255] /* ty=Tensor[(1, 1, 232, 232), int8] */, 141 /* ty=int32 span=fused Conv_232_quant:0:0 */, meta[relay.Constant][256] /* ty=Tensor[(232), int32] span=fused Conv_232_quant:0:0 */, 0.0407685f /* ty=float32 span=fused Conv_232_quant:0:0 */, meta[relay.Constant][257] /* ty=Tensor[(232), float32] span=fused Conv_232_quant:0:0 */, padding=[0, 0, 0, 0], channels=232, kernel_size=[1, 1], data_layout="NHWC", kernel_layout="HWIO", out_dtype="int32") /* ty=Tensor[(1, 7, 7, 232), int32] span=fused Conv_232_quant:0:0 */;
  %333 = add(%332, meta[relay.Constant][258] /* ty=Tensor[(1, 1, 1, 232), int32] */) /* ty=Tensor[(1, 7, 7, 232), int32] */;
  %334 = qnn.requantize(%333, meta[relay.Constant][259] /* ty=Tensor[(232), float32] span=fused Conv_232_quant:0:0 */, 0 /* ty=int32 span=fused Conv_232_quant:0:0 */, 0.0174625f /* ty=float32 span=fused Conv_232_quant:0:0 */, 0 /* ty=int32 span=fused Conv_232_quant:0:0 */, axis=3, rounding="UPWARD", compute_dtype="int64", out_dtype="uint8") /* ty=Tensor[(1, 7, 7, 232), uint8] span=fused Conv_232_quant:0:0 */;
  %335 = qnn.dequantize(%334, 0.0174625f /* ty=float32 span=fused Conv_232_quant.585_scale:0:0 */, 0 /* ty=int32 span=585_DequantizeLinear:0:0 */, out_dtype="float32", axis=0) /* ty=Tensor[(1, 7, 7, 232), float32] span=585_DequantizeLinear:0:0 */;
  %336 = qnn.dequantize(%323, 0.0173247f /* ty=float32 span=575_QuantizeLinear.575_scale:0:0 */, 0 /* ty=int32 span=576_DequantizeLinear:0:0 */, out_dtype="float32", axis=0) /* ty=Tensor[(1, 232, 7, 7), float32] span=576_DequantizeLinear:0:0 */;
  %337 = layout_transform(%335, src_layout="NHWC", dst_layout="NCHW") /* ty=Tensor[(1, 232, 7, 7), float32] */;
  %338 = (%336, %337) /* ty=(Tensor[(1, 232, 7, 7), float32], Tensor[(1, 232, 7, 7), float32]) span=Concat_235:0:0 */;
  %339 = concatenate(%338, axis=1) /* ty=Tensor[(1, 464, 7, 7), float32] span=Concat_235:0:0 */;
  %340 = reshape(%339, newshape=[1, 2, 232, 7, 7]) /* ty=Tensor[(1, 2, 232, 7, 7), float32] span=Reshape_237:0:0 */;
  %341 = transpose(%340, axes=[0, 2, 1, 3, 4]) /* ty=Tensor[(1, 232, 2, 7, 7), float32] span=Transpose_238:0:0 */;
  %342 = reshape(%341, newshape=[1, -1, 7, 7]) /* ty=Tensor[(1, 464, 7, 7), float32] span=Reshape_240:0:0 */;
  %343 = qnn.quantize(%342, 0.0174625f /* ty=float32 span=591_QuantizeLinear.591_scale:0:0 */, 0 /* ty=int32 span=591_QuantizeLinear:0:0 */, out_dtype="uint8", axis=0) /* ty=Tensor[(1, 464, 7, 7), uint8] span=591_QuantizeLinear:0:0 */;
  %344 = split(%343, indices_or_sections=[232], axis=1) /* ty=(Tensor[(1, 232, 7, 7), uint8], Tensor[(1, 232, 7, 7), uint8]) span=Split_241_quant:0:0 */;
  %345 = %344.0 /* ty=Tensor[(1, 232, 7, 7), uint8] */;
  %346 = %344.1 /* ty=Tensor[(1, 232, 7, 7), uint8] */;
  %347 = layout_transform(%346, src_layout="NCHW", dst_layout="NHWC") /* ty=Tensor[(1, 7, 7, 232), uint8] */;
  %348 = qnn.conv2d(%347, meta[relay.Constant][260] /* ty=Tensor[(1, 1, 232, 232), int8] */, 0 /* ty=int32 span=fused Conv_242_quant:0:0 */, meta[relay.Constant][261] /* ty=Tensor[(232), int32] span=fused Conv_242_quant:0:0 */, 0.0174625f /* ty=float32 span=fused Conv_242_quant:0:0 */, meta[relay.Constant][262] /* ty=Tensor[(232), float32] span=fused Conv_242_quant:0:0 */, padding=[0, 0, 0, 0], channels=232, kernel_size=[1, 1], data_layout="NHWC", kernel_layout="HWIO", out_dtype="int32") /* ty=Tensor[(1, 7, 7, 232), int32] span=fused Conv_242_quant:0:0 */;
  %349 = add(%348, meta[relay.Constant][263] /* ty=Tensor[(1, 1, 1, 232), int32] */) /* ty=Tensor[(1, 7, 7, 232), int32] */;
  %350 = qnn.requantize(%349, meta[relay.Constant][264] /* ty=Tensor[(232), float32] span=fused Conv_242_quant:0:0 */, 0 /* ty=int32 span=fused Conv_242_quant:0:0 */, 0.0098126f /* ty=float32 span=fused Conv_242_quant:0:0 */, 0 /* ty=int32 span=fused Conv_242_quant:0:0 */, axis=3, rounding="UPWARD", compute_dtype="int64", out_dtype="uint8") /* ty=Tensor[(1, 7, 7, 232), uint8] span=fused Conv_242_quant:0:0 */;
  %351 = qnn.conv2d(%350, meta[relay.Constant][265] /* ty=Tensor[(3, 3, 232, 1), int8] */, 0 /* ty=int32 span=Conv_245_quant:0:0 */, meta[relay.Constant][266] /* ty=Tensor[(232), int32] span=Conv_245_quant:0:0 */, 0.0098126f /* ty=float32 span=Conv_245_quant:0:0 */, meta[relay.Constant][267] /* ty=Tensor[(232), float32] span=Conv_245_quant:0:0 */, padding=[1, 1, 1, 1], groups=232, channels=232, kernel_size=[3, 3], data_layout="NHWC", kernel_layout="HWOI", out_dtype="int32") /* ty=Tensor[(1, 7, 7, 232), int32] span=Conv_245_quant:0:0 */;
  %352 = add(%351, meta[relay.Constant][268] /* ty=Tensor[(1, 1, 1, 232), int32] */) /* ty=Tensor[(1, 7, 7, 232), int32] */;
  %353 = qnn.requantize(%352, meta[relay.Constant][269] /* ty=Tensor[(232), float32] span=Conv_245_quant:0:0 */, 0 /* ty=int32 span=Conv_245_quant:0:0 */, 0.0482444f /* ty=float32 span=Conv_245_quant:0:0 */, 132 /* ty=int32 span=Conv_245_quant:0:0 */, axis=3, rounding="UPWARD", compute_dtype="int64", out_dtype="uint8") /* ty=Tensor[(1, 7, 7, 232), uint8] span=Conv_245_quant:0:0 */;
  %354 = qnn.conv2d(%353, meta[relay.Constant][270] /* ty=Tensor[(1, 1, 232, 232), int8] */, 132 /* ty=int32 span=fused Conv_247_quant:0:0 */, meta[relay.Constant][271] /* ty=Tensor[(232), int32] span=fused Conv_247_quant:0:0 */, 0.0482444f /* ty=float32 span=fused Conv_247_quant:0:0 */, meta[relay.Constant][272] /* ty=Tensor[(232), float32] span=fused Conv_247_quant:0:0 */, padding=[0, 0, 0, 0], channels=232, kernel_size=[1, 1], data_layout="NHWC", kernel_layout="HWIO", out_dtype="int32") /* ty=Tensor[(1, 7, 7, 232), int32] span=fused Conv_247_quant:0:0 */;
  %355 = add(%354, meta[relay.Constant][273] /* ty=Tensor[(1, 1, 1, 232), int32] */) /* ty=Tensor[(1, 7, 7, 232), int32] */;
  %356 = qnn.requantize(%355, meta[relay.Constant][274] /* ty=Tensor[(232), float32] span=fused Conv_247_quant:0:0 */, 0 /* ty=int32 span=fused Conv_247_quant:0:0 */, 0.019249f /* ty=float32 span=fused Conv_247_quant:0:0 */, 0 /* ty=int32 span=fused Conv_247_quant:0:0 */, axis=3, rounding="UPWARD", compute_dtype="int64", out_dtype="uint8") /* ty=Tensor[(1, 7, 7, 232), uint8] span=fused Conv_247_quant:0:0 */;
  %357 = qnn.dequantize(%356, 0.019249f /* ty=float32 span=fused Conv_247_quant.601_scale:0:0 */, 0 /* ty=int32 span=601_DequantizeLinear:0:0 */, out_dtype="float32", axis=0) /* ty=Tensor[(1, 7, 7, 232), float32] span=601_DequantizeLinear:0:0 */;
  %358 = qnn.dequantize(%345, 0.0174625f /* ty=float32 span=591_QuantizeLinear.591_scale:0:0 */, 0 /* ty=int32 span=592_DequantizeLinear:0:0 */, out_dtype="float32", axis=0) /* ty=Tensor[(1, 232, 7, 7), float32] span=592_DequantizeLinear:0:0 */;
  %359 = layout_transform(%357, src_layout="NHWC", dst_layout="NCHW") /* ty=Tensor[(1, 232, 7, 7), float32] */;
  %360 = (%358, %359) /* ty=(Tensor[(1, 232, 7, 7), float32], Tensor[(1, 232, 7, 7), float32]) span=Concat_250:0:0 */;
  %361 = concatenate(%360, axis=1) /* ty=Tensor[(1, 464, 7, 7), float32] span=Concat_250:0:0 */;
  %362 = reshape(%361, newshape=[1, 2, 232, 7, 7]) /* ty=Tensor[(1, 2, 232, 7, 7), float32] span=Reshape_252:0:0 */;
  %363 = transpose(%362, axes=[0, 2, 1, 3, 4]) /* ty=Tensor[(1, 232, 2, 7, 7), float32] span=Transpose_253:0:0 */;
  %364 = reshape(%363, newshape=[1, -1, 7, 7]) /* ty=Tensor[(1, 464, 7, 7), float32] span=Reshape_255:0:0 */;
  %365 = qnn.quantize(%364, 0.019249f /* ty=float32 span=607_QuantizeLinear.607_scale:0:0 */, 0 /* ty=int32 span=607_QuantizeLinear:0:0 */, out_dtype="uint8", axis=0) /* ty=Tensor[(1, 464, 7, 7), uint8] span=607_QuantizeLinear:0:0 */;
  %366 = layout_transform(%365, src_layout="NCHW", dst_layout="NHWC") /* ty=Tensor[(1, 7, 7, 464), uint8] */;
  %367 = qnn.conv2d(%366, meta[relay.Constant][275] /* ty=Tensor[(1, 1, 464, 1024), int8] */, 0 /* ty=int32 span=fused Conv_256_quant:0:0 */, meta[relay.Constant][276] /* ty=Tensor[(1024), int32] span=fused Conv_256_quant:0:0 */, 0.019249f /* ty=float32 span=fused Conv_256_quant:0:0 */, meta[relay.Constant][277] /* ty=Tensor[(1024), float32] span=fused Conv_256_quant:0:0 */, padding=[0, 0, 0, 0], channels=1024, kernel_size=[1, 1], data_layout="NHWC", kernel_layout="HWIO", out_dtype="int32") /* ty=Tensor[(1, 7, 7, 1024), int32] span=fused Conv_256_quant:0:0 */;
  %368 = add(%367, meta[relay.Constant][278] /* ty=Tensor[(1, 1, 1, 1024), int32] */) /* ty=Tensor[(1, 7, 7, 1024), int32] */;
  %369 = qnn.requantize(%368, meta[relay.Constant][279] /* ty=Tensor[(1024), float32] span=fused Conv_256_quant:0:0 */, 0 /* ty=int32 span=fused Conv_256_quant:0:0 */, 0.0107276f /* ty=float32 span=fused Conv_256_quant:0:0 */, 0 /* ty=int32 span=fused Conv_256_quant:0:0 */, axis=3, rounding="UPWARD", compute_dtype="int64", out_dtype="uint8") /* ty=Tensor[(1, 7, 7, 1024), uint8] span=fused Conv_256_quant:0:0 */;
  %370 = qnn.dequantize(%369, 0.0107276f /* ty=float32 span=fused Conv_256_quant.610_scale:0:0 */, 0 /* ty=int32 span=610_DequantizeLinear:0:0 */, out_dtype="float32", axis=0) /* ty=Tensor[(1, 7, 7, 1024), float32] span=610_DequantizeLinear:0:0 */;
  %371 = mean(%370, axis=[1, 2]) /* ty=Tensor[(1, 1024), float32] span=ReduceMean_259:0:0 */;
  %372 = qnn.quantize(%371, 0.000975963f /* ty=float32 span=611_QuantizeLinear.611_scale:0:0 */, 0 /* ty=int32 span=611_QuantizeLinear:0:0 */, out_dtype="uint8", axis=0) /* ty=Tensor[(1, 1024), uint8] span=611_QuantizeLinear:0:0 */;
  %373 = qnn.dense(%372, meta[relay.Constant][280] /* ty=Tensor[(1000, 1024), int8] span=Gemm_260_MatMul_quant:0:0 */, 0 /* ty=int32 span=Gemm_260_MatMul_quant:0:0 */, 0 /* ty=int32 span=Gemm_260_MatMul_quant:0:0 */, 0.000975963f /* ty=float32 span=611_QuantizeLinear.611_scale:0:0 */, 0.316708f /* ty=float32 span=Gemm_260_MatMul_quant.fc.weight_scale:0:0 */, units=1000, out_dtype="int32") /* ty=Tensor[(1, 1000), int32] span=Gemm_260_MatMul_quant:0:0 */;
  %374 = qnn.requantize(%373, 0.000309095f /* ty=float32 span=Gemm_260_MatMul_quant:0:0 */, 0 /* ty=int32 span=Gemm_260_MatMul_quant:0:0 */, 0.17454f /* ty=float32 span=Gemm_260_MatMul_quant.output_MatMul_scale:0:0 */, 49 /* ty=int32 span=Gemm_260_MatMul_quant:0:0 */, axis=1, rounding="TONEAREST", compute_dtype="int64", out_dtype="uint8") /* ty=Tensor[(1, 1000), uint8] span=Gemm_260_MatMul_quant:0:0 */;
  %375 = qnn.dequantize(meta[relay.Constant][281] /* ty=Tensor[(1000), uint8] span=Gemm_260_Add_quant.fc.bias_quantized:0:0 */, 0.0173267f /* ty=float32 span=Gemm_260_Add_quant:0:0 */, 224 /* ty=int32 span=Gemm_260_Add_quant:0:0 */, out_dtype="float32") /* ty=Tensor[(1000), float32] span=Gemm_260_Add_quant:0:0 */;
  %376 = qnn.dequantize(%374, 0.17454f /* ty=float32 span=Gemm_260_Add_quant:0:0 */, 49 /* ty=int32 span=Gemm_260_Add_quant:0:0 */, out_dtype="float32", axis=1) /* ty=Tensor[(1, 1000), float32] span=Gemm_260_Add_quant:0:0 */;
  %377 = expand_dims(%375, axis=0) /* ty=Tensor[(1, 1000), float32] */;
  %378 = add(%376, %377) /* ty=Tensor[(1, 1000), float32] span=Gemm_260_Add_quant:0:0 */;
  %379 = qnn.quantize(%378, 0.1724f /* ty=float32 span=Gemm_260_Add_quant:0:0 */, 54 /* ty=int32 span=Gemm_260_Add_quant:0:0 */, out_dtype="uint8", axis=1) /* ty=Tensor[(1, 1000), uint8] span=Gemm_260_Add_quant:0:0 */;
  qnn.dequantize(%379, 0.1724f /* ty=float32 span=Gemm_260_Add_quant.output_scale:0:0 */, 54 /* ty=int32 span=output_DequantizeLinear:0:0 */, out_dtype="float32", axis=0) /* ty=Tensor[(1, 1000), float32] span=output_DequantizeLinear:0:0 */
}

