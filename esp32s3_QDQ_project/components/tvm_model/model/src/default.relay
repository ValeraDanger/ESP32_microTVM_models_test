def @main(%input_1: Tensor[(1, 32, 32, 3), float32] /* ty=Tensor[(1, 32, 32, 3), float32] span=Transpose_1.input_1:0:0 */) -> Tensor[(1, 10), float32] {
  %0 = transpose(%input_1, axes=[0, 3, 1, 2]) /* ty=Tensor[(1, 3, 32, 32), float32] span=Transpose_1:0:0 */;
  %1 = qnn.quantize(%0, 2.00787f /* ty=float32 span=QuantizeLinear_1.model/activation/Relu;model/batch_normalization/FusedBatchNormV3;model/conv2d/BiasAdd/ReadVariableOp/resource;model/conv2d/BiasAdd;model/conv2d_2/Conv2D;model/conv2d/Conv2D1__31:0_scale:0:0 */, 0 /* ty=int32 span=QuantizeLinear_1:0:0 */, out_dtype="int8", axis=1) /* ty=Tensor[(1, 3, 32, 32), int8] span=QuantizeLinear_1:0:0 */;
  %2 = qnn.dequantize(%1, 2.00787f /* ty=float32 span=QuantizeLinear_1.model/activation/Relu;model/batch_normalization/FusedBatchNormV3;model/conv2d/BiasAdd/ReadVariableOp/resource;model/conv2d/BiasAdd;model/conv2d_2/Conv2D;model/conv2d/Conv2D1__31:0_scale:0:0 */, 0 /* ty=int32 span=DequantizeLinear_1:0:0 */, out_dtype="float32", axis=1) /* ty=Tensor[(1, 3, 32, 32), float32] span=DequantizeLinear_1:0:0 */;
  %3 = qnn.dequantize(meta[relay.Constant][0] /* ty=Tensor[(16, 3, 3, 3), int8] span=DequantizeLinear_3.const_fold_opt__108_quantized:0:0 */, 0.000166913f /* ty=float32 span=DequantizeLinear_3.const_fold_opt__108_scale:0:0 */, 0 /* ty=int32 span=DequantizeLinear_3:0:0 */, out_dtype="float32", axis=1) /* ty=Tensor[(16, 3, 3, 3), float32] span=DequantizeLinear_3:0:0 */;
  %4 = qnn.dequantize(meta[relay.Constant][1] /* ty=Tensor[(16), int32] span=DequantizeLinear_34.model/activation/Relu;model/batch_normalization/FusedBatchNormV3;model/conv2d/BiasAdd/ReadVariableOp/resource;model/conv2d/BiasAdd;model/conv2d_2/Conv2D;model/conv2d/Conv2D_quantized:0:0 */, 0.00033514f /* ty=float32 span=DequantizeLinear_34.model/activation/Relu;model/batch_normalization/FusedBatchNormV3;model/conv2d/BiasAdd/ReadVariableOp/resource;model/conv2d/BiasAdd;model/conv2d_2/Conv2D;model/conv2d/Conv2D_quantized_scale:0:0 */, 0 /* ty=int32 span=DequantizeLinear_34:0:0 */, out_dtype="float32", axis=0) /* ty=Tensor[(16), float32] span=DequantizeLinear_34:0:0 */;
  %5 = expand_dims(%4, axis=1, num_newaxis=2) /* ty=Tensor[(16, 1, 1), float32] */;
  %6 = nn.conv2d(%2, %3, padding=[1, 1, 1, 1], channels=16, kernel_size=[3, 3]) /* ty=Tensor[(1, 16, 32, 32), float32] span=Conv_1:0:0 */;
  %7 = expand_dims(%5, axis=0) /* ty=Tensor[(1, 16, 1, 1), float32] */;
  %8 = add(%6, %7) /* ty=Tensor[(1, 16, 32, 32), float32] */;
  %9 = qnn.quantize(%8, 0.0790973f /* ty=float32 span=QuantizeLinear_2.model/activation/Relu;model/batch_normalization/FusedBatchNormV3;model/conv2d/BiasAdd/ReadVariableOp/resource;model/conv2d/BiasAdd;model/conv2d_2/Conv2D;model/conv2d/Conv2D1_scale:0:0 */, 0 /* ty=int32 span=QuantizeLinear_2:0:0 */, out_dtype="int8", axis=1) /* ty=Tensor[(1, 16, 32, 32), int8] span=QuantizeLinear_2:0:0 */;
  %10 = nn.relu(%9) /* ty=Tensor[(1, 16, 32, 32), int8] span=Relu_1:0:0 */;
  %11 = qnn.dequantize(%10, 0.0790973f /* ty=float32 span=DequantizeLinear_4.Relu__5:0_scale:0:0 */, 0 /* ty=int32 span=DequantizeLinear_4:0:0 */, out_dtype="float32", axis=1) /* ty=Tensor[(1, 16, 32, 32), float32] span=DequantizeLinear_4:0:0 */;
  %12 = qnn.dequantize(meta[relay.Constant][2] /* ty=Tensor[(16, 16, 3, 3), int8] span=DequantizeLinear_6.const_fold_opt__119_quantized:0:0 */, 0.00775049f /* ty=float32 span=DequantizeLinear_6.const_fold_opt__119_scale:0:0 */, 0 /* ty=int32 span=DequantizeLinear_6:0:0 */, out_dtype="float32", axis=1) /* ty=Tensor[(16, 16, 3, 3), float32] span=DequantizeLinear_6:0:0 */;
  %13 = qnn.dequantize(meta[relay.Constant][3] /* ty=Tensor[(16), int32] span=DequantizeLinear_35.model/activation_1/Relu;model/batch_normalization_1/FusedBatchNormV3;model/conv2d_1/BiasAdd/ReadVariableOp/resource;model/conv2d_1/BiasAdd;model/conv2d_2/Conv2D;model/conv2d_1/Conv2D_quantized:0:0 */, 0.000613043f /* ty=float32 span=DequantizeLinear_35.model/activation_1/Relu;model/batch_normalization_1/FusedBatchNormV3;model/conv2d_1/BiasAdd/ReadVariableOp/resource;model/conv2d_1/BiasAdd;model/conv2d_2/Conv2D;model/conv2d_1/Conv2D_quantized_scale:0:0 */, 0 /* ty=int32 span=DequantizeLinear_35:0:0 */, out_dtype="float32", axis=0) /* ty=Tensor[(16), float32] span=DequantizeLinear_35:0:0 */;
  %14 = expand_dims(%13, axis=1, num_newaxis=2) /* ty=Tensor[(16, 1, 1), float32] */;
  %15 = nn.conv2d(%11, %12, padding=[1, 1, 1, 1], channels=16, kernel_size=[3, 3]) /* ty=Tensor[(1, 16, 32, 32), float32] span=Conv_2:0:0 */;
  %16 = expand_dims(%14, axis=0) /* ty=Tensor[(1, 16, 1, 1), float32] */;
  %17 = add(%15, %16) /* ty=Tensor[(1, 16, 32, 32), float32] */;
  %18 = qnn.quantize(%17, 0.20158f /* ty=float32 span=QuantizeLinear_4.model/activation_1/Relu;model/batch_normalization_1/FusedBatchNormV3;model/conv2d_1/BiasAdd/ReadVariableOp/resource;model/conv2d_1/BiasAdd;model/conv2d_2/Conv2D;model/conv2d_1/Conv2D1_scale:0:0 */, 0 /* ty=int32 span=QuantizeLinear_4:0:0 */, out_dtype="int8", axis=1) /* ty=Tensor[(1, 16, 32, 32), int8] span=QuantizeLinear_4:0:0 */;
  %19 = qnn.dequantize(%18, 0.20158f /* ty=float32 span=QuantizeLinear_4.model/activation_1/Relu;model/batch_normalization_1/FusedBatchNormV3;model/conv2d_1/BiasAdd/ReadVariableOp/resource;model/conv2d_1/BiasAdd;model/conv2d_2/Conv2D;model/conv2d_1/Conv2D1_scale:0:0 */, 0 /* ty=int32 span=DequantizeLinear_5:0:0 */, out_dtype="float32", axis=1) /* ty=Tensor[(1, 16, 32, 32), float32] span=DequantizeLinear_5:0:0 */;
  %20 = nn.relu(%19) /* ty=Tensor[(1, 16, 32, 32), float32] span=Relu_2:0:0 */;
  %21 = qnn.quantize(%20, 0.153187f /* ty=float32 span=QuantizeLinear_5.Relu__8:0_scale:0:0 */, 0 /* ty=int32 span=QuantizeLinear_5:0:0 */, out_dtype="int8", axis=1) /* ty=Tensor[(1, 16, 32, 32), int8] span=QuantizeLinear_5:0:0 */;
  %22 = qnn.dequantize(%21, 0.153187f /* ty=float32 span=QuantizeLinear_5.Relu__8:0_scale:0:0 */, 0 /* ty=int32 span=DequantizeLinear_7:0:0 */, out_dtype="float32", axis=1) /* ty=Tensor[(1, 16, 32, 32), float32] span=DequantizeLinear_7:0:0 */;
  %23 = qnn.dequantize(meta[relay.Constant][4] /* ty=Tensor[(16, 16, 3, 3), int8] span=DequantizeLinear_9.const_fold_opt__125_quantized:0:0 */, 0.00407408f /* ty=float32 span=DequantizeLinear_9.const_fold_opt__125_scale:0:0 */, 0 /* ty=int32 span=DequantizeLinear_9:0:0 */, out_dtype="float32", axis=1) /* ty=Tensor[(16, 16, 3, 3), float32] span=DequantizeLinear_9:0:0 */;
  %24 = qnn.dequantize(meta[relay.Constant][5] /* ty=Tensor[(16), int32] span=DequantizeLinear_36.model/batch_normalization_2/FusedBatchNormV3;model/conv2d_2/BiasAdd/ReadVariableOp/resource;model/conv2d_2/BiasAdd_quantized:0:0 */, 0.000624096f /* ty=float32 span=DequantizeLinear_36.model/batch_normalization_2/FusedBatchNormV3;model/conv2d_2/BiasAdd/ReadVariableOp/resource;model/conv2d_2/BiasAdd_quantized_scale:0:0 */, 0 /* ty=int32 span=DequantizeLinear_36:0:0 */, out_dtype="float32", axis=0) /* ty=Tensor[(16), float32] span=DequantizeLinear_36:0:0 */;
  %25 = expand_dims(%24, axis=1, num_newaxis=2) /* ty=Tensor[(16, 1, 1), float32] */;
  %26 = nn.conv2d(%22, %23, padding=[1, 1, 1, 1], channels=16, kernel_size=[3, 3]) /* ty=Tensor[(1, 16, 32, 32), float32] span=Conv_3:0:0 */;
  %27 = expand_dims(%25, axis=0) /* ty=Tensor[(1, 16, 1, 1), float32] */;
  %28 = add(%26, %27) /* ty=Tensor[(1, 16, 32, 32), float32] */;
  %29 = qnn.quantize(%28, 0.107967f /* ty=float32 span=QuantizeLinear_6.model/batch_normalization_2/FusedBatchNormV3;model/conv2d_2/BiasAdd/ReadVariableOp/resource;model/conv2d_2/BiasAdd;model/conv2d_2/Conv2D_scale:0:0 */, 0 /* ty=int32 span=QuantizeLinear_6:0:0 */, out_dtype="int8", axis=1) /* ty=Tensor[(1, 16, 32, 32), int8] span=QuantizeLinear_6:0:0 */;
  %30 = qnn.dequantize(%29, 0.107967f /* ty=float32 span=QuantizeLinear_6.model/batch_normalization_2/FusedBatchNormV3;model/conv2d_2/BiasAdd/ReadVariableOp/resource;model/conv2d_2/BiasAdd;model/conv2d_2/Conv2D_scale:0:0 */, 0 /* ty=int32 span=DequantizeLinear_8:0:0 */, out_dtype="float32", axis=1) /* ty=Tensor[(1, 16, 32, 32), float32] span=DequantizeLinear_8:0:0 */;
  %31 = add(%11, %30) /* ty=Tensor[(1, 16, 32, 32), float32] span=Add_1:0:0 */;
  %32 = qnn.quantize(%31, 0.107967f /* ty=float32 span=QuantizeLinear_7.model/activation_2/Relu;model/add/add_scale:0:0 */, 0 /* ty=int32 span=QuantizeLinear_7:0:0 */, out_dtype="int8", axis=1) /* ty=Tensor[(1, 16, 32, 32), int8] span=QuantizeLinear_7:0:0 */;
  %33 = qnn.dequantize(%32, 0.107967f /* ty=float32 span=QuantizeLinear_7.model/activation_2/Relu;model/add/add_scale:0:0 */, 0 /* ty=int32 span=DequantizeLinear_10:0:0 */, out_dtype="float32", axis=1) /* ty=Tensor[(1, 16, 32, 32), float32] span=DequantizeLinear_10:0:0 */;
  %34 = nn.relu(%33) /* ty=Tensor[(1, 16, 32, 32), float32] span=Relu_3:0:0 */;
  %35 = qnn.quantize(%34, 0.102292f /* ty=float32 span=QuantizeLinear_8.Relu__12:0_scale:0:0 */, 0 /* ty=int32 span=QuantizeLinear_8:0:0 */, out_dtype="int8", axis=1) /* ty=Tensor[(1, 16, 32, 32), int8] span=QuantizeLinear_8:0:0 */;
  %36 = qnn.dequantize(%35, 0.102292f /* ty=float32 span=QuantizeLinear_8.Relu__12:0_scale:0:0 */, 0 /* ty=int32 span=DequantizeLinear_11:0:0 */, out_dtype="float32", axis=1) /* ty=Tensor[(1, 16, 32, 32), float32] span=DequantizeLinear_11:0:0 */;
  %37 = qnn.dequantize(meta[relay.Constant][6] /* ty=Tensor[(32, 16, 1, 1), int8] span=DequantizeLinear_18.const_fold_opt__112_quantized:0:0 */, 0.00468374f /* ty=float32 span=DequantizeLinear_18.const_fold_opt__112_scale:0:0 */, 0 /* ty=int32 span=DequantizeLinear_18:0:0 */, out_dtype="float32", axis=1) /* ty=Tensor[(32, 16, 1, 1), float32] span=DequantizeLinear_18:0:0 */;
  %38 = qnn.dequantize(meta[relay.Constant][7] /* ty=Tensor[(32), int32] span=DequantizeLinear_39.model/conv2d_5/BiasAdd;model/conv2d_5/Conv2D;model/conv2d_5/BiasAdd/ReadVariableOp/resource_quantized:0:0 */, 0.000479111f /* ty=float32 span=DequantizeLinear_39.model/conv2d_5/BiasAdd;model/conv2d_5/Conv2D;model/conv2d_5/BiasAdd/ReadVariableOp/resource_quantized_scale:0:0 */, 0 /* ty=int32 span=DequantizeLinear_39:0:0 */, out_dtype="float32", axis=0) /* ty=Tensor[(32), float32] span=DequantizeLinear_39:0:0 */;
  %39 = expand_dims(%38, axis=1, num_newaxis=2) /* ty=Tensor[(32, 1, 1), float32] */;
  %40 = nn.conv2d(%36, %37, strides=[2, 2], padding=[0, 0, 0, 0], channels=32, kernel_size=[1, 1]) /* ty=Tensor[(1, 32, 16, 16), float32] span=Conv_6:0:0 */;
  %41 = expand_dims(%39, axis=0) /* ty=Tensor[(1, 32, 1, 1), float32] */;
  %42 = add(%40, %41) /* ty=Tensor[(1, 32, 16, 16), float32] */;
  %43 = qnn.quantize(%42, 0.0508577f /* ty=float32 span=QuantizeLinear_12.model/conv2d_5/BiasAdd;model/conv2d_5/Conv2D;model/conv2d_5/BiasAdd/ReadVariableOp/resource1_scale:0:0 */, 0 /* ty=int32 span=QuantizeLinear_12:0:0 */, out_dtype="int8", axis=1) /* ty=Tensor[(1, 32, 16, 16), int8] span=QuantizeLinear_12:0:0 */;
  %44 = qnn.dequantize(meta[relay.Constant][8] /* ty=Tensor[(32, 16, 3, 3), int8] span=DequantizeLinear_13.const_fold_opt__115_quantized:0:0 */, 0.00341045f /* ty=float32 span=DequantizeLinear_13.const_fold_opt__115_scale:0:0 */, 0 /* ty=int32 span=DequantizeLinear_13:0:0 */, out_dtype="float32", axis=1) /* ty=Tensor[(32, 16, 3, 3), float32] span=DequantizeLinear_13:0:0 */;
  %45 = qnn.dequantize(meta[relay.Constant][9] /* ty=Tensor[(32), int32] span=DequantizeLinear_37.model/activation_3/Relu;model/batch_normalization_3/FusedBatchNormV3;model/conv2d_3/BiasAdd/ReadVariableOp/resource;model/conv2d_3/BiasAdd;model/conv2d_5/Conv2D;model/conv2d_3/Conv2D_quantized:0:0 */, 0.000348864f /* ty=float32 span=DequantizeLinear_37.model/activation_3/Relu;model/batch_normalization_3/FusedBatchNormV3;model/conv2d_3/BiasAdd/ReadVariableOp/resource;model/conv2d_3/BiasAdd;model/conv2d_5/Conv2D;model/conv2d_3/Conv2D_quantized_scale:0:0 */, 0 /* ty=int32 span=DequantizeLinear_37:0:0 */, out_dtype="float32", axis=0) /* ty=Tensor[(32), float32] span=DequantizeLinear_37:0:0 */;
  %46 = expand_dims(%45, axis=1, num_newaxis=2) /* ty=Tensor[(32, 1, 1), float32] */;
  %47 = nn.conv2d(%36, %44, strides=[2, 2], padding=[0, 0, 1, 1], channels=32, kernel_size=[3, 3]) /* ty=Tensor[(1, 32, 16, 16), float32] span=Conv_4:0:0 */;
  %48 = expand_dims(%46, axis=0) /* ty=Tensor[(1, 32, 1, 1), float32] */;
  %49 = add(%47, %48) /* ty=Tensor[(1, 32, 16, 16), float32] */;
  %50 = qnn.quantize(%49, 0.0969278f /* ty=float32 span=QuantizeLinear_9.model/activation_3/Relu;model/batch_normalization_3/FusedBatchNormV3;model/conv2d_3/BiasAdd/ReadVariableOp/resource;model/conv2d_3/BiasAdd;model/conv2d_5/Conv2D;model/conv2d_3/Conv2D1_scale:0:0 */, 0 /* ty=int32 span=QuantizeLinear_9:0:0 */, out_dtype="int8", axis=1) /* ty=Tensor[(1, 32, 16, 16), int8] span=QuantizeLinear_9:0:0 */;
  %51 = qnn.dequantize(%50, 0.0969278f /* ty=float32 span=QuantizeLinear_9.model/activation_3/Relu;model/batch_normalization_3/FusedBatchNormV3;model/conv2d_3/BiasAdd/ReadVariableOp/resource;model/conv2d_3/BiasAdd;model/conv2d_5/Conv2D;model/conv2d_3/Conv2D1_scale:0:0 */, 0 /* ty=int32 span=DequantizeLinear_12:0:0 */, out_dtype="float32", axis=1) /* ty=Tensor[(1, 32, 16, 16), float32] span=DequantizeLinear_12:0:0 */;
  %52 = nn.relu(%51) /* ty=Tensor[(1, 32, 16, 16), float32] span=Relu_4:0:0 */;
  %53 = qnn.quantize(%52, 0.0917053f /* ty=float32 span=QuantizeLinear_10.Relu__14:0_scale:0:0 */, 0 /* ty=int32 span=QuantizeLinear_10:0:0 */, out_dtype="int8", axis=1) /* ty=Tensor[(1, 32, 16, 16), int8] span=QuantizeLinear_10:0:0 */;
  %54 = qnn.dequantize(%53, 0.0917053f /* ty=float32 span=QuantizeLinear_10.Relu__14:0_scale:0:0 */, 0 /* ty=int32 span=DequantizeLinear_14:0:0 */, out_dtype="float32", axis=1) /* ty=Tensor[(1, 32, 16, 16), float32] span=DequantizeLinear_14:0:0 */;
  %55 = qnn.dequantize(meta[relay.Constant][10] /* ty=Tensor[(32, 32, 3, 3), int8] span=DequantizeLinear_16.const_fold_opt__121_quantized:0:0 */, 0.00603094f /* ty=float32 span=DequantizeLinear_16.const_fold_opt__121_scale:0:0 */, 0 /* ty=int32 span=DequantizeLinear_16:0:0 */, out_dtype="float32", axis=1) /* ty=Tensor[(32, 32, 3, 3), float32] span=DequantizeLinear_16:0:0 */;
  %56 = qnn.dequantize(meta[relay.Constant][11] /* ty=Tensor[(32), int32] span=DequantizeLinear_38.model/batch_normalization_4/FusedBatchNormV3;model/conv2d_4/BiasAdd/ReadVariableOp/resource;model/conv2d_4/BiasAdd_quantized:0:0 */, 0.000553069f /* ty=float32 span=DequantizeLinear_38.model/batch_normalization_4/FusedBatchNormV3;model/conv2d_4/BiasAdd/ReadVariableOp/resource;model/conv2d_4/BiasAdd_quantized_scale:0:0 */, 0 /* ty=int32 span=DequantizeLinear_38:0:0 */, out_dtype="float32", axis=0) /* ty=Tensor[(32), float32] span=DequantizeLinear_38:0:0 */;
  %57 = expand_dims(%56, axis=1, num_newaxis=2) /* ty=Tensor[(32, 1, 1), float32] */;
  %58 = nn.conv2d(%54, %55, padding=[1, 1, 1, 1], channels=32, kernel_size=[3, 3]) /* ty=Tensor[(1, 32, 16, 16), float32] span=Conv_5:0:0 */;
  %59 = expand_dims(%57, axis=0) /* ty=Tensor[(1, 32, 1, 1), float32] */;
  %60 = add(%58, %59) /* ty=Tensor[(1, 32, 16, 16), float32] */;
  %61 = qnn.quantize(%60, 0.117619f /* ty=float32 span=QuantizeLinear_11.model/batch_normalization_4/FusedBatchNormV3;model/conv2d_4/BiasAdd/ReadVariableOp/resource;model/conv2d_4/BiasAdd;model/conv2d_5/Conv2D;model/conv2d_4/Conv2D_scale:0:0 */, 0 /* ty=int32 span=QuantizeLinear_11:0:0 */, out_dtype="int8", axis=1) /* ty=Tensor[(1, 32, 16, 16), int8] span=QuantizeLinear_11:0:0 */;
  %62 = qnn.dequantize(%43, 0.0508577f /* ty=float32 span=QuantizeLinear_12.model/conv2d_5/BiasAdd;model/conv2d_5/Conv2D;model/conv2d_5/BiasAdd/ReadVariableOp/resource1_scale:0:0 */, 0 /* ty=int32 span=DequantizeLinear_17:0:0 */, out_dtype="float32", axis=1) /* ty=Tensor[(1, 32, 16, 16), float32] span=DequantizeLinear_17:0:0 */;
  %63 = qnn.dequantize(%61, 0.117619f /* ty=float32 span=QuantizeLinear_11.model/batch_normalization_4/FusedBatchNormV3;model/conv2d_4/BiasAdd/ReadVariableOp/resource;model/conv2d_4/BiasAdd;model/conv2d_5/Conv2D;model/conv2d_4/Conv2D_scale:0:0 */, 0 /* ty=int32 span=DequantizeLinear_15:0:0 */, out_dtype="float32", axis=1) /* ty=Tensor[(1, 32, 16, 16), float32] span=DequantizeLinear_15:0:0 */;
  %64 = add(%62, %63) /* ty=Tensor[(1, 32, 16, 16), float32] span=Add_2:0:0 */;
  %65 = qnn.quantize(%64, 0.137376f /* ty=float32 span=QuantizeLinear_13.model/activation_4/Relu;model/add_1/add_scale:0:0 */, 0 /* ty=int32 span=QuantizeLinear_13:0:0 */, out_dtype="int8", axis=1) /* ty=Tensor[(1, 32, 16, 16), int8] span=QuantizeLinear_13:0:0 */;
  %66 = qnn.dequantize(%65, 0.137376f /* ty=float32 span=QuantizeLinear_13.model/activation_4/Relu;model/add_1/add_scale:0:0 */, 0 /* ty=int32 span=DequantizeLinear_19:0:0 */, out_dtype="float32", axis=1) /* ty=Tensor[(1, 32, 16, 16), float32] span=DequantizeLinear_19:0:0 */;
  %67 = nn.relu(%66) /* ty=Tensor[(1, 32, 16, 16), float32] span=Relu_5:0:0 */;
  %68 = qnn.quantize(%67, 0.106892f /* ty=float32 span=QuantizeLinear_14.Relu__19:0_scale:0:0 */, 0 /* ty=int32 span=QuantizeLinear_14:0:0 */, out_dtype="int8", axis=1) /* ty=Tensor[(1, 32, 16, 16), int8] span=QuantizeLinear_14:0:0 */;
  %69 = qnn.dequantize(%68, 0.106892f /* ty=float32 span=QuantizeLinear_14.Relu__19:0_scale:0:0 */, 0 /* ty=int32 span=DequantizeLinear_20:0:0 */, out_dtype="float32", axis=1) /* ty=Tensor[(1, 32, 16, 16), float32] span=DequantizeLinear_20:0:0 */;
  %70 = qnn.dequantize(meta[relay.Constant][12] /* ty=Tensor[(64, 32, 1, 1), int8] span=DequantizeLinear_27.const_fold_opt__117_quantized:0:0 */, 0.00545248f /* ty=float32 span=DequantizeLinear_27.const_fold_opt__117_scale:0:0 */, 0 /* ty=int32 span=DequantizeLinear_27:0:0 */, out_dtype="float32", axis=1) /* ty=Tensor[(64, 32, 1, 1), float32] span=DequantizeLinear_27:0:0 */;
  %71 = qnn.dequantize(meta[relay.Constant][13] /* ty=Tensor[(64), int32] span=DequantizeLinear_42.model/conv2d_8/BiasAdd;model/conv2d_8/Conv2D;model/conv2d_8/BiasAdd/ReadVariableOp/resource_quantized:0:0 */, 0.000582824f /* ty=float32 span=DequantizeLinear_42.model/conv2d_8/BiasAdd;model/conv2d_8/Conv2D;model/conv2d_8/BiasAdd/ReadVariableOp/resource_quantized_scale:0:0 */, 0 /* ty=int32 span=DequantizeLinear_42:0:0 */, out_dtype="float32", axis=0) /* ty=Tensor[(64), float32] span=DequantizeLinear_42:0:0 */;
  %72 = expand_dims(%71, axis=1, num_newaxis=2) /* ty=Tensor[(64, 1, 1), float32] */;
  %73 = nn.conv2d(%69, %70, strides=[2, 2], padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(1, 64, 8, 8), float32] span=Conv_9:0:0 */;
  %74 = expand_dims(%72, axis=0) /* ty=Tensor[(1, 64, 1, 1), float32] */;
  %75 = add(%73, %74) /* ty=Tensor[(1, 64, 8, 8), float32] */;
  %76 = qnn.quantize(%75, 0.109617f /* ty=float32 span=QuantizeLinear_18.model/conv2d_8/BiasAdd;model/conv2d_8/Conv2D;model/conv2d_8/BiasAdd/ReadVariableOp/resource1_scale:0:0 */, 0 /* ty=int32 span=QuantizeLinear_18:0:0 */, out_dtype="int8", axis=1) /* ty=Tensor[(1, 64, 8, 8), int8] span=QuantizeLinear_18:0:0 */;
  %77 = qnn.dequantize(meta[relay.Constant][14] /* ty=Tensor[(64, 32, 3, 3), int8] span=DequantizeLinear_22.const_fold_opt__120_quantized:0:0 */, 0.00147699f /* ty=float32 span=DequantizeLinear_22.const_fold_opt__120_scale:0:0 */, 0 /* ty=int32 span=DequantizeLinear_22:0:0 */, out_dtype="float32", axis=1) /* ty=Tensor[(64, 32, 3, 3), float32] span=DequantizeLinear_22:0:0 */;
  %78 = qnn.dequantize(meta[relay.Constant][15] /* ty=Tensor[(64), int32] span=DequantizeLinear_40.model/activation_5/Relu;model/batch_normalization_5/FusedBatchNormV3;model/conv2d_6/BiasAdd/ReadVariableOp/resource;model/conv2d_6/BiasAdd;model/conv2d_8/Conv2D;model/conv2d_6/Conv2D_quantized:0:0 */, 0.000157878f /* ty=float32 span=DequantizeLinear_40.model/activation_5/Relu;model/batch_normalization_5/FusedBatchNormV3;model/conv2d_6/BiasAdd/ReadVariableOp/resource;model/conv2d_6/BiasAdd;model/conv2d_8/Conv2D;model/conv2d_6/Conv2D_quantized_scale:0:0 */, 0 /* ty=int32 span=DequantizeLinear_40:0:0 */, out_dtype="float32", axis=0) /* ty=Tensor[(64), float32] span=DequantizeLinear_40:0:0 */;
  %79 = expand_dims(%78, axis=1, num_newaxis=2) /* ty=Tensor[(64, 1, 1), float32] */;
  %80 = nn.conv2d(%69, %77, strides=[2, 2], padding=[0, 0, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 8, 8), float32] span=Conv_7:0:0 */;
  %81 = expand_dims(%79, axis=0) /* ty=Tensor[(1, 64, 1, 1), float32] */;
  %82 = add(%80, %81) /* ty=Tensor[(1, 64, 8, 8), float32] */;
  %83 = qnn.quantize(%82, 0.0774434f /* ty=float32 span=QuantizeLinear_15.model/activation_5/Relu;model/batch_normalization_5/FusedBatchNormV3;model/conv2d_6/BiasAdd/ReadVariableOp/resource;model/conv2d_6/BiasAdd;model/conv2d_8/Conv2D;model/conv2d_6/Conv2D1_scale:0:0 */, 0 /* ty=int32 span=QuantizeLinear_15:0:0 */, out_dtype="int8", axis=1) /* ty=Tensor[(1, 64, 8, 8), int8] span=QuantizeLinear_15:0:0 */;
  %84 = qnn.dequantize(%83, 0.0774434f /* ty=float32 span=QuantizeLinear_15.model/activation_5/Relu;model/batch_normalization_5/FusedBatchNormV3;model/conv2d_6/BiasAdd/ReadVariableOp/resource;model/conv2d_6/BiasAdd;model/conv2d_8/Conv2D;model/conv2d_6/Conv2D1_scale:0:0 */, 0 /* ty=int32 span=DequantizeLinear_21:0:0 */, out_dtype="float32", axis=1) /* ty=Tensor[(1, 64, 8, 8), float32] span=DequantizeLinear_21:0:0 */;
  %85 = nn.relu(%84) /* ty=Tensor[(1, 64, 8, 8), float32] span=Relu_6:0:0 */;
  %86 = qnn.quantize(%85, 0.0571244f /* ty=float32 span=QuantizeLinear_16.Relu__21:0_scale:0:0 */, 0 /* ty=int32 span=QuantizeLinear_16:0:0 */, out_dtype="int8", axis=1) /* ty=Tensor[(1, 64, 8, 8), int8] span=QuantizeLinear_16:0:0 */;
  %87 = qnn.dequantize(%86, 0.0571244f /* ty=float32 span=QuantizeLinear_16.Relu__21:0_scale:0:0 */, 0 /* ty=int32 span=DequantizeLinear_23:0:0 */, out_dtype="float32", axis=1) /* ty=Tensor[(1, 64, 8, 8), float32] span=DequantizeLinear_23:0:0 */;
  %88 = qnn.dequantize(meta[relay.Constant][16] /* ty=Tensor[(64, 64, 3, 3), int8] span=DequantizeLinear_25.const_fold_opt__123_quantized:0:0 */, 0.0144741f /* ty=float32 span=DequantizeLinear_25.const_fold_opt__123_scale:0:0 */, 0 /* ty=int32 span=DequantizeLinear_25:0:0 */, out_dtype="float32", axis=1) /* ty=Tensor[(64, 64, 3, 3), float32] span=DequantizeLinear_25:0:0 */;
  %89 = qnn.dequantize(meta[relay.Constant][17] /* ty=Tensor[(64), int32] span=DequantizeLinear_41.model/batch_normalization_6/FusedBatchNormV3;model/conv2d_7/BiasAdd/ReadVariableOp/resource;model/conv2d_7/BiasAdd_quantized:0:0 */, 0.000826827f /* ty=float32 span=DequantizeLinear_41.model/batch_normalization_6/FusedBatchNormV3;model/conv2d_7/BiasAdd/ReadVariableOp/resource;model/conv2d_7/BiasAdd_quantized_scale:0:0 */, 0 /* ty=int32 span=DequantizeLinear_41:0:0 */, out_dtype="float32", axis=0) /* ty=Tensor[(64), float32] span=DequantizeLinear_41:0:0 */;
  %90 = expand_dims(%89, axis=1, num_newaxis=2) /* ty=Tensor[(64, 1, 1), float32] */;
  %91 = nn.conv2d(%87, %88, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 8, 8), float32] span=Conv_8:0:0 */;
  %92 = expand_dims(%90, axis=0) /* ty=Tensor[(1, 64, 1, 1), float32] */;
  %93 = add(%91, %92) /* ty=Tensor[(1, 64, 8, 8), float32] */;
  %94 = qnn.quantize(%93, 0.220277f /* ty=float32 span=QuantizeLinear_17.model/batch_normalization_6/FusedBatchNormV3;model/conv2d_7/BiasAdd/ReadVariableOp/resource;model/conv2d_7/BiasAdd;model/conv2d_8/Conv2D;model/conv2d_7/Conv2D_scale:0:0 */, 0 /* ty=int32 span=QuantizeLinear_17:0:0 */, out_dtype="int8", axis=1) /* ty=Tensor[(1, 64, 8, 8), int8] span=QuantizeLinear_17:0:0 */;
  %95 = qnn.dequantize(%76, 0.109617f /* ty=float32 span=QuantizeLinear_18.model/conv2d_8/BiasAdd;model/conv2d_8/Conv2D;model/conv2d_8/BiasAdd/ReadVariableOp/resource1_scale:0:0 */, 0 /* ty=int32 span=DequantizeLinear_26:0:0 */, out_dtype="float32", axis=1) /* ty=Tensor[(1, 64, 8, 8), float32] span=DequantizeLinear_26:0:0 */;
  %96 = qnn.dequantize(%94, 0.220277f /* ty=float32 span=QuantizeLinear_17.model/batch_normalization_6/FusedBatchNormV3;model/conv2d_7/BiasAdd/ReadVariableOp/resource;model/conv2d_7/BiasAdd;model/conv2d_8/Conv2D;model/conv2d_7/Conv2D_scale:0:0 */, 0 /* ty=int32 span=DequantizeLinear_24:0:0 */, out_dtype="float32", axis=1) /* ty=Tensor[(1, 64, 8, 8), float32] span=DequantizeLinear_24:0:0 */;
  %97 = add(%95, %96) /* ty=Tensor[(1, 64, 8, 8), float32] span=Add_3:0:0 */;
  %98 = qnn.quantize(%97, 0.258798f /* ty=float32 span=QuantizeLinear_19.model/activation_6/Relu;model/add_2/add_scale:0:0 */, 0 /* ty=int32 span=QuantizeLinear_19:0:0 */, out_dtype="int8", axis=1) /* ty=Tensor[(1, 64, 8, 8), int8] span=QuantizeLinear_19:0:0 */;
  %99 = qnn.dequantize(%98, 0.258798f /* ty=float32 span=QuantizeLinear_19.model/activation_6/Relu;model/add_2/add_scale:0:0 */, 0 /* ty=int32 span=DequantizeLinear_28:0:0 */, out_dtype="float32", axis=1) /* ty=Tensor[(1, 64, 8, 8), float32] span=DequantizeLinear_28:0:0 */;
  %100 = nn.relu(%99) /* ty=Tensor[(1, 64, 8, 8), float32] span=Relu_7:0:0 */;
  %101 = qnn.quantize(%100, 0.255139f /* ty=float32 span=QuantizeLinear_20.Relu__26:0_scale:0:0 */, 0 /* ty=int32 span=QuantizeLinear_20:0:0 */, out_dtype="int8", axis=1) /* ty=Tensor[(1, 64, 8, 8), int8] span=QuantizeLinear_20:0:0 */;
  %102 = qnn.dequantize(%101, 0.255139f /* ty=float32 span=QuantizeLinear_20.Relu__26:0_scale:0:0 */, 0 /* ty=int32 span=DequantizeLinear_29:0:0 */, out_dtype="float32", axis=1) /* ty=Tensor[(1, 64, 8, 8), float32] span=DequantizeLinear_29:0:0 */;
  %103 = layout_transform(%102, src_layout="NCHW", dst_layout="NHWC") /* ty=Tensor[(1, 8, 8, 64), float32] */;
  %104 = nn.avg_pool2d(%103, pool_size=[8, 8], strides=[8, 8], padding=[0, 0, 0, 0], layout="NHWC", out_layout="NHWC") /* ty=Tensor[(1, 1, 1, 64), float32] span=AveragePool_1:0:0 */;
  %105 = qnn.quantize(%104, 0.255139f /* ty=float32 span=QuantizeLinear_20.Relu__26:0_scale:0:0 */, 0 /* ty=int32 span=QuantizeLinear_22:0:0 */, out_dtype="int8", axis=3) /* ty=Tensor[(1, 1, 1, 64), int8] span=QuantizeLinear_22:0:0 */;
  %106 = qnn.dequantize(%105, 0.255139f /* ty=float32 span=QuantizeLinear_20.Relu__26:0_scale:0:0 */, 0 /* ty=int32 span=DequantizeLinear_32:0:0 */, out_dtype="float32", axis=3) /* ty=Tensor[(1, 1, 1, 64), float32] span=DequantizeLinear_32:0:0 */;
  %107 = layout_transform(%106, src_layout="NHWC", dst_layout="NCHW") /* ty=Tensor[(1, 64, 1, 1), float32] */;
  %108 = reshape(%107, newshape=[-1, 64]) /* ty=Tensor[(1, 64), float32] span=Reshape_1:0:0 */;
  %109 = qnn.quantize(%108, 0.255139f /* ty=float32 span=QuantizeLinear_20.Relu__26:0_scale:0:0 */, 0 /* ty=int32 span=QuantizeLinear_23:0:0 */, out_dtype="int8", axis=1) /* ty=Tensor[(1, 64), int8] span=QuantizeLinear_23:0:0 */;
  %110 = qnn.dequantize(meta[relay.Constant][18] /* ty=Tensor[(64, 10), int8] span=DequantizeLinear_31.const_fold_opt__110_quantized:0:0 */, 0.0305544f /* ty=float32 span=DequantizeLinear_31.const_fold_opt__110_scale:0:0 */, 0 /* ty=int32 span=DequantizeLinear_31:0:0 */, out_dtype="float32", axis=1) /* ty=Tensor[(64, 10), float32] span=DequantizeLinear_31:0:0 */;
  %111 = qnn.dequantize(%109, 0.255139f /* ty=float32 span=QuantizeLinear_20.Relu__26:0_scale:0:0 */, 0 /* ty=int32 span=DequantizeLinear_33:0:0 */, out_dtype="float32", axis=1) /* ty=Tensor[(1, 64), float32] span=DequantizeLinear_33:0:0 */;
  %112 = transpose(%110, axes=[1, 0]) /* ty=Tensor[(10, 64), float32] span=Gemm_1:0:0 */;
  %113 = qnn.dequantize(meta[relay.Constant][19] /* ty=Tensor[(10), int32] span=DequantizeLinear_43.model/dense/BiasAdd/ReadVariableOp/resource_quantized:0:0 */, 0.00779561f /* ty=float32 span=DequantizeLinear_43.model/dense/BiasAdd/ReadVariableOp/resource_quantized_scale:0:0 */, 0 /* ty=int32 span=DequantizeLinear_43:0:0 */, out_dtype="float32", axis=0) /* ty=Tensor[(10), float32] span=DequantizeLinear_43:0:0 */;
  %114 = nn.dense(%111, %112, units=10) /* ty=Tensor[(1, 10), float32] span=Gemm_1:0:0 */;
  %115 = expand_dims(%113, axis=0) /* ty=Tensor[(1, 10), float32] */;
  %116 = add(%114, %115) /* ty=Tensor[(1, 10), float32] span=Gemm_1:0:0 */;
  %117 = qnn.quantize(%116, 0.205823f /* ty=float32 span=QuantizeLinear_21.Add__29:0_scale:0:0 */, 0 /* ty=int32 span=QuantizeLinear_21:0:0 */, out_dtype="int8", axis=1) /* ty=Tensor[(1, 10), int8] span=QuantizeLinear_21:0:0 */;
  %118 = qnn.dequantize(%117, 0.205823f /* ty=float32 span=QuantizeLinear_21.Add__29:0_scale:0:0 */, 0 /* ty=int32 span=DequantizeLinear_30:0:0 */, out_dtype="float32", axis=1) /* ty=Tensor[(1, 10), float32] span=DequantizeLinear_30:0:0 */;
  nn.softmax(%118, axis=1) /* ty=Tensor[(1, 10), float32] span=Softmax_1:0:0 */
}

