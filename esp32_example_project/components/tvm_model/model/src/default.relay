def @main(%input_1: Tensor[(1, 32, 32, 3), float32] /* ty=Tensor[(1, 32, 32, 3), float32] span=Transpose_1.input_1:0:0 */) -> Tensor[(1, 10), float32] {
  %0 = transpose(%input_1, axes=[0, 3, 1, 2]) /* ty=Tensor[(1, 3, 32, 32), float32] span=Transpose_1:0:0 */;
  %1 = qnn.quantize(%0, 4f /* ty=float32 span=QuantizeLinear_1.model/activation/Relu;model/batch_normalization/FusedBatchNormV3;model/conv2d/BiasAdd/ReadVariableOp/resource;model/conv2d/BiasAdd;model/conv2d_2/Conv2D;model/conv2d/Conv2D1__31:0_scale:0:0 */, 0 /* ty=int32 span=QuantizeLinear_1:0:0 */, out_dtype="int8", axis=1) /* ty=Tensor[(1, 3, 32, 32), int8] span=QuantizeLinear_1:0:0 */;
  %2 = qnn.conv2d(%1, meta[relay.Constant][0] /* ty=Tensor[(16, 3, 3, 3), int8] span=QLinearConv_1.const_fold_opt__108_quantized:0:0 */, 0 /* ty=int32 span=QLinearConv_1:0:0 */, 0 /* ty=int32 span=QLinearConv_1:0:0 */, 4f /* ty=float32 span=QLinearConv_1:0:0 */, 0.000244141f /* ty=float32 span=QLinearConv_1:0:0 */, padding=[1, 1, 1, 1], channels=16, kernel_size=[3, 3], out_dtype="int32") /* ty=Tensor[(1, 16, 32, 32), int32] span=QLinearConv_1:0:0 */;
  %3 = qnn.requantize(%2, 0.000976562f /* ty=float32 span=QLinearConv_1:0:0 */, 0 /* ty=int32 span=QLinearConv_1:0:0 */, 0.125f /* ty=float32 span=QLinearConv_1:0:0 */, 0 /* ty=int32 span=QLinearConv_1:0:0 */, axis=1, rounding="UPWARD", compute_dtype="int64", out_dtype="int8") /* ty=Tensor[(1, 16, 32, 32), int8] span=QLinearConv_1:0:0 */;
  %4 = add(%3, meta[relay.Constant][1] /* ty=Tensor[(16, 1, 1), int8] span=Add_1.model/activation/Relu;model/batch_normalization/FusedBatchNormV3;model/conv2d/BiasAdd/ReadVariableOp/resource;model/conv2d/BiasAdd;model/conv2d_2/Conv2D;model/conv2d/Conv2D_quantized_dl:0:0 */) /* ty=Tensor[(1, 16, 32, 32), int8] span=Add_1:0:0 */;
  %5 = nn.relu(%4) /* ty=Tensor[(1, 16, 32, 32), int8] span=Relu_1:0:0 */;
  %6 = qnn.conv2d(%5, meta[relay.Constant][2] /* ty=Tensor[(16, 16, 3, 3), int8] span=QLinearConv_2.const_fold_opt__119_quantized:0:0 */, 0 /* ty=int32 span=QLinearConv_2:0:0 */, 0 /* ty=int32 span=QLinearConv_2:0:0 */, 0.125f /* ty=float32 span=QLinearConv_2:0:0 */, 0.0078125f /* ty=float32 span=QLinearConv_2:0:0 */, padding=[1, 1, 1, 1], channels=16, kernel_size=[3, 3], out_dtype="int32") /* ty=Tensor[(1, 16, 32, 32), int32] span=QLinearConv_2:0:0 */;
  %7 = qnn.requantize(%6, 0.000976562f /* ty=float32 span=QLinearConv_2:0:0 */, 0 /* ty=int32 span=QLinearConv_2:0:0 */, 0.25f /* ty=float32 span=QLinearConv_2:0:0 */, 0 /* ty=int32 span=QLinearConv_2:0:0 */, axis=1, rounding="UPWARD", compute_dtype="int64", out_dtype="int8") /* ty=Tensor[(1, 16, 32, 32), int8] span=QLinearConv_2:0:0 */;
  %8 = add(%7, meta[relay.Constant][3] /* ty=Tensor[(16, 1, 1), int8] span=Add_2.model/activation_1/Relu;model/batch_normalization_1/FusedBatchNormV3;model/conv2d_1/BiasAdd/ReadVariableOp/resource;model/conv2d_1/BiasAdd;model/conv2d_2/Conv2D;model/conv2d_1/Conv2D_quantized_dl:0:0 */) /* ty=Tensor[(1, 16, 32, 32), int8] span=Add_2:0:0 */;
  %9 = nn.relu(%8) /* ty=Tensor[(1, 16, 32, 32), int8] span=Relu_2:0:0 */;
  %10 = qnn.conv2d(%9, meta[relay.Constant][4] /* ty=Tensor[(16, 16, 3, 3), int8] span=QLinearConv_3.const_fold_opt__125_quantized:0:0 */, 0 /* ty=int32 span=QLinearConv_3:0:0 */, 0 /* ty=int32 span=QLinearConv_3:0:0 */, 0.25f /* ty=float32 span=QLinearConv_3:0:0 */, 0.0078125f /* ty=float32 span=QLinearConv_3:0:0 */, padding=[1, 1, 1, 1], channels=16, kernel_size=[3, 3], out_dtype="int32") /* ty=Tensor[(1, 16, 32, 32), int32] span=QLinearConv_3:0:0 */;
  %11 = qnn.requantize(%10, 0.00195312f /* ty=float32 span=QLinearConv_3:0:0 */, 0 /* ty=int32 span=QLinearConv_3:0:0 */, 0.125f /* ty=float32 span=QLinearConv_3:0:0 */, 0 /* ty=int32 span=QLinearConv_3:0:0 */, axis=1, rounding="UPWARD", compute_dtype="int64", out_dtype="int8") /* ty=Tensor[(1, 16, 32, 32), int8] span=QLinearConv_3:0:0 */;
  %12 = add(%11, meta[relay.Constant][5] /* ty=Tensor[(16, 1, 1), int8] span=Add_3.model/batch_normalization_2/FusedBatchNormV3;model/conv2d_2/BiasAdd/ReadVariableOp/resource;model/conv2d_2/BiasAdd_quantized_dl:0:0 */) /* ty=Tensor[(1, 16, 32, 32), int8] span=Add_3:0:0 */;
  %13 = qnn.dequantize(%5, 0.125f /* ty=float32 span=QLinearAdd_1:0:0 */, 0 /* ty=int32 span=QLinearAdd_1:0:0 */, out_dtype="float32") /* ty=Tensor[(1, 16, 32, 32), float32] span=QLinearAdd_1:0:0 */;
  %14 = qnn.dequantize(%12, 0.125f /* ty=float32 span=QLinearAdd_1:0:0 */, 0 /* ty=int32 span=QLinearAdd_1:0:0 */, out_dtype="float32") /* ty=Tensor[(1, 16, 32, 32), float32] span=QLinearAdd_1:0:0 */;
  %15 = add(%13, %14) /* ty=Tensor[(1, 16, 32, 32), float32] span=QLinearAdd_1:0:0 */;
  %16 = qnn.quantize(%15, 0.125f /* ty=float32 span=QLinearAdd_1:0:0 */, 0 /* ty=int32 span=QLinearAdd_1:0:0 */, out_dtype="int8") /* ty=Tensor[(1, 16, 32, 32), int8] span=QLinearAdd_1:0:0 */;
  %17 = nn.relu(%16) /* ty=Tensor[(1, 16, 32, 32), int8] span=Relu_3:0:0 */;
  %18 = qnn.conv2d(%17, meta[relay.Constant][6] /* ty=Tensor[(32, 16, 1, 1), int8] span=QLinearConv_6.const_fold_opt__112_quantized:0:0 */, 0 /* ty=int32 span=QLinearConv_6:0:0 */, 0 /* ty=int32 span=QLinearConv_6:0:0 */, 0.125f /* ty=float32 span=QLinearConv_6:0:0 */, 0.0078125f /* ty=float32 span=QLinearConv_6:0:0 */, strides=[2, 2], padding=[0, 0, 0, 0], channels=32, kernel_size=[1, 1], out_dtype="int32") /* ty=Tensor[(1, 32, 16, 16), int32] span=QLinearConv_6:0:0 */;
  %19 = qnn.requantize(%18, 0.000976562f /* ty=float32 span=QLinearConv_6:0:0 */, 0 /* ty=int32 span=QLinearConv_6:0:0 */, 0.0625f /* ty=float32 span=QLinearConv_6:0:0 */, 0 /* ty=int32 span=QLinearConv_6:0:0 */, axis=1, rounding="UPWARD", compute_dtype="int64", out_dtype="int8") /* ty=Tensor[(1, 32, 16, 16), int8] span=QLinearConv_6:0:0 */;
  %20 = add(%19, meta[relay.Constant][7] /* ty=Tensor[(32, 1, 1), int8] span=Add_6.model/conv2d_5/BiasAdd;model/conv2d_5/Conv2D;model/conv2d_5/BiasAdd/ReadVariableOp/resource_quantized_dl:0:0 */) /* ty=Tensor[(1, 32, 16, 16), int8] span=Add_6:0:0 */;
  %21 = qnn.conv2d(%17, meta[relay.Constant][8] /* ty=Tensor[(32, 16, 3, 3), int8] span=QLinearConv_4.const_fold_opt__115_quantized:0:0 */, 0 /* ty=int32 span=QLinearConv_4:0:0 */, 0 /* ty=int32 span=QLinearConv_4:0:0 */, 0.125f /* ty=float32 span=QLinearConv_4:0:0 */, 0.00390625f /* ty=float32 span=QLinearConv_4:0:0 */, strides=[2, 2], padding=[0, 0, 1, 1], channels=32, kernel_size=[3, 3], out_dtype="int32") /* ty=Tensor[(1, 32, 16, 16), int32] span=QLinearConv_4:0:0 */;
  %22 = qnn.requantize(%21, 0.000488281f /* ty=float32 span=QLinearConv_4:0:0 */, 0 /* ty=int32 span=QLinearConv_4:0:0 */, 0.125f /* ty=float32 span=QLinearConv_4:0:0 */, 0 /* ty=int32 span=QLinearConv_4:0:0 */, axis=1, rounding="UPWARD", compute_dtype="int64", out_dtype="int8") /* ty=Tensor[(1, 32, 16, 16), int8] span=QLinearConv_4:0:0 */;
  %23 = add(%22, meta[relay.Constant][9] /* ty=Tensor[(32, 1, 1), int8] span=Add_4.model/activation_3/Relu;model/batch_normalization_3/FusedBatchNormV3;model/conv2d_3/BiasAdd/ReadVariableOp/resource;model/conv2d_3/BiasAdd;model/conv2d_5/Conv2D;model/conv2d_3/Conv2D_quantized_dl:0:0 */) /* ty=Tensor[(1, 32, 16, 16), int8] span=Add_4:0:0 */;
  %24 = nn.relu(%23) /* ty=Tensor[(1, 32, 16, 16), int8] span=Relu_4:0:0 */;
  %25 = qnn.conv2d(%24, meta[relay.Constant][10] /* ty=Tensor[(32, 32, 3, 3), int8] span=QLinearConv_5.const_fold_opt__121_quantized:0:0 */, 0 /* ty=int32 span=QLinearConv_5:0:0 */, 0 /* ty=int32 span=QLinearConv_5:0:0 */, 0.125f /* ty=float32 span=QLinearConv_5:0:0 */, 0.0078125f /* ty=float32 span=QLinearConv_5:0:0 */, padding=[1, 1, 1, 1], channels=32, kernel_size=[3, 3], out_dtype="int32") /* ty=Tensor[(1, 32, 16, 16), int32] span=QLinearConv_5:0:0 */;
  %26 = qnn.requantize(%25, 0.000976562f /* ty=float32 span=QLinearConv_5:0:0 */, 0 /* ty=int32 span=QLinearConv_5:0:0 */, 0.125f /* ty=float32 span=QLinearConv_5:0:0 */, 0 /* ty=int32 span=QLinearConv_5:0:0 */, axis=1, rounding="UPWARD", compute_dtype="int64", out_dtype="int8") /* ty=Tensor[(1, 32, 16, 16), int8] span=QLinearConv_5:0:0 */;
  %27 = add(%26, meta[relay.Constant][11] /* ty=Tensor[(32, 1, 1), int8] span=Add_5.model/batch_normalization_4/FusedBatchNormV3;model/conv2d_4/BiasAdd/ReadVariableOp/resource;model/conv2d_4/BiasAdd_quantized_dl:0:0 */) /* ty=Tensor[(1, 32, 16, 16), int8] span=Add_5:0:0 */;
  %28 = qnn.dequantize(%20, 0.0625f /* ty=float32 span=QLinearAdd_2:0:0 */, 0 /* ty=int32 span=QLinearAdd_2:0:0 */, out_dtype="float32") /* ty=Tensor[(1, 32, 16, 16), float32] span=QLinearAdd_2:0:0 */;
  %29 = qnn.dequantize(%27, 0.125f /* ty=float32 span=QLinearAdd_2:0:0 */, 0 /* ty=int32 span=QLinearAdd_2:0:0 */, out_dtype="float32") /* ty=Tensor[(1, 32, 16, 16), float32] span=QLinearAdd_2:0:0 */;
  %30 = add(%28, %29) /* ty=Tensor[(1, 32, 16, 16), float32] span=QLinearAdd_2:0:0 */;
  %31 = qnn.quantize(%30, 0.25f /* ty=float32 span=QLinearAdd_2:0:0 */, 0 /* ty=int32 span=QLinearAdd_2:0:0 */, out_dtype="int8") /* ty=Tensor[(1, 32, 16, 16), int8] span=QLinearAdd_2:0:0 */;
  %32 = qnn.dequantize(%31, 0.25f /* ty=float32 span=QLinearAdd_2.model/activation_4/Relu;model/add_1/add_scale:0:0 */, 0 /* ty=int32 span=DequantizeLinear_5:0:0 */, out_dtype="float32", axis=1) /* ty=Tensor[(1, 32, 16, 16), float32] span=DequantizeLinear_5:0:0 */;
  %33 = nn.relu(%32) /* ty=Tensor[(1, 32, 16, 16), float32] span=Relu_5:0:0 */;
  %34 = qnn.quantize(%33, 0.125f /* ty=float32 span=QuantizeLinear_6.Relu__19:0_scale:0:0 */, 0 /* ty=int32 span=QuantizeLinear_6:0:0 */, out_dtype="int8", axis=1) /* ty=Tensor[(1, 32, 16, 16), int8] span=QuantizeLinear_6:0:0 */;
  %35 = qnn.conv2d(%34, meta[relay.Constant][12] /* ty=Tensor[(64, 32, 1, 1), int8] span=QLinearConv_9.const_fold_opt__117_quantized:0:0 */, 0 /* ty=int32 span=QLinearConv_9:0:0 */, 0 /* ty=int32 span=QLinearConv_9:0:0 */, 0.125f /* ty=float32 span=QLinearConv_9:0:0 */, 0.0078125f /* ty=float32 span=QLinearConv_9:0:0 */, strides=[2, 2], padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1], out_dtype="int32") /* ty=Tensor[(1, 64, 8, 8), int32] span=QLinearConv_9:0:0 */;
  %36 = qnn.requantize(%35, 0.000976562f /* ty=float32 span=QLinearConv_9:0:0 */, 0 /* ty=int32 span=QLinearConv_9:0:0 */, 0.125f /* ty=float32 span=QLinearConv_9:0:0 */, 0 /* ty=int32 span=QLinearConv_9:0:0 */, axis=1, rounding="UPWARD", compute_dtype="int64", out_dtype="int8") /* ty=Tensor[(1, 64, 8, 8), int8] span=QLinearConv_9:0:0 */;
  %37 = add(%36, meta[relay.Constant][13] /* ty=Tensor[(64, 1, 1), int8] span=Add_9.model/conv2d_8/BiasAdd;model/conv2d_8/Conv2D;model/conv2d_8/BiasAdd/ReadVariableOp/resource_quantized_dl:0:0 */) /* ty=Tensor[(1, 64, 8, 8), int8] span=Add_9:0:0 */;
  %38 = qnn.conv2d(%34, meta[relay.Constant][14] /* ty=Tensor[(64, 32, 3, 3), int8] span=QLinearConv_7.const_fold_opt__120_quantized:0:0 */, 0 /* ty=int32 span=QLinearConv_7:0:0 */, 0 /* ty=int32 span=QLinearConv_7:0:0 */, 0.125f /* ty=float32 span=QLinearConv_7:0:0 */, 0.00195312f /* ty=float32 span=QLinearConv_7:0:0 */, strides=[2, 2], padding=[0, 0, 1, 1], channels=64, kernel_size=[3, 3], out_dtype="int32") /* ty=Tensor[(1, 64, 8, 8), int32] span=QLinearConv_7:0:0 */;
  %39 = qnn.requantize(%38, 0.000244141f /* ty=float32 span=QLinearConv_7:0:0 */, 0 /* ty=int32 span=QLinearConv_7:0:0 */, 0.125f /* ty=float32 span=QLinearConv_7:0:0 */, 0 /* ty=int32 span=QLinearConv_7:0:0 */, axis=1, rounding="UPWARD", compute_dtype="int64", out_dtype="int8") /* ty=Tensor[(1, 64, 8, 8), int8] span=QLinearConv_7:0:0 */;
  %40 = add(%39, meta[relay.Constant][15] /* ty=Tensor[(64, 1, 1), int8] span=Add_7.model/activation_5/Relu;model/batch_normalization_5/FusedBatchNormV3;model/conv2d_6/BiasAdd/ReadVariableOp/resource;model/conv2d_6/BiasAdd;model/conv2d_8/Conv2D;model/conv2d_6/Conv2D_quantized_dl:0:0 */) /* ty=Tensor[(1, 64, 8, 8), int8] span=Add_7:0:0 */;
  %41 = qnn.dequantize(%40, 0.125f /* ty=float32 span=QLinearConv_7.model/activation_5/Relu;model/batch_normalization_5/FusedBatchNormV3;model/conv2d_6/BiasAdd/ReadVariableOp/resource;model/conv2d_6/BiasAdd;model/conv2d_8/Conv2D;model/conv2d_6/Conv2D1_scale:0:0 */, 0 /* ty=int32 span=DequantizeLinear_6:0:0 */, out_dtype="float32", axis=1) /* ty=Tensor[(1, 64, 8, 8), float32] span=DequantizeLinear_6:0:0 */;
  %42 = nn.relu(%41) /* ty=Tensor[(1, 64, 8, 8), float32] span=Relu_6:0:0 */;
  %43 = qnn.quantize(%42, 0.0625f /* ty=float32 span=QuantizeLinear_7.Relu__21:0_scale:0:0 */, 0 /* ty=int32 span=QuantizeLinear_7:0:0 */, out_dtype="int8", axis=1) /* ty=Tensor[(1, 64, 8, 8), int8] span=QuantizeLinear_7:0:0 */;
  %44 = qnn.conv2d(%43, meta[relay.Constant][16] /* ty=Tensor[(64, 64, 3, 3), int8] span=QLinearConv_8.const_fold_opt__123_quantized:0:0 */, 0 /* ty=int32 span=QLinearConv_8:0:0 */, 0 /* ty=int32 span=QLinearConv_8:0:0 */, 0.0625f /* ty=float32 span=QLinearConv_8:0:0 */, 0.015625f /* ty=float32 span=QLinearConv_8:0:0 */, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3], out_dtype="int32") /* ty=Tensor[(1, 64, 8, 8), int32] span=QLinearConv_8:0:0 */;
  %45 = qnn.requantize(%44, 0.000976562f /* ty=float32 span=QLinearConv_8:0:0 */, 0 /* ty=int32 span=QLinearConv_8:0:0 */, 0.25f /* ty=float32 span=QLinearConv_8:0:0 */, 0 /* ty=int32 span=QLinearConv_8:0:0 */, axis=1, rounding="UPWARD", compute_dtype="int64", out_dtype="int8") /* ty=Tensor[(1, 64, 8, 8), int8] span=QLinearConv_8:0:0 */;
  %46 = add(%45, meta[relay.Constant][17] /* ty=Tensor[(64, 1, 1), int8] span=Add_8.model/batch_normalization_6/FusedBatchNormV3;model/conv2d_7/BiasAdd/ReadVariableOp/resource;model/conv2d_7/BiasAdd_quantized_dl:0:0 */) /* ty=Tensor[(1, 64, 8, 8), int8] span=Add_8:0:0 */;
  %47 = qnn.dequantize(%37, 0.125f /* ty=float32 span=QLinearAdd_3:0:0 */, 0 /* ty=int32 span=QLinearAdd_3:0:0 */, out_dtype="float32") /* ty=Tensor[(1, 64, 8, 8), float32] span=QLinearAdd_3:0:0 */;
  %48 = qnn.dequantize(%46, 0.25f /* ty=float32 span=QLinearAdd_3:0:0 */, 0 /* ty=int32 span=QLinearAdd_3:0:0 */, out_dtype="float32") /* ty=Tensor[(1, 64, 8, 8), float32] span=QLinearAdd_3:0:0 */;
  %49 = add(%47, %48) /* ty=Tensor[(1, 64, 8, 8), float32] span=QLinearAdd_3:0:0 */;
  %50 = qnn.quantize(%49, 0.5f /* ty=float32 span=QLinearAdd_3:0:0 */, 0 /* ty=int32 span=QLinearAdd_3:0:0 */, out_dtype="int8") /* ty=Tensor[(1, 64, 8, 8), int8] span=QLinearAdd_3:0:0 */;
  %51 = nn.relu(%50) /* ty=Tensor[(1, 64, 8, 8), int8] span=Relu_7:0:0 */;
  %52 = qnn.dequantize(%51, 0.5f /* ty=float32 span=QLinearAveragePool_1:0:0 */, 0 /* ty=int32 span=QLinearAveragePool_1:0:0 */, out_dtype="float32") /* ty=Tensor[(1, 64, 8, 8), float32] span=QLinearAveragePool_1:0:0 */;
  %53 = nn.avg_pool2d(%52, pool_size=[8, 8], strides=[8, 8], padding=[0, 0, 0, 0]) /* ty=Tensor[(1, 64, 1, 1), float32] span=QLinearAveragePool_1:0:0 */;
  %54 = qnn.quantize(%53, 0.0625f /* ty=float32 span=QLinearAveragePool_1:0:0 */, 0 /* ty=int32 span=QLinearAveragePool_1:0:0 */, out_dtype="int8") /* ty=Tensor[(1, 64, 1, 1), int8] span=QLinearAveragePool_1:0:0 */;
  %55 = reshape(%54, newshape=[-1, 64]) /* ty=Tensor[(1, 64), int8] span=Reshape_1:0:0 */;
  %56 = qnn.dense(%55, meta[relay.Constant][18] /* ty=Tensor[(10, 64), int8] span=QGemm_1:0:0 */, 0 /* ty=int32 span=QGemm_1:0:0 */, 0 /* ty=int32 span=QGemm_1:0:0 */, 0.0625f /* ty=float32 span=QGemm_1:0:0 */, 0.03125f /* ty=float32 span=QGemm_1:0:0 */, units=10, out_dtype="int32") /* ty=Tensor[(1, 10), int32] span=QGemm_1:0:0 */;
  %57 = add(%56, meta[relay.Constant][19] /* ty=Tensor[(10), int32] span=QGemm_1.model/dense/BiasAdd/ReadVariableOp/resource_quantized:0:0 */) /* ty=Tensor[(1, 10), int32] span=QGemm_1:0:0 */;
  %58 = qnn.requantize(%57, 0.00195312f /* ty=float32 span=QGemm_1:0:0 */, 0 /* ty=int32 span=QGemm_1:0:0 */, 0.25f /* ty=float32 span=QGemm_1.Add__29:0_scale:0:0 */, 0 /* ty=int32 span=QGemm_1:0:0 */, rounding="TONEAREST", compute_dtype="int64", out_dtype="int8") /* ty=Tensor[(1, 10), int8] span=QGemm_1:0:0 */;
  %59 = qnn.dequantize(%58, 0.25f /* ty=float32 span=QGemm_1.Add__29:0_scale:0:0 */, 0 /* ty=int32 span=DequantizeLinear_8:0:0 */, out_dtype="float32", axis=1) /* ty=Tensor[(1, 10), float32] span=DequantizeLinear_8:0:0 */;
  nn.softmax(%59, axis=1) /* ty=Tensor[(1, 10), float32] span=Softmax_1:0:0 */
}

