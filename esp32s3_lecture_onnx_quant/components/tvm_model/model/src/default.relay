def @main(%input: Tensor[(1, 28, 28, 1), float32] /* ty=Tensor[(1, 28, 28, 1), float32] span=Reshape_1.input:0:0 */) -> Tensor[(1, 10), float32] {
  %0 = reshape(%input, newshape=[-1, 1, 28, 28]) /* ty=Tensor[(1, 1, 28, 28), float32] span=Reshape_1:0:0 */;
  %1 = qnn.quantize(%0, 4f /* ty=float32 span=QuantizeLinear_1.sequential_1/input/BiasAdd__6:0_scale:0:0 */, 0 /* ty=int32 span=QuantizeLinear_1:0:0 */, out_dtype="int8", axis=1) /* ty=Tensor[(1, 1, 28, 28), int8] span=QuantizeLinear_1:0:0 */;
  %2 = layout_transform(%1, src_layout="NCHW", dst_layout="NHWC") /* ty=Tensor[(1, 28, 28, 1), int8] */;
  %3 = qnn.conv2d(%2, meta[relay.Constant][0] /* ty=Tensor[(3, 3, 1, 64), int8] */, 0 /* ty=int32 span=QLinearConv_1:0:0 */, 0 /* ty=int32 span=QLinearConv_1:0:0 */, 4f /* ty=float32 span=QLinearConv_1:0:0 */, 0.00195312f /* ty=float32 span=QLinearConv_1:0:0 */, padding=[0, 0, 0, 0], channels=64, kernel_size=[3, 3], data_layout="NHWC", kernel_layout="HWIO", out_dtype="int32") /* ty=Tensor[(1, 26, 26, 64), int32] span=QLinearConv_1:0:0 */;
  %4 = qnn.requantize(%3, 0.0078125f /* ty=float32 span=QLinearConv_1:0:0 */, 0 /* ty=int32 span=QLinearConv_1:0:0 */, 2f /* ty=float32 span=QLinearConv_1:0:0 */, 0 /* ty=int32 span=QLinearConv_1:0:0 */, axis=3, rounding="UPWARD", compute_dtype="int64", out_dtype="int8") /* ty=Tensor[(1, 26, 26, 64), int8] span=QLinearConv_1:0:0 */;
  %5 = add(%4, meta[relay.Constant][1] /* ty=Tensor[(1, 1, 1, 64), int8] */) /* ty=Tensor[(1, 26, 26, 64), int8] span=Add_1:0:0 */;
  %6 = qnn.dequantize(%5, 2f /* ty=float32 span=QLinearConv_1.sequential_1/input/BiasAdd:0_scale:0:0 */, 0 /* ty=int32 span=DequantizeLinear_1:0:0 */, out_dtype="float32", axis=3) /* ty=Tensor[(1, 26, 26, 64), float32] span=DequantizeLinear_1:0:0 */;
  %7 = nn.relu(%6) /* ty=Tensor[(1, 26, 26, 64), float32] span=Relu_1:0:0 */;
  %8 = qnn.quantize(%7, 1f /* ty=float32 span=QuantizeLinear_2.sequential_1/input/Relu:0_scale:0:0 */, 0 /* ty=int32 span=QuantizeLinear_2:0:0 */, out_dtype="int8", axis=3) /* ty=Tensor[(1, 26, 26, 64), int8] span=QuantizeLinear_2:0:0 */;
  %9 = @tvmgen_default_esp_main_0(%8, meta[relay.Constant][2] /* ty=Tensor[(32, 3, 3, 64), int8] */, meta[relay.Constant][3] /* ty=Tensor[(1, 1, 1, 32), int8] */) /* ty=Tensor[(1, 24, 24, 32), int8] */;
  %10 = qnn.dequantize(%9, 1f /* ty=float32 span=QLinearConv_2.sequential_1/conv2d_1/BiasAdd:0_scale:0:0 */, 0 /* ty=int32 span=DequantizeLinear_2:0:0 */, out_dtype="float32", axis=3) /* ty=Tensor[(1, 24, 24, 32), float32] span=DequantizeLinear_2:0:0 */;
  %11 = nn.relu(%10) /* ty=Tensor[(1, 24, 24, 32), float32] span=Relu_2:0:0 */;
  %12 = transpose(%11, axes=[0, 1, 2, 3]) /* ty=Tensor[(1, 24, 24, 32), float32] span=Transpose_1:0:0 */;
  %13 = reshape(%12, newshape=[-1, 18432]) /* ty=Tensor[(1, 18432), float32] span=Reshape_2:0:0 */;
  %14 = qnn.quantize(%13, 0.25f /* ty=float32 span=QuantizeLinear_3.sequential_1/flatten_1/Reshape:0_scale:0:0 */, 0 /* ty=int32 span=QuantizeLinear_3:0:0 */, out_dtype="int8", axis=1) /* ty=Tensor[(1, 18432), int8] span=QuantizeLinear_3:0:0 */;
  %15 = qnn.dense(%14, meta[relay.Constant][4] /* ty=Tensor[(10, 18432), int8] span=QGemm_1:0:0 */, 0 /* ty=int32 span=QGemm_1:0:0 */, 0 /* ty=int32 span=QGemm_1:0:0 */, 0.25f /* ty=float32 span=QGemm_1:0:0 */, 0.00195312f /* ty=float32 span=QGemm_1:0:0 */, units=10, out_dtype="int32") /* ty=Tensor[(1, 10), int32] span=QGemm_1:0:0 */;
  %16 = add(%15, meta[relay.Constant][5] /* ty=Tensor[(1, 10), int32] */) /* ty=Tensor[(1, 10), int32] span=QGemm_1:0:0 */;
  %17 = qnn.requantize(%16, 0.000488281f /* ty=float32 span=QGemm_1:0:0 */, 0 /* ty=int32 span=QGemm_1:0:0 */, 0.25f /* ty=float32 span=QGemm_1.sequential_1/dense_1/BiasAdd:0_scale:0:0 */, 0 /* ty=int32 span=QGemm_1:0:0 */, axis=1, rounding="TONEAREST", compute_dtype="int64", out_dtype="int8") /* ty=Tensor[(1, 10), int8] span=QGemm_1:0:0 */;
  %18 = qnn.dequantize(%17, 0.25f /* ty=float32 span=QGemm_1.sequential_1/dense_1/BiasAdd:0_scale:0:0 */, 0 /* ty=int32 span=DequantizeLinear_3:0:0 */, out_dtype="float32", axis=1) /* ty=Tensor[(1, 10), float32] span=DequantizeLinear_3:0:0 */;
  nn.softmax(%18, axis=1) /* ty=Tensor[(1, 10), float32] span=Softmax_1:0:0 */
}

def @tvmgen_default_esp_main_0(%esp_0_i0: Tensor[(1, 26, 26, 64), int8] /* ty=Tensor[(1, 26, 26, 64), int8] */, %esp_func_var_0: Tensor[(32, 3, 3, 64), int8] /* ty=Tensor[(32, 3, 3, 64), int8] */, %esp_func_var_1: Tensor[(1, 1, 1, 32), int8] /* ty=Tensor[(1, 1, 1, 32), int8] */, Compiler="esp", Primitive=1, Inline=1, global_symbol="tvmgen_default_esp_main_0") -> Tensor[(1, 24, 24, 32), int8] {
  %21 = fn (%FunctionVar_0_0: Tensor[(1, 26, 26, 64), int8] /* ty=Tensor[(1, 26, 26, 64), int8] */, %tvm_var_extract_const_0: Tensor[(32, 3, 3, 64), int8] /* ty=Tensor[(32, 3, 3, 64), int8] */, %tvm_var_extract_const_1: Tensor[(1, 1, 1, 32), int8] /* ty=Tensor[(1, 1, 1, 32), int8] */, PartitionedFromPattern="qnn.conv2d_qnn.requantize_add_", Composite="esp.qnn_conv2d_onnx") -> Tensor[(1, 24, 24, 32), int8] {
    %19 = qnn.conv2d(%FunctionVar_0_0, %tvm_var_extract_const_0, 0 /* ty=int32 span=QLinearConv_2:0:0 */, 0 /* ty=int32 span=QLinearConv_2:0:0 */, 0f /* ty=float32 */, -9f /* ty=float32 */, padding=[0, 0, 0, 0], channels=32, kernel_size=[3, 3], data_layout="NHWC", kernel_layout="OHWI", out_dtype="int32") /* ty=Tensor[(1, 24, 24, 32), int32] */;
    %20 = qnn.requantize(%19, 0.00195312f /* ty=float32 span=QLinearConv_2:0:0 */, 0 /* ty=int32 span=QLinearConv_2:0:0 */, 0f /* ty=float32 */, 0 /* ty=int32 span=QLinearConv_2:0:0 */, axis=3, out_dtype="int8") /* ty=Tensor[(1, 24, 24, 32), int8] */;
    add(%20, %tvm_var_extract_const_1) /* ty=Tensor[(1, 24, 24, 32), int8] */
  } /* ty=fn (Tensor[(1, 26, 26, 64), int8], Tensor[(32, 3, 3, 64), int8], Tensor[(1, 1, 1, 32), int8]) -> Tensor[(1, 24, 24, 32), int8] */;
  %21(%esp_0_i0, %esp_func_var_0, %esp_func_var_1) /* ty=Tensor[(1, 24, 24, 32), int8] */
}

